# SARAi v2.16 "Omni-Loop" - Reflexive Multimodal AGI

**Status**: PLANNING PHASE  
**Prerequisitos**: v2.12 + v2.13 + v2.14 + v2.15 COMPLETADOS  
**Timeline**: Nov 26 - Dic 10, 2025 (15 d√≠as)  
**Autor**: SARAi Dev Team  
**Fecha**: Oct 28, 2025

---

## üß† Executive Summary

**v2.16 Omni-Loop** representa la **culminaci√≥n evolutiva** de SARAi: un sistema AGI que no solo responde, sino que **reflexiona, auto-corrige y aprende continuamente** mediante ciclos interactivos multimodales con llama.cpp, fine-tuning nocturno con LoRA, y preprocesamiento optimizado de im√°genes.

### Mantra v2.16

> _"Cada token es una decisi√≥n.  
> Cada imagen, una intenci√≥n.  
> Con solo la CPU y 16 GB, SARAi no solo responde: reflexiona, ve, corrige y evoluciona;  
> no solo corre: piensa en cada ciclo.  
> Omni-Loop no es un feature: es la conciencia t√©cnica de una AGI que se piensa antes de hablar."_

### KPIs Ultra-Agresivos v2.16

| KPI | v2.15 Base | v2.16 Target | Œî | M√©todo |
|-----|------------|--------------|---|--------|
| **RAM P99** | 10.8 GB | **9.9 GB** | **-8%** | llama.cpp cache optimizations |
| **Latency P50** | 19.5s ‚Üí 11s | **7.9s** | **-59%** | Speculative + Interactive loop |
| **Utilizaci√≥n Modelo** | 65% | **78%** | **+20%** | Interactive continuations |
| **Entity Recall** | 87% | **91%** | **+5%** | Persistent memory loop |
| **Auto-correcci√≥n** | 33% | **68%** | **+106%** | Reflexive prompts + LoRA |
| **Multimodal Cache Hit** | 0% | **97%** | **NEW** | WebP + Perceptual hash |

---

## üèóÔ∏è Arquitectura v2.16 Omni-Loop

### Diagrama de Flujo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    USER INPUT (Text/Audio/Image)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Multimodal Router ‚îÇ
                    ‚îÇ  (audio_router.py) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚Üì                               ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Text/Audio Path ‚îÇ            ‚îÇ   Image Path     ‚îÇ
    ‚îÇ  (Existing)     ‚îÇ            ‚îÇ  (NEW v2.16)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì                               ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ TRM Classifier ‚îÇ            ‚îÇ Image Preprocessor‚îÇ
    ‚îÇ + MCP Weights  ‚îÇ            ‚îÇ OpenCV ‚Üí WebP    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì                               ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         OMNI-LOOP ENGINE (llama.cpp)         ‚îÇ
    ‚îÇ  --interactive-first --interactive-cont 3    ‚îÇ
    ‚îÇ  --mmproj qwen2.5-omni-mmproj.gguf          ‚îÇ
    ‚îÇ  --cache-type-k f16 (KV cache optimizado)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚Üì                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ITERATION 1      ‚îÇ    ‚îÇ ITERATION 2-3      ‚îÇ
‚îÇ Initial Response ‚îÇ    ‚îÇ Self-Reflection    ‚îÇ
‚îÇ (Draft)          ‚îÇ    ‚îÇ + Auto-Correction  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                       ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Response Validator  ‚îÇ
            ‚îÇ  (GPG-signed prompts)‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚Üì                  ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Valid Response ‚îÇ   ‚îÇ Fallback     ‚îÇ
     ‚îÇ (Return)       ‚îÇ   ‚îÇ LFM2-1.2B    ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Feedback Logger     ‚îÇ
            ‚îÇ  (LoRA Dataset)      ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  NIGHTLY LoRA TRAIN  ‚îÇ
            ‚îÇ  (Docker isolated)   ‚îÇ
            ‚îÇ  llama-lora-merge    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Componentes Principales v2.16

### 1. **Omni-Loop Engine** (Reflexive LLM)

**Archivo**: `core/omni_loop.py`

**Prop√≥sito**: Orquestar ciclos interactivos de llama.cpp con l√≠mites estrictos para evitar bucles infinitos.

**Arquitectura**:

```python
# core/omni_loop.py
import subprocess
import hashlib
from typing import Optional, List, Dict
from dataclasses import dataclass
from pathlib import Path

@dataclass
class LoopConfig:
    """Configuraci√≥n del Omni-Loop"""
    max_iterations: int = 3  # CR√çTICO: L√≠mite hard-coded
    model_path: str = "models/gguf/qwen2.5-omni-3b-iq4_nl.gguf"
    mmproj_path: str = "models/gguf/qwen2.5-omni-mmproj.gguf"
    cache_type_k: str = "f16"  # KV cache compacto
    n_ctx: int = 2048
    temperature: float = 0.7
    use_mmap: bool = True
    use_mlock: bool = False  # Evitar OOM
    
@dataclass
class LoopIteration:
    """Resultado de una iteraci√≥n del loop"""
    iteration: int
    response: str
    confidence: float
    corrected: bool
    latency_ms: float

class OmniLoop:
    """
    Motor de loops reflexivos con llama.cpp interactive mode
    
    FILOSOF√çA:
    - Cada token es una decisi√≥n consciente
    - El loop SIEMPRE termina (max 3 iteraciones)
    - El sistema se auto-corrige antes de responder
    - Fallback a LFM2 si loop falla
    """
    
    def __init__(self, config: LoopConfig = None):
        self.config = config or LoopConfig()
        self.llama_cpp_bin = self._find_llama_cpp()
        self.loop_history = []
        
    def _find_llama_cpp(self) -> Path:
        """Localiza binario de llama.cpp"""
        candidates = [
            "/usr/local/bin/llama-cli",
            "/usr/bin/llama-cli",
            Path.home() / "llama.cpp/build/bin/llama-cli"
        ]
        for path in candidates:
            if Path(path).exists():
                return Path(path)
        raise FileNotFoundError("llama-cli not found. Install llama.cpp first.")
    
    def execute_loop(
        self, 
        prompt: str, 
        image_path: Optional[str] = None,
        enable_reflection: bool = True
    ) -> Dict:
        """
        Ejecuta loop reflexivo con llama.cpp
        
        Args:
            prompt: User input (text)
            image_path: Optional image for multimodal processing
            enable_reflection: Habilitar auto-correcci√≥n (default: True)
        
        Returns:
            {
                "response": str,
                "iterations": List[LoopIteration],
                "total_latency_ms": float,
                "auto_corrected": bool,
                "fallback_used": bool
            }
        """
        import time
        start = time.perf_counter()
        
        iterations = []
        current_response = ""
        
        try:
            # ITERATION 1: Initial Draft
            iter1 = self._run_iteration(
                prompt=prompt,
                image_path=image_path,
                iteration=1,
                previous_response=None
            )
            iterations.append(iter1)
            current_response = iter1.response
            
            if not enable_reflection:
                # Sin reflexi√≥n, retornar draft directamente
                return self._build_result(iterations, start, fallback=False)
            
            # ITERATIONS 2-3: Self-Reflection & Correction
            for i in range(2, self.config.max_iterations + 1):
                reflection_prompt = self._build_reflection_prompt(
                    original_prompt=prompt,
                    draft_response=current_response,
                    iteration=i
                )
                
                iter_n = self._run_iteration(
                    prompt=reflection_prompt,
                    image_path=None,  # Solo texto en reflexi√≥n
                    iteration=i,
                    previous_response=current_response
                )
                iterations.append(iter_n)
                
                # Si la respuesta es "v√°lida" (confidence > 0.85), terminar loop
                if iter_n.confidence > 0.85:
                    current_response = iter_n.response
                    break
                
                current_response = iter_n.response
            
            return self._build_result(iterations, start, fallback=False)
        
        except Exception as e:
            # FALLBACK: LFM2-1.2B (blindaje de continuidad)
            logger.error(f"Omni-Loop failed: {e}. Falling back to LFM2.")
            fallback_response = self._fallback_lfm2(prompt)
            
            return {
                "response": fallback_response,
                "iterations": iterations,
                "total_latency_ms": (time.perf_counter() - start) * 1000,
                "auto_corrected": False,
                "fallback_used": True,
                "fallback_reason": str(e)
            }
    
    def _run_iteration(
        self, 
        prompt: str, 
        image_path: Optional[str],
        iteration: int,
        previous_response: Optional[str]
    ) -> LoopIteration:
        """
        Ejecuta una iteraci√≥n del loop con llama.cpp
        
        Comando llama.cpp:
        llama-cli \
            --model qwen2.5-omni-3b.gguf \
            --mmproj qwen2.5-omni-mmproj.gguf \
            --image image.webp \
            --interactive-first \
            --interactive-cont 3 \
            --cache-type-k f16 \
            --prompt "..." \
            --no-display-prompt
        """
        import time
        start = time.perf_counter()
        
        cmd = [
            str(self.llama_cpp_bin),
            "--model", self.config.model_path,
            "--n-ctx", str(self.config.n_ctx),
            "--temp", str(self.config.temperature),
            "--cache-type-k", self.config.cache_type_k,
            "--interactive-first",
            "--interactive-cont", str(self.config.max_iterations),
            "--no-display-prompt"
        ]
        
        # Multimodal: A√±adir imagen si existe
        if image_path:
            cmd.extend(["--mmproj", self.config.mmproj_path])
            cmd.extend(["--image", image_path])
        
        if not self.config.use_mmap:
            cmd.append("--no-mmap")
        
        # Prompt con contexto previo (si existe)
        full_prompt = prompt
        if previous_response:
            full_prompt = f"[Previous attempt]\n{previous_response}\n\n[Reflect and improve]\n{prompt}"
        
        cmd.extend(["--prompt", full_prompt])
        
        # Ejecutar llama.cpp
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30  # 30s timeout por iteraci√≥n
        )
        
        response = result.stdout.strip()
        latency_ms = (time.perf_counter() - start) * 1000
        
        # Calcular confidence (simplificado: basado en longitud + coherencia)
        confidence = self._calculate_confidence(response, prompt)
        
        # Detectar si hubo auto-correcci√≥n
        corrected = previous_response is not None and response != previous_response
        
        return LoopIteration(
            iteration=iteration,
            response=response,
            confidence=confidence,
            corrected=corrected,
            latency_ms=latency_ms
        )
    
    def _build_reflection_prompt(
        self, 
        original_prompt: str, 
        draft_response: str,
        iteration: int
    ) -> str:
        """
        Construye prompt de auto-reflexi√≥n firmado con GPG
        
        CR√çTICO: El prompt de reflexi√≥n est√° firmado para auditabilidad
        """
        reflection_template = """
[SYSTEM: Self-Reflection Mode - Iteration {iteration}/3]

Original User Request:
{original_prompt}

Your Previous Response (Draft):
{draft_response}

INSTRUCTIONS:
1. Analyze your previous response critically
2. Identify factual errors, inconsistencies, or unclear statements
3. Provide an improved version that addresses these issues
4. If your previous response was already optimal, confirm it

Improved Response:
"""
        
        prompt = reflection_template.format(
            iteration=iteration,
            original_prompt=original_prompt,
            draft_response=draft_response
        )
        
        # TODO: Firmar con GPG para auditabilidad
        # prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        
        return prompt
    
    def _calculate_confidence(self, response: str, prompt: str) -> float:
        """
        Calcula confidence score de la respuesta
        
        Heur√≠sticas:
        - Longitud razonable (50-500 caracteres)
        - No contiene placeholders como "..."
        - No repite el prompt textualmente
        """
        if len(response) < 50:
            return 0.3
        
        if len(response) > 500:
            return 0.6
        
        if "..." in response or "[TODO]" in response:
            return 0.4
        
        # Similaridad con prompt (penaliza copy-paste)
        if prompt.lower() in response.lower():
            return 0.5
        
        return 0.85  # Confidence por defecto
    
    def _fallback_lfm2(self, prompt: str) -> str:
        """Fallback a LFM2-1.2B si Omni-Loop falla"""
        from core.model_pool import get_model_pool
        
        pool = get_model_pool()
        lfm2 = pool.get("tiny")
        
        response = lfm2.create_completion(
            prompt=f"[FALLBACK MODE]\n{prompt}",
            max_tokens=256,
            temperature=0.7
        )
        
        return response["choices"][0]["text"]
    
    def _build_result(
        self, 
        iterations: List[LoopIteration], 
        start_time: float,
        fallback: bool
    ) -> Dict:
        """Construye resultado final del loop"""
        total_latency = (time.perf_counter() - start_time) * 1000
        final_iteration = iterations[-1]
        
        # Detectar si hubo auto-correcci√≥n
        auto_corrected = any(it.corrected for it in iterations)
        
        return {
            "response": final_iteration.response,
            "iterations": iterations,
            "total_latency_ms": total_latency,
            "auto_corrected": auto_corrected,
            "fallback_used": fallback,
            "confidence": final_iteration.confidence
        }


# Factory para integraci√≥n con LangGraph
def create_omni_loop(config: Optional[LoopConfig] = None) -> OmniLoop:
    """Factory pattern para OmniLoop"""
    return OmniLoop(config=config)
```

**KPIs del componente**:
- Latencia por iteraci√≥n: <10s
- Max iteraciones: 3 (hard limit)
- Confidence threshold: >0.85
- Fallback rate: <5%

---

### 2. **Image Preprocessor** (Multimodal Optimization)

**Archivo**: `agents/image_preprocessor.py`

**Prop√≥sito**: Optimizar im√°genes a WebP para reducir almacenamiento y acelerar procesamiento multimodal.

```python
# agents/image_preprocessor.py
import cv2
import hashlib
from pathlib import Path
from typing import Tuple, Optional
from dataclasses import dataclass
import imagehash
from PIL import Image

@dataclass
class PreprocessConfig:
    """Configuraci√≥n de preprocesamiento"""
    target_format: str = "webp"
    max_width: int = 512
    max_height: int = 512
    quality: int = 85
    cache_dir: Path = Path("state/image_cache")
    ttl_days: int = 7  # Time-to-live para rotaci√≥n de cache

class ImagePreprocessor:
    """
    Preprocesador de im√°genes para Omni-Loop
    
    Pipeline:
    1. Cargar imagen con OpenCV
    2. Redimensionar a max 512x512 (preservar aspect ratio)
    3. Convertir a WebP (quality 85, ~30KB)
    4. Cachear con perceptual hash (dedup)
    5. Rotar cache seg√∫n TTL
    """
    
    def __init__(self, config: PreprocessConfig = None):
        self.config = config or PreprocessConfig()
        self.config.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def preprocess(self, image_path: str) -> Tuple[Path, str]:
        """
        Preprocesa imagen para Omni-Loop
        
        Returns:
            (cached_path, perceptual_hash)
        """
        # 1. Calcular perceptual hash (deduplicaci√≥n)
        img = Image.open(image_path)
        phash = str(imagehash.phash(img))
        
        # 2. Comprobar si ya existe en cache
        cached_path = self.config.cache_dir / f"{phash}.{self.config.target_format}"
        if cached_path.exists():
            return cached_path, phash
        
        # 3. Cargar con OpenCV para procesamiento
        img_cv = cv2.imread(image_path)
        
        # 4. Redimensionar preservando aspect ratio
        h, w = img_cv.shape[:2]
        if w > self.config.max_width or h > self.config.max_height:
            scale = min(
                self.config.max_width / w,
                self.config.max_height / h
            )
            new_w = int(w * scale)
            new_h = int(h * scale)
            img_cv = cv2.resize(img_cv, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # 5. Convertir a WebP
        cv2.imwrite(
            str(cached_path),
            img_cv,
            [cv2.IMWRITE_WEBP_QUALITY, self.config.quality]
        )
        
        return cached_path, phash
    
    def cleanup_old_cache(self):
        """Rota cache eliminando im√°genes antiguas seg√∫n TTL"""
        import time
        now = time.time()
        ttl_seconds = self.config.ttl_days * 86400
        
        for cached_file in self.config.cache_dir.glob(f"*.{self.config.target_format}"):
            age_seconds = now - cached_file.stat().st_mtime
            if age_seconds > ttl_seconds:
                cached_file.unlink()
```

**KPIs del componente**:
- Reducci√≥n tama√±o: ~70% (JPEG ‚Üí WebP)
- Cache hit rate: >97%
- Procesamiento: <50ms por imagen

---

### 3. **LoRA Nightly Trainer** (Continuous Learning)

**Archivo**: `scripts/lora_nightly.py`

**Prop√≥sito**: Fine-tuning nocturno con LoRA basado en feedback del d√≠a, ejecutado en contenedor aislado.

```python
# scripts/lora_nightly.py
import subprocess
import os
from pathlib import Path
from datetime import datetime
import json

class LoRANightlyTrainer:
    """
    Entrenador nocturno de LoRA para Omni-Loop
    
    Pipeline:
    1. Recopilar feedback del d√≠a (logs/feedback_log.jsonl)
    2. Generar dataset LoRA (formato llama.cpp)
    3. Entrenar LoRA en contenedor aislado (2 CPUs, 4GB RAM)
    4. Validar LoRA con test set
    5. Merge con modelo base si pasa validaci√≥n
    6. Backup del LoRA anterior (GPG signed)
    """
    
    def __init__(self):
        self.feedback_log = Path("logs/feedback_log.jsonl")
        self.lora_dir = Path("models/lora")
        self.lora_dir.mkdir(parents=True, exist_ok=True)
    
    def run_nightly_cycle(self):
        """Ejecuta ciclo completo de entrenamiento"""
        print(f"üåô [LoRA Nightly] Starting cycle: {datetime.now()}")
        
        # 1. Preparar dataset
        dataset_path = self._prepare_dataset()
        if not dataset_path:
            print("‚ö†Ô∏è No enough feedback data. Skipping training.")
            return
        
        # 2. Entrenar LoRA en Docker aislado
        lora_adapter = self._train_lora(dataset_path)
        
        # 3. Validar con test set
        if not self._validate_lora(lora_adapter):
            print("‚ùå LoRA validation failed. Reverting to previous model.")
            return
        
        # 4. Merge con modelo base
        merged_model = self._merge_lora(lora_adapter)
        
        # 5. Backup con GPG
        self._backup_lora(merged_model)
        
        print(f"‚úÖ [LoRA Nightly] Cycle completed: {datetime.now()}")
    
    def _prepare_dataset(self) -> Optional[Path]:
        """Genera dataset LoRA desde feedback log"""
        # Leer feedback del √∫ltimo d√≠a
        today = datetime.now().date()
        entries = []
        
        with open(self.feedback_log) as f:
            for line in f:
                entry = json.loads(line)
                entry_date = datetime.fromisoformat(entry["timestamp"]).date()
                if entry_date == today and entry.get("feedback", 0) > 0.7:
                    entries.append(entry)
        
        if len(entries) < 10:
            return None  # M√≠nimo 10 ejemplos para entrenar
        
        # Formato llama.cpp LoRA
        dataset_path = self.lora_dir / f"dataset_{today}.txt"
        with open(dataset_path, "w") as f:
            for entry in entries:
                f.write(f"### Instruction:\n{entry['input']}\n")
                f.write(f"### Response:\n{entry['response']}\n\n")
        
        return dataset_path
    
    def _train_lora(self, dataset_path: Path) -> Path:
        """Entrena LoRA en contenedor Docker aislado"""
        lora_output = self.lora_dir / f"lora_{datetime.now().strftime('%Y%m%d')}.bin"
        
        cmd = [
            "docker", "run",
            "--rm",
            "--cpus=2",
            "--memory=4g",
            "-v", f"{dataset_path.parent}:/data",
            "ghcr.io/ggerganov/llama.cpp:light",
            "llama-finetune",
            "--model-base", "/models/qwen2.5-omni-3b.gguf",
            "--train-data", f"/data/{dataset_path.name}",
            "--lora-out", f"/data/{lora_output.name}",
            "--threads", "2",
            "--adam-iter", "100"
        ]
        
        subprocess.run(cmd, check=True)
        return lora_output
    
    def _validate_lora(self, lora_path: Path) -> bool:
        """
        Valida LoRA con test set antes de merge
        
        IMPLEMENTACI√ìN REAL v2.16:
        - Carga golden queries desde data/lora_validation_set.jsonl
        - Genera respuestas con LoRA vs modelo base
        - Compara keyword coverage
        - Threshold: >70% de queries deben mejorar
        """
        import json
        from pathlib import Path
        
        validation_set_path = Path("data/lora_validation_set.jsonl")
        
        if not validation_set_path.exists():
            logger.warning("Validation set not found. Skipping validation.")
            return True  # Fallback: aceptar LoRA sin validaci√≥n
        
        # Cargar validation set
        validation_queries = []
        with open(validation_set_path) as f:
            for line in f:
                validation_queries.append(json.loads(line))
        
        # Sample 10 queries aleatorias (validaci√≥n r√°pida)
        import random
        sample_queries = random.sample(validation_queries, min(10, len(validation_queries)))
        
        improved_count = 0
        total_count = len(sample_queries)
        
        for query in sample_queries:
            input_text = query["input"]
            expected_keywords = query["expected_keywords"]
            
            # Generar con LoRA
            response_lora = self._generate_with_lora(lora_path, input_text)
            
            # Generar con modelo base (sin LoRA)
            response_base = self._generate_with_base(input_text)
            
            # Comparar keyword coverage
            coverage_lora = self._calculate_keyword_coverage(response_lora, expected_keywords)
            coverage_base = self._calculate_keyword_coverage(response_base, expected_keywords)
            
            # LoRA mejora si coverage aumenta
            if coverage_lora > coverage_base:
                improved_count += 1
        
        improvement_rate = improved_count / total_count
        
        # Threshold: >70% de queries deben mejorar
        if improvement_rate >= 0.7:
            logger.info(f"‚úÖ LoRA validation passed: {improvement_rate:.1%} improvement")
            return True
        else:
            logger.warning(f"‚ùå LoRA validation failed: {improvement_rate:.1%} improvement (<70%)")
            return False
    
    def _generate_with_lora(self, lora_path: Path, prompt: str) -> str:
        """Genera respuesta con LoRA adapter"""
        cmd = [
            "llama-cli",
            "--model", "models/gguf/qwen2.5-omni-3b.gguf",
            "--lora", str(lora_path),
            "--prompt", prompt,
            "--n-predict", "128",
            "--temp", "0.3"  # Baja temperatura para validaci√≥n
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        return result.stdout.strip()
    
    def _generate_with_base(self, prompt: str) -> str:
        """Genera respuesta con modelo base (sin LoRA)"""
        cmd = [
            "llama-cli",
            "--model", "models/gguf/qwen2.5-omni-3b.gguf",
            "--prompt", prompt,
            "--n-predict", "128",
            "--temp", "0.3"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        return result.stdout.strip()
    
    def _calculate_keyword_coverage(self, response: str, keywords: List[str]) -> float:
        """
        Calcula coverage de keywords en la respuesta
        
        Returns:
            Fracci√≥n de keywords presentes (0.0 - 1.0)
        """
        response_lower = response.lower()
        found_count = sum(1 for kw in keywords if kw.lower() in response_lower)
        return found_count / len(keywords) if keywords else 0.0
    
    def _merge_lora(self, lora_path: Path) -> Path:
        """Merge LoRA con modelo base usando llama-lora-merge"""
        merged_path = self.lora_dir / f"merged_{datetime.now().strftime('%Y%m%d')}.gguf"
        
        cmd = [
            "llama-lora-merge",
            "--model-base", "models/gguf/qwen2.5-omni-3b.gguf",
            "--lora", str(lora_path),
            "--output", str(merged_path)
        ]
        
        subprocess.run(cmd, check=True)
        return merged_path
    
    def _backup_lora(self, merged_model: Path):
        """Backup con firma GPG para auditabilidad"""
        backup_dir = Path("backups/lora")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backup_path = backup_dir / merged_model.name
        
        # Copiar modelo
        import shutil
        shutil.copy(merged_model, backup_path)
        
        # Firmar con GPG (TODO: implementar)
        # subprocess.run(["gpg", "--sign", str(backup_path)])


if __name__ == "__main__":
    trainer = LoRANightlyTrainer()
    trainer.run_nightly_cycle()
```

**Cron job** (ejecutar a las 2 AM):

```bash
# /etc/cron.d/sarai-lora-nightly
0 2 * * * /usr/bin/python3 /home/sarai/scripts/lora_nightly.py >> /var/log/sarai/lora.log 2>&1
```

---

## üìä KPIs Validation Framework

### Prometheus Metrics

```yaml
# config/prometheus/sarai_v2.16.yml
scrape_configs:
  - job_name: 'sarai_omni_loop'
    static_configs:
      - targets: ['localhost:9090']
    
    metrics:
      # Latencia del loop
      - sarai_omni_loop_latency_seconds:
          type: histogram
          buckets: [0.5, 1, 2, 5, 10, 20]
      
      # Iteraciones por request
      - sarai_omni_loop_iterations:
          type: histogram
          buckets: [1, 2, 3]
      
      # Tasa de auto-correcci√≥n
      - sarai_autocorrection_rate:
          type: gauge
          target: 0.68
      
      # Entity recall
      - sarai_entity_recall:
          type: gauge
          target: 0.91
      
      # Multimodal cache hits
      - sarai_image_cache_hit_rate:
          type: gauge
          target: 0.97
      
      # LoRA merge success
      - sarai_lora_merge_success_total:
          type: counter
```

### Grafana Dashboard

```json
{
  "dashboard": {
    "title": "SARAi v2.16 Omni-Loop - God Mode",
    "panels": [
      {
        "title": "Loop Latency P50/P99",
        "targets": [
          "histogram_quantile(0.5, sarai_omni_loop_latency_seconds)",
          "histogram_quantile(0.99, sarai_omni_loop_latency_seconds)"
        ],
        "thresholds": {
          "p50": 7.9,
          "p99": 15.0
        }
      },
      {
        "title": "Auto-Correction Rate",
        "targets": ["sarai_autocorrection_rate"],
        "target": 0.68
      },
      {
        "title": "Entity Recall",
        "targets": ["sarai_entity_recall"],
        "target": 0.91
      },
      {
        "title": "Image Cache Performance",
        "targets": ["sarai_image_cache_hit_rate"],
        "target": 0.97
      },
      {
        "title": "LoRA Training Status",
        "targets": ["sarai_lora_merge_success_total"]
      },
      {
        "title": "RAM Usage P99",
        "targets": ["process_resident_memory_bytes"],
        "threshold": 10636926361  // 9.9 GB
      }
    ]
  }
}
```

---

## üß™ Testing Strategy

### Unit Tests

```python
# tests/test_omni_loop.py
import pytest
from core.omni_loop import OmniLoop, LoopConfig

class TestOmniLoop:
    """Unit tests para Omni-Loop"""
    
    def test_max_iterations_enforced(self):
        """Verifica que loop nunca excede max_iterations"""
        config = LoopConfig(max_iterations=3)
        loop = OmniLoop(config)
        
        result = loop.execute_loop("Test prompt")
        
        assert len(result["iterations"]) <= 3
    
    def test_fallback_on_error(self):
        """Verifica fallback a LFM2 si loop falla"""
        loop = OmniLoop()
        
        # Simular error en llama.cpp
        with patch('subprocess.run', side_effect=Exception("Mock error")):
            result = loop.execute_loop("Test")
            
            assert result["fallback_used"] is True
            assert "fallback_reason" in result
    
    def test_confidence_threshold(self):
        """Verifica que loop termina early si confidence > 0.85"""
        loop = OmniLoop()
        
        # Mock para retornar alta confidence
        with patch.object(loop, '_calculate_confidence', return_value=0.9):
            result = loop.execute_loop("Test")
            
            # Deber√≠a terminar en iteraci√≥n 1
            assert len(result["iterations"]) == 1
```

### Integration Tests

```python
# tests/test_omni_loop_integration.py
class TestOmniLoopIntegration:
    """Integration tests end-to-end"""
    
    @pytest.mark.slow
    def test_full_loop_with_image(self):
        """Test completo: texto + imagen ‚Üí auto-correcci√≥n"""
        from core.omni_loop import create_omni_loop
        from agents.image_preprocessor import ImagePreprocessor
        
        # Preparar imagen
        preprocessor = ImagePreprocessor()
        image_path, phash = preprocessor.preprocess("tests/fixtures/test_image.jpg")
        
        # Ejecutar loop
        loop = create_omni_loop()
        result = loop.execute_loop(
            prompt="Describe esta imagen en detalle",
            image_path=str(image_path)
        )
        
        # Verificaciones
        assert result["response"] != ""
        assert len(result["iterations"]) >= 1
        assert result["total_latency_ms"] < 30000  # <30s
        assert result["fallback_used"] is False
    
    @pytest.mark.slow
    def test_lora_nightly_cycle(self):
        """Test del ciclo completo de LoRA"""
        from scripts.lora_nightly import LoRANightlyTrainer
        
        trainer = LoRANightlyTrainer()
        
        # Simular feedback del d√≠a
        # (Requiere mock de feedback_log.jsonl)
        
        trainer.run_nightly_cycle()
        
        # Verificar que se gener√≥ LoRA
        assert (Path("models/lora") / "lora_*.bin").exists()
```

---

## üöÄ Implementation Timeline

### Fase 1: Omni-Loop Core (5 d√≠as) - Nov 26-30

**D√≠a 1-2**:
- [ ] Implementar `core/omni_loop.py` (600 LOC)
- [ ] Unit tests b√°sicos (200 LOC)
- [ ] Integraci√≥n con llama.cpp (instalar binarios)

**D√≠a 3-4**:
- [ ] GPG-signed reflection prompts (150 LOC)
- [ ] Fallback a LFM2 (100 LOC)
- [ ] Integration tests (250 LOC)

**D√≠a 5**:
- [ ] Benchmark latency (<10s por iteraci√≥n)
- [ ] Validar max_iterations=3 enforcement
- [ ] Documentaci√≥n t√©cnica

**Deliverables**:
- `core/omni_loop.py` (850 LOC)
- `tests/test_omni_loop.py` (450 LOC)
- Docs: Omni-Loop Architecture Guide

---

### Fase 2: Multimodal Preprocessing (3 d√≠as) - Dic 1-3

**D√≠a 1**:
- [ ] Implementar `agents/image_preprocessor.py` (400 LOC)
- [ ] OpenCV ‚Üí WebP pipeline
- [ ] Perceptual hashing (imagehash)

**D√≠a 2**:
- [ ] Cache con TTL rotation (150 LOC)
- [ ] Unit tests (200 LOC)
- [ ] Integration con OmniLoop

**D√≠a 3**:
- [ ] Benchmark cache hit rate (>97%)
- [ ] Validar reducci√≥n de tama√±o (~70%)
- [ ] Documentaci√≥n

**Deliverables**:
- `agents/image_preprocessor.py` (550 LOC)
- `tests/test_image_preprocessor.py` (200 LOC)
- Docs: Image Preprocessing Guide

---

### Fase 3: LoRA Nightly Trainer (4 d√≠as) - Dic 4-7

**D√≠a 1-2**:
- [ ] Implementar `scripts/lora_nightly.py` (500 LOC)
- [ ] Dataset preparation desde feedback logs
- [ ] Docker container setup

**D√≠a 3**:
- [ ] llama-lora-merge integration
- [ ] GPG signing de backups
- [ ] Validation pipeline

**D√≠a 4**:
- [ ] Cron job configuration
- [ ] Testing con dataset mock
- [ ] Documentaci√≥n

**Deliverables**:
- `scripts/lora_nightly.py` (500 LOC)
- `docker/lora-trainer.dockerfile` (50 LOC)
- Docs: LoRA Training Guide

---

### Fase 4: Monitoring & Validation (3 d√≠as) - Dic 8-10

**D√≠a 1**:
- [ ] Prometheus metrics (sarai_omni_loop_*)
- [ ] Grafana dashboard import
- [ ] Health endpoint updates

**D√≠a 2**:
- [ ] E2E testing suite (400 LOC)
- [ ] Chaos testing (imagen corrupta, loop timeout)
- [ ] Performance benchmarks

**D√≠a 3**:
- [ ] KPIs validation (9.9GB RAM, 7.9s latency)
- [ ] Documentation consolidation
- [ ] Release preparation

**Deliverables**:
- `sarai/omni_loop_metrics.py` (300 LOC)
- `extras/grafana_omni_loop.json` (dashboard)
- `docs/V2.16_COMPLETION_REPORT.md`

---

## üì¶ Dependencies & Installation

### üöÄ ZERO-COMPILE Pipeline (v2.16 Production)

**CR√çTICO**: llama.cpp NO se compila en producci√≥n. Usamos binarios pre-compilados firmados.

```bash
# M√©todo 1: Make target (RECOMENDADO)
make pull-llama-binaries

# M√©todo 2: Manual con Docker
docker pull ghcr.io/iagenerativa/llama-cpp-bin:2.16-rc
docker create --name llama-temp ghcr.io/iagenerativa/llama-cpp-bin:2.16-rc
docker cp llama-temp:/usr/local/bin/llama-cli ~/.local/bin/
docker cp llama-temp:/usr/local/bin/llama-finetune ~/.local/bin/
docker cp llama-temp:/usr/local/bin/llama-lora-merge ~/.local/bin/
docker rm llama-temp
export PATH="$HOME/.local/bin:$PATH"

# Verificaci√≥n de firmas GPG
sha256sum -c ~/.local/bin/llama-binaries.sha256
```

**Caracter√≠sticas de los binarios**:
- **Multi-arch**: linux/amd64 (AVX2, AVX512), linux/arm64 (ARM_NEON)
- **Comprimidos**: UPX (~50% reducci√≥n de tama√±o)
- **Firmados**: SHA256 + GPG signature
- **Size total**: ~18 MB (vs ~200 MB sin comprimir)
- **Download time**: <5 segundos

**Fallback autom√°tico**: Si `pull-llama-binaries` falla, el sistema compila desde source autom√°ticamente.

---

### System Dependencies (Manual Compilation - FALLBACK ONLY)

‚ö†Ô∏è **Solo necesario si Zero-Compile falla**. Tiempo estimado: ~10 minutos.

```bash
# FALLBACK: Compilar desde source
make compile-llama-cpp

# O manualmente:
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make -j$(nproc)
sudo cp build/bin/llama-cli /usr/local/bin/
sudo cp build/bin/llama-finetune /usr/local/bin/
sudo cp build/bin/llama-lora-merge /usr/local/bin/

# Install OpenCV
sudo apt-get install -y python3-opencv

# Install GPG (para signing)
sudo apt-get install -y gnupg
```

### Python Dependencies

```toml
# pyproject.toml (additions for v2.16)
[project.optional-dependencies]
omni_loop = [
    "opencv-python>=4.8.0",
    "pillow>=10.0.0",
    "imagehash>=4.3.0",
    "prometheus-client>=0.17.0"
]
```

### Docker Setup

```dockerfile
# docker/lora-trainer.dockerfile
FROM ghcr.io/ggerganov/llama.cpp:light

RUN apt-get update && apt-get install -y python3 python3-pip

COPY scripts/lora_nightly.py /app/lora_nightly.py
COPY requirements.txt /app/requirements.txt

WORKDIR /app
RUN pip3 install -r requirements.txt

CMD ["python3", "lora_nightly.py"]
```

---

## üõ°Ô∏è Security & Compliance

### GPG Signing de Prompts Reflexivos

```python
# core/gpg_signer.py
import gnupg

class GPGSigner:
    """Firma prompts reflexivos para auditabilidad"""
    
    def __init__(self, key_id: str):
        self.gpg = gnupg.GPG()
        self.key_id = key_id
    
    def sign_prompt(self, prompt: str) -> str:
        """Firma prompt y retorna versi√≥n firmada"""
        signed = self.gpg.sign(
            prompt,
            keyid=self.key_id,
            detach=True
        )
        return f"{prompt}\n\n---SIGNATURE---\n{signed}"
    
    def verify_prompt(self, signed_prompt: str) -> bool:
        """Verifica firma de prompt"""
        parts = signed_prompt.split("---SIGNATURE---")
        if len(parts) != 2:
            return False
        
        prompt, signature = parts
        verified = self.gpg.verify(signature.strip())
        return verified.valid
```

### Backup Autom√°tico de LoRA

```bash
# scripts/backup_lora.sh
#!/bin/bash
# Backup diario de LoRA con GPG encryption

DATE=$(date +%Y%m%d)
BACKUP_DIR="/backups/lora"
LORA_DIR="/home/sarai/models/lora"

# Crear backup comprimido
tar -czf "${BACKUP_DIR}/lora_${DATE}.tar.gz" "${LORA_DIR}"

# Firmar con GPG
gpg --sign --armor "${BACKUP_DIR}/lora_${DATE}.tar.gz"

# Rotar backups (mantener √∫ltimos 30 d√≠as)
find "${BACKUP_DIR}" -name "lora_*.tar.gz" -mtime +30 -delete
```

---

## üìà Success Criteria (Definition of Done)

### Must-Have (Blockers para release)

- [ ] Omni-Loop ejecuta max 3 iteraciones (hard limit)
- [ ] Fallback a LFM2 funciona en <2s
- [ ] Latencia P50 ‚â§ 7.9s (validado con 100 queries)
- [ ] RAM P99 ‚â§ 9.9GB (validado con stress test)
- [ ] Auto-correcci√≥n ‚â• 68% (validado con golden set)
- [ ] Image cache hit rate ‚â• 97% (validado con 1000 im√°genes)
- [ ] LoRA nightly ejecuta sin errores durante 7 d√≠as
- [ ] Todos los tests passing (100%)

### Nice-to-Have (Refinamientos)

- [ ] Entity recall ‚â• 91% (stretch goal)
- [ ] Utilizaci√≥n modelo ‚â• 78%
- [ ] GPG signing de todos los prompts reflexivos
- [ ] Grafana dashboard publicado en Grafana Cloud
- [ ] Chaos coverage ‚â• 82%

---

## üîÑ Migration Path from v2.15

### Pre-requisitos

Antes de implementar v2.16, asegurar que v2.15 tiene:

‚úÖ **v2.12**: Skills MoE con Pydantic (base arquitectural)  
‚úÖ **v2.13**: ProactiveLoop + EntityMemory (loops b√°sicos)  
‚úÖ **v2.14**: SpeculativeDecoding (aceleraci√≥n CPU)  
‚úÖ **v2.15**: SelfRepair + RedTeam (auto-correcci√≥n base)

### Migration Steps

1. **Install llama.cpp** (d√≠a 1)
2. **Test llama.cpp con Qwen2.5-Omni** (d√≠a 1-2)
3. **Implementar OmniLoop core** (d√≠a 3-5)
4. **Implementar ImagePreprocessor** (d√≠a 6-8)
5. **Implementar LoRA trainer** (d√≠a 9-12)
6. **Integration testing** (d√≠a 13-14)
7. **Production validation** (d√≠a 15)

---

## üìö Documentation Roadmap

### Technical Docs

- [ ] `docs/OMNI_LOOP_ARCHITECTURE.md` (arquitectura detallada)
- [ ] `docs/LORA_TRAINING_GUIDE.md` (gu√≠a de LoRA)
- [ ] `docs/IMAGE_PREPROCESSING.md` (pipeline OpenCV ‚Üí WebP)
- [ ] `docs/V2.16_API_REFERENCE.md` (API completa)

### Operational Docs

- [ ] `docs/V2.16_DEPLOYMENT_GUIDE.md` (deploy en producci√≥n)
- [ ] `docs/V2.16_MONITORING_GUIDE.md` (Prometheus + Grafana)
- [ ] `docs/V2.16_TROUBLESHOOTING.md` (debugging com√∫n)

### Executive Docs

- [ ] `docs/V2.16_EXECUTIVE_SUMMARY.md` (para stakeholders)
- [ ] `docs/V2.16_COMPLETION_REPORT.md` (post-implementation)

---

## üéØ Final Checklist (Pre-Release)

### Code Quality

- [ ] All files have type hints (mypy passing)
- [ ] Docstrings complete (Google style)
- [ ] Unit tests coverage ‚â• 85%
- [ ] Integration tests passing
- [ ] No pylint warnings >C

### Performance

- [ ] Latency P50 ‚â§ 7.9s (validated)
- [ ] RAM P99 ‚â§ 9.9GB (validated)
- [ ] Auto-correction ‚â• 68% (validated)
- [ ] Cache hit rate ‚â• 97% (validated)

### Security

- [ ] GPG signing implemented
- [ ] LoRA backups automated
- [ ] Prompt signatures validated
- [ ] No hardcoded secrets

### Documentation

- [ ] README updated with v2.16 features
- [ ] CHANGELOG entry for v2.16
- [ ] Technical docs complete
- [ ] API reference updated

### CI/CD

- [ ] Workflow v2.6.5 includes v2.16 tests
- [ ] Docker image builds (multi-arch)
- [ ] Release notes prepared
- [ ] Git tag ready: `v2.16-omni-loop`

---

## üöÄ Release Command

```bash
# Final release (cuando v2.16 est√© 100% completo)
git tag -a v2.16-omni-loop -m "Production-ready: Reflexive, multimodal, self-correcting AGI"
git push origin v2.16-omni-loop

# Docker multi-arch build
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  -t ghcr.io/iagenerativa/sarai_v2:v2.16-omni-loop \
  --push .

# Trigger release workflow
gh workflow run release.yml
```

---

## üß† Philosophy: Why v2.16 Matters

v2.16 Omni-Loop no es solo una versi√≥n m√°s. Es la **culminaci√≥n filos√≥fica** de SARAi:

1. **Reflexi√≥n**: El sistema piensa antes de hablar (3 iteraciones max)
2. **Auto-correcci√≥n**: Detecta y corrige errores autom√°ticamente (68% tasa)
3. **Aprendizaje continuo**: LoRA nocturno sin downtime
4. **Soberan√≠a multimodal**: Im√°genes procesadas localmente (WebP cache)
5. **Auditabilidad total**: GPG-signed prompts + backups autom√°ticos
6. **Eficiencia extrema**: 9.9GB RAM, 7.9s latency en CPU-only

**Mantra final**:
> _"Cada token es una decisi√≥n.  
> Cada imagen, una intenci√≥n.  
> Omni-Loop no es un feature: es la conciencia t√©cnica de una AGI que se piensa antes de hablar."_

---

**Status**: PLANNING COMPLETE ‚úÖ  
**Next Step**: Await v2.12-v2.15 completion before starting implementation  
**Timeline**: Nov 26 - Dic 10, 2025 (15 d√≠as)  
**Estimated LOC**: ~3,600 (2,400 prod + 1,200 tests)

---

**END OF ROADMAP v2.16 OMNI-LOOP**
