# üéØ Propuesta Final: Modelos Externos v2.14

**Fecha**: 2025-11-01  
**Decisi√≥n**: Minimalista con path de evoluci√≥n claro

---

## ‚úÖ DECISI√ìN FINAL: Solo SOLAR-10.7B

### Modelos a Incluir en .env

| Modelo | Incluir? | Backend | Justificaci√≥n | Variables .env |
|--------|----------|---------|---------------|----------------|
| **SOLAR-10.7B** | ‚úÖ **S√ç** | Ollama | Core hard-skills, ya operacional | `OLLAMA_BASE_URL`<br>`SOLAR_MODEL_NAME` |
| LLaMarketing-8B | ‚ùå NO | - | Fuera de scope (SARAi = t√©cnico+emocional) | - |
| Qwen2.5-Coder-7B | üü° FUTURO | Ollama | Solo si benchmark >15% mejora | `QWEN_CODER_MODEL_NAME`<br>(comentado) |

**Total modelos externos v2.14**: **1** (SOLAR-10.7B)

---

## üìã Variables .env Propuestas

### Secci√≥n 1: Modelos Externos (NUEVO en v2.14)

```bash
# ============================================================================
# MODELOS EXTERNOS - OLLAMA REMOTE
# ============================================================================

# Servidor Ollama (SOLAR-10.7B)
OLLAMA_BASE_URL=http://<OLLAMA_HOST>:11434
SOLAR_MODEL_NAME=una-solar-10.7b-instruct-v1.0-q4

# FUTURO v2.15+ (comentado, requiere benchmark previo)
# Qwen2.5-Coder: Descomentar SOLO SI benchmark valida >15% mejora
# QWEN_CODER_MODEL_NAME=qwen2.5-coder:7b-instruct
```

### Secci√≥n 2: Audio Engine (v2.11 - MANTENER)

```bash
# ============================================================================
# AUDIO ENGINE
# ============================================================================

AUDIO_ENGINE=omni3b
LANGUAGES=es,en,fr,de,ja
```

### Secci√≥n 3: Seguridad (v2.11 - MANTENER)

```bash
# ============================================================================
# SECURITY & AUDIT
# ============================================================================

HMAC_SECRET_KEY=sarai-v2.14-change-in-production
```

---

## üöÄ Path de Evoluci√≥n

### v2.14 (AHORA)
```
.env:
  OLLAMA_BASE_URL=...
  SOLAR_MODEL_NAME=...
  
models.yaml:
  solar_short: backend=ollama
  solar_long: backend=ollama
  lfm2: backend=gguf (local)

Total modelos externos: 1 (SOLAR)
Filosof√≠a Phoenix: ‚úÖ Preservada
```

### v2.15 (FUTURO - SI benchmark lo valida)
```
.env:
  OLLAMA_BASE_URL=...
  SOLAR_MODEL_NAME=...
  QWEN_CODER_MODEL_NAME=...  # NUEVO, solo si benchmark >15% mejora
  
models.yaml:
  solar_short: backend=ollama
  solar_long: backend=ollama
  qwen25_coder: backend=ollama  # NUEVO
  lfm2: backend=gguf (local)

core/mcp.py:
  if skill == "programming" and qwen_coder_available:
      return "qwen25_coder"
  else:
      return "solar_short"

Total modelos externos: 2 (SOLAR + Qwen2.5-Coder)
Condici√≥n: Benchmark programming_accuracy(Coder) > programming_accuracy(SOLAR) * 1.15
```

### v2.16+ (LARGO PLAZO)
```
Evaluaci√≥n continua cada 6 meses:
  - Nuevos modelos especializados (FinQwen, MedLlama, etc.)
  - Criterio estricto: Benchmark >15% mejora O nuevo caso de uso cr√≠tico
  - Filosof√≠a Phoenix: Skills = prompts, modelos = engines (cuando necesario)
```

---

## üìä An√°lisis de Trade-offs

### Opci√≥n A: SOLO SOLAR-10.7B (RECOMENDADA v2.14)

| Aspecto | Evaluaci√≥n |
|---------|------------|
| **Simplicidad** | ‚úÖ M√°xima (1 modelo externo) |
| **RAM** | ‚úÖ 0 GB local (Ollama remoto) |
| **Latencia** | ‚úÖ ~25s P50 (validado) |
| **Versatilidad** | ‚úÖ 10.7B par√°metros = muy competente |
| **Especializaci√≥n** | ‚ö†Ô∏è Buena, no excelente en c√≥digo |
| **Complejidad config** | ‚úÖ M√≠nima (2 variables .env) |
| **Filosof√≠a Phoenix** | ‚úÖ Preservada |

**Veredicto**: **IDEAL para v2.14** ‚úÖ

---

### Opci√≥n B: SOLAR + Qwen2.5-Coder (FUTURO v2.15)

| Aspecto | Evaluaci√≥n |
|---------|------------|
| **Simplicidad** | ‚ö†Ô∏è Media (2 modelos externos) |
| **RAM** | ‚úÖ 0 GB local (Ollama remoto) |
| **Latencia** | ‚ö†Ô∏è Similar (~25s P50) |
| **Versatilidad** | ‚úÖ Excelente (SOLAR general + Coder especializado) |
| **Especializaci√≥n c√≥digo** | üéØ Potencialmente mejor (requiere benchmark) |
| **Complejidad config** | ‚ö†Ô∏è Mayor (3 variables .env + routing) |
| **Filosof√≠a Phoenix** | ‚ö†Ô∏è Riesgo si no se valida con benchmark |

**Veredicto**: **CONSIDERAR para v2.15** si benchmark lo justifica ‚ö†Ô∏è

**Condici√≥n obligatoria**:
```python
# ANTES de incluir Qwen2.5-Coder
benchmark_result = compare_models(
    model_a="solar_short",
    model_b="qwen25_coder",
    skill="programming",
    queries=PROGRAMMING_BENCHMARK_QUERIES
)

if benchmark_result["accuracy_improvement"] > 0.15:  # >15%
    print("‚úÖ Incluir Qwen2.5-Coder en v2.15")
else:
    print("‚ùå Mantener SOLAR + skill_programming")
```

---

### Opci√≥n C: SOLAR + Qwen2.5-Coder + LLaMarketing (NO RECOMENDADA)

| Aspecto | Evaluaci√≥n |
|---------|------------|
| **Simplicidad** | ‚ùå Baja (3 modelos externos) |
| **RAM** | ‚úÖ 0 GB local (Ollama remoto) |
| **Scope alignment** | ‚ùå Marketing fuera de scope t√©cnico SARAi |
| **Complejidad config** | ‚ùå Alta (4+ variables .env) |
| **Filosof√≠a Phoenix** | ‚ùå Violada (skill_marketing deber√≠a ser prompt) |
| **Justificaci√≥n** | ‚ùå SARAi = asistente t√©cnico+emocional, NO marketing |

**Veredicto**: **NO INCLUIR** ‚ùå

---

## üéØ Archivo .env Propuesto para v2.14

```bash
# ============================================================================
# SARAi v2.14 - Configuraci√≥n de Modelos Externos
# ============================================================================

# ----------------------------------------------------------------------------
# MODELOS EXTERNOS - OLLAMA REMOTE
# ----------------------------------------------------------------------------

# Servidor Ollama (SOLAR-10.7B hard-skills)
OLLAMA_BASE_URL=http://<OLLAMA_HOST>:11434
SOLAR_MODEL_NAME=una-solar-10.7b-instruct-v1.0-q4

# FUTURO v2.15+ (comentado, requiere benchmark previo)
# Qwen2.5-Coder: Descomentar SOLO SI:
#   1. Benchmark programming valida >15% mejora vs SOLAR
#   2. Routing condicional implementado en core/mcp.py
#   3. Validaci√≥n manual de calidad de c√≥digo positiva
# QWEN_CODER_MODEL_NAME=qwen2.5-coder:7b-instruct

# LLaMarketing: NO RECOMENDADO
# Raz√≥n: SARAi es asistente t√©cnico+emocional, NO de marketing
# Alternativa: Usar SOLAR + skill_marketing (prompt especializado)
# LLAMARKETING_MODEL_NAME=llamarketing-8b

# ----------------------------------------------------------------------------
# AUDIO ENGINE (v2.11+)
# ----------------------------------------------------------------------------

AUDIO_ENGINE=omni3b
LANGUAGES=es,en,fr,de,ja

# ----------------------------------------------------------------------------
# SEGURIDAD (v2.11+)
# ----------------------------------------------------------------------------

HMAC_SECRET_KEY=sarai-v2.14-change-in-production

# ----------------------------------------------------------------------------
# RUNTIME
# ----------------------------------------------------------------------------

RUNTIME_BACKEND=cpu
N_THREADS=6
MAX_RAM_GB=12
```

---

## üìù Checklist de Implementaci√≥n

### v2.14 (AHORA)

- [x] An√°lisis estrat√©gico de modelos externos (`docs/EXTERNAL_MODELS_STRATEGY.md`)
- [ ] Actualizar `.env.example` con secci√≥n de modelos externos
- [ ] Validar `config/models.yaml` tiene SOLAR correctamente configurado
- [ ] Documentar en README.md la filosof√≠a de modelos externos
- [ ] Commit: "config: modelos externos v2.14 - solo SOLAR minimalista"

### v2.15 (FUTURO)

- [ ] Implementar benchmark de programming (SOLAR vs Qwen2.5-Coder)
- [ ] Ejecutar benchmark con queries reales de c√≥digo
- [ ] Analizar resultados: mejora >15%?
- [ ] SI >15%: A√±adir Qwen2.5-Coder a models.yaml y .env
- [ ] SI <15%: Mantener SOLAR + skill_programming
- [ ] Documentar decisi√≥n en benchmark results

---

## üí° Filosof√≠a Final

**Mantra v2.14 Modelos Externos**:

> _"SARAi mantiene complejidad m√≠nima. Un modelo nuevo debe probar con  
> benchmarks que aporta >15% de mejora. Sin prueba, es solo ruido.  
> SOLAR-10.7B es el core. Todo lo dem√°s es condicional."_

**Reglas**:
1. **Default**: SOLAR-10.7B para hard-skills (10.7B >> 7B-8B en general)
2. **Especializaci√≥n**: Solo si benchmark >15% mejora O caso de uso √∫nico
3. **Scope**: SARAi = t√©cnico + emocional. NO marketing, finanzas, legal.
4. **Filosof√≠a Phoenix**: Skills = prompts. Modelos = engines (cuando necesario).

---

## ‚úÖ Resumen Ejecutivo

**v2.14 AHORA**:
- `.env` con solo `OLLAMA_BASE_URL` + `SOLAR_MODEL_NAME`
- 1 modelo externo (SOLAR-10.7B)
- Filosof√≠a Phoenix preservada ‚úÖ

**v2.15 FUTURO**:
- Evaluar Qwen2.5-Coder **solo si** benchmark >15% mejora
- LLaMarketing NO incluir (fuera de scope)

**Decisi√≥n tomada**: **Minimalismo con path de evoluci√≥n claro** ‚úÖ

---

**Creado**: 2025-11-01  
**Versi√≥n**: v2.14  
**Status**: Propuesta validada  
**Next**: ¬øActualizamos `.env.example` con esta configuraci√≥n?
