# Unified Model Wrapper - GuÃ­a Completa v2.14

**Fecha**: 1 de noviembre de 2025  
**VersiÃ³n**: v2.14  
**Estado**: âœ… PRODUCCIÃ“N

---

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [FilosofÃ­a](#filosofÃ­a)
3. [Arquitectura](#arquitectura)
4. [Backends Soportados](#backends-soportados)
5. [Uso BÃ¡sico](#uso-bÃ¡sico)
6. [ConfiguraciÃ³n](#configuraciÃ³n)
7. [Ejemplos Avanzados](#ejemplos-avanzados)
8. [API Reference](#api-reference)
9. [Troubleshooting](#troubleshooting)
10. [MigraciÃ³n desde model_pool](#migraciÃ³n-desde-model_pool)

---

## IntroducciÃ³n

El **Unified Model Wrapper** es la capa de abstracciÃ³n universal para todos los modelos de SARAi v2.14. Permite usar cualquier modelo (GGUF, Transformers, Multimodal, APIs) con una interfaz Ãºnica basada en LangChain.

### Â¿Por quÃ© Unified Wrapper?

**Antes v2.13** (model_pool):
```python
# CÃ³digo especÃ­fico por backend
from llama_cpp import Llama
solar = Llama(model_path="...", n_ctx=512, ...)

from transformers import AutoModelForCausalLM
lfm2 = AutoModelForCausalLM.from_pretrained("...", load_in_4bit=True, ...)
```

**DespuÃ©s v2.14** (Unified Wrapper):
```python
# Una interfaz para TODOS los modelos
from core.unified_model_wrapper import get_model

solar = get_model("solar_short")  # GGUF o Ollama
lfm2 = get_model("lfm2")           # GGUF local
embeddings = get_model("embeddings")  # Embeddings
qwen = get_model("qwen3_vl")       # Multimodal

# TODOS usan la misma API
response = solar.invoke("Â¿QuÃ© es Python?")
```

---

## FilosofÃ­a

### Principio Fundamental

> **"SARAi no debe conocer sus modelos. Solo debe invocar capacidades."**

- **YAML define** â†’ `config/models.yaml` es la Ãºnica fuente de verdad
- **LangChain orquesta** â†’ Pipelines LCEL declarativos
- **Wrapper abstrae** â†’ Backends intercambiables sin cambiar cÃ³digo

### Ventajas

| Aspecto | Antes (model_pool) | DespuÃ©s (Unified Wrapper) |
|---------|-------------------|---------------------------|
| **Agregar modelo** | Modificar cÃ³digo Python | Solo editar YAML |
| **Cambiar backend** | Reescribir lÃ³gica | Cambiar 1 lÃ­nea en YAML |
| **Testing** | Mocks complejos | IntegraciÃ³n real |
| **MigraciÃ³n GPU** | Reescribir todo | Cambiar `backend: "gguf"` â†’ `backend: "transformers"` |
| **APIs cloud** | CÃ³digo custom | `backend: "openai_api"` |

---

## Arquitectura

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              config/models.yaml                     â”‚
â”‚           (Fuente Ãºnica de verdad)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ModelRegistry (Factory)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  GGUF  â”‚Transform â”‚Multimodalâ”‚  Ollama  â”‚       â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚  â”‚OpenAI  â”‚Embedding â”‚ PyTorch  â”‚  Config  â”‚       â”‚
â”‚  â”‚  API   â”‚(Vectors) â”‚Checkpointâ”‚ (System) â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        UnifiedModelWrapper (Interface)              â”‚
â”‚         LangChain Runnable + LCEL                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Clases Principales

1. **`UnifiedModelWrapper`** (Clase base abstracta)
   - Interfaz: LangChain `Runnable`
   - MÃ©todos: `invoke()`, `ainvoke()`, `stream()`, `batch()`
   - Lifecycle: `_load_model()`, `unload()`, `_ensure_loaded()`

2. **`ModelRegistry`** (Factory + Singleton)
   - Carga `config/models.yaml`
   - Crea wrappers segÃºn backend
   - Cache automÃ¡tico (misma instancia si re-solicitado)

3. **Backend Wrappers** (Implementaciones especÃ­ficas)
   - `GGUFModelWrapper`: llama-cpp-python
   - `TransformersModelWrapper`: HuggingFace 4-bit
   - `MultimodalModelWrapper`: Qwen3-VL, Qwen-Omni
   - `OllamaModelWrapper`: API local
   - `OpenAIAPIWrapper`: GPT-4, Claude, Gemini
   - `EmbeddingModelWrapper`: EmbeddingGemma-300M
   - `PyTorchCheckpointWrapper`: TRM, MCP (futuro)

---

## Backends Soportados

### 1. GGUF (CPU Optimizado)

**Uso**: Modelos cuantizados para CPU con `llama-cpp-python`

```yaml
# config/models.yaml
lfm2:
  name: "LiquidAI-LFM2-1.2B"
  backend: "gguf"
  model_path: "models/cache/lfm2/lfm2-1.2b.Q4_K_M.gguf"
  n_ctx: 2048
  n_threads: 6
  temperature: 0.8
```

```python
# Uso
lfm2 = get_model("lfm2")
response = lfm2.invoke("Explica Python en 50 palabras")
```

### 2. Transformers (GPU con 4-bit)

**Uso**: Modelos HuggingFace con cuantizaciÃ³n automÃ¡tica

```yaml
solar_gpu:
  name: "SOLAR-10.7B (GPU)"
  backend: "transformers"
  repo_id: "upstage/SOLAR-10.7B-Instruct-v1.0"
  load_in_4bit: true
  device_map: "auto"
```

```python
solar = get_model("solar_gpu")
response = solar.invoke("Â¿CÃ³mo funciona la IA?")
```

### 3. Multimodal (VisiÃ³n + Audio)

**Uso**: Modelos Qwen para imagen y audio

```yaml
qwen3_vl:
  name: "Qwen3-VL-4B-Instruct"
  backend: "multimodal"
  repo_id: "Qwen/Qwen3-VL-4B-Instruct"
  supports_images: true
  supports_video: true
```

```python
qwen = get_model("qwen3_vl")

# Procesar imagen
response = qwen.invoke({
    "text": "Describe esta imagen",
    "image": "path/to/image.jpg"
})
```

### 4. Ollama (API Local)

**Uso**: Modelos servidos por Ollama

```yaml
solar_short:
  name: "SOLAR-10.7B (Ollama)"
  backend: "ollama"
  api_url: "${OLLAMA_BASE_URL}"  # Resuelve env var
  model_name: "hf.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF:Q4_K_M"
```

```python
solar = get_model("solar_short")
response = solar.invoke("Hola, Â¿cÃ³mo estÃ¡s?")
```

### 5. OpenAI API (Cloud)

**Uso**: GPT-4, Claude, Gemini

```yaml
gpt4:
  name: "GPT-4 Turbo"
  backend: "openai_api"
  api_url: "https://api.openai.com/v1"
  api_key: "${OPENAI_API_KEY}"
  model_name: "gpt-4-turbo-preview"
```

```python
gpt4 = get_model("gpt4")
response = gpt4.invoke("Explica cuÃ¡ntica")
```

### 6. Embedding (Vectores SemÃ¡nticos)

**Uso**: EmbeddingGemma-300M para clasificaciÃ³n

```yaml
embeddings:
  name: "EmbeddingGemma-300M"
  backend: "embedding"
  repo_id: "google/embeddinggemma-300m-qat-q4_0-unquantized"
  embedding_dim: 768
```

```python
embeddings = get_model("embeddings")

# Single embedding
vector = embeddings.invoke("SARAi es una AGI local")
print(vector.shape)  # (768,)

# Batch
vectors = embeddings.batch_encode(["texto1", "texto2", "texto3"])
print(vectors.shape)  # (3, 768)
```

### 7. PyTorch Checkpoint (Sistema Interno)

**Uso**: TRM, MCP (componentes internos)

```yaml
trm_classifier:
  name: "TRM-Dual-7M"
  backend: "pytorch_checkpoint"
  checkpoint_path: "models/trm_classifier/checkpoint.pth"
  device: "cpu"
```

### 8. Config (Configuraciones)

**Uso**: No son modelos, solo configuraciÃ³n

```yaml
legacy_mappings:
  backend: "config"
  expert: solar_long
  tiny: lfm2
```

---

## Uso BÃ¡sico

### 1. Cargar Modelo

```python
from core.unified_model_wrapper import get_model

# Forma simple
solar = get_model("solar_short")

# Con validaciÃ³n
try:
    model = get_model("modelo_inexistente")
except ValueError as e:
    print(f"Error: {e}")
```

### 2. Invocar (SÃ­ncrono)

```python
# Texto simple
response = solar.invoke("Â¿QuÃ© es Python?")

# Con configuraciÃ³n
response = solar.invoke(
    "Explica IA",
    config={"temperature": 0.9, "max_tokens": 200}
)
```

### 3. Invocar (AsÃ­ncrono)

```python
import asyncio

async def main():
    response = await solar.ainvoke("Hola")
    print(response)

asyncio.run(main())
```

### 4. Streaming

```python
for chunk in solar.stream("Cuenta del 1 al 10"):
    print(chunk, end="", flush=True)
```

### 5. Batch Processing

```python
queries = [
    "Â¿QuÃ© es Python?",
    "Â¿QuÃ© es JavaScript?",
    "Â¿QuÃ© es Rust?"
]

responses = solar.batch(queries)
for q, r in zip(queries, responses):
    print(f"Q: {q}\nA: {r}\n")
```

### 6. Listar Modelos Disponibles

```python
from core.unified_model_wrapper import list_available_models

models = list_available_models()
print(models)
# ['solar_short', 'solar_long', 'lfm2', 'qwen3_vl', 'embeddings', ...]
```

---

## ConfiguraciÃ³n

### Estructura de `models.yaml`

```yaml
nombre_modelo:
  # Metadata
  name: "Nombre Humano del Modelo"
  type: "text" | "multimodal" | "embedding" | "classifier" | "orchestrator"
  backend: "gguf" | "transformers" | "multimodal" | "ollama" | "openai_api" | "embedding" | "pytorch_checkpoint" | "config"
  
  # ConfiguraciÃ³n especÃ­fica del backend
  # (ver secciÃ³n de cada backend)
  
  # GestiÃ³n de memoria (opcional)
  load_on_demand: true | false  # Lazy loading
  priority: 1-10  # Prioridad de descarga (10 = mÃ¡xima)
  max_memory_mb: 4096  # LÃ­mite de RAM
```

### Variables de Entorno

El wrapper resuelve automÃ¡ticamente `${VARIABLE}`:

```yaml
api_url: "${OLLAMA_BASE_URL}"  # â†’ http://<OLLAMA_HOST>:11434
api_key: "${OPENAI_API_KEY}"   # â†’ sk-...
model_name: "${SOLAR_MODEL_NAME}"  # â†’ solar-10.7b-q4_k_m
```

**Fallback**: Si la variable no existe, usa un valor por defecto o lanza error.

---

## Ejemplos Avanzados

### 1. Pipeline LCEL con Unified Wrapper

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Template
prompt = ChatPromptTemplate.from_template(
    "Responde en {idioma}: {pregunta}"
)

# Pipeline
chain = prompt | solar | StrOutputParser()

# Invocar
response = chain.invoke({
    "idioma": "espaÃ±ol",
    "pregunta": "Â¿QuÃ© es la IA?"
})
```

### 2. Fallback Chain

```python
from langchain_core.runnables import RunnableBranch

# Fallback: solar â†’ lfm2 â†’ error
chain = solar.with_fallbacks([
    get_model("lfm2"),
])

response = chain.invoke("Hola")
```

### 3. Multimodal con Imagen

```python
qwen = get_model("qwen3_vl")

# Desde archivo
response = qwen.invoke({
    "text": "Â¿QuÃ© aparece en esta imagen?",
    "image": "diagrams/arquitectura.png"
})

# Desde base64
import base64
with open("image.jpg", "rb") as f:
    image_b64 = base64.b64encode(f.read()).decode()

response = qwen.invoke({
    "text": "Describe esto",
    "image": f"data:image/jpeg;base64,{image_b64}"
})
```

### 4. Embeddings para ClasificaciÃ³n

```python
embeddings = get_model("embeddings")

# Clasificar intenciÃ³n
query = "Â¿CÃ³mo instalar Python?"
categories = [
    "pregunta tÃ©cnica",
    "saludo",
    "despedida",
    "consulta general"
]

# Generar embeddings
query_emb = embeddings.invoke(query)
category_embs = embeddings.batch_encode(categories)

# Similitud coseno
import numpy as np
similarities = np.dot(category_embs, query_emb)
best_match = categories[np.argmax(similarities)]

print(f"CategorÃ­a: {best_match}")  # â†’ "pregunta tÃ©cnica"
```

### 5. GestiÃ³n Manual de Memoria

```python
from core.unified_model_wrapper import ModelRegistry

registry = ModelRegistry()

# Ver modelos cargados
loaded = registry.get_loaded_models()
print(loaded)  # ['solar_short']

# Descargar manualmente
registry.unload_model("solar_short")

# Descargar TODOS
registry.unload_all()
```

---

## API Reference

### `get_model(name: str) -> UnifiedModelWrapper`

Obtiene wrapper para modelo especificado.

**Args:**
- `name`: Nombre del modelo (key en `models.yaml`)

**Returns:**
- Instancia de `UnifiedModelWrapper` (o subclase)

**Raises:**
- `ValueError`: Si el modelo no existe en config

**Ejemplo:**
```python
solar = get_model("solar_short")
```

---

### `list_available_models() -> List[str]`

Lista todos los modelos disponibles en `models.yaml`.

**Returns:**
- Lista de nombres de modelos

**Ejemplo:**
```python
models = list_available_models()
# ['solar_short', 'solar_long', 'lfm2', ...]
```

---

### `UnifiedModelWrapper.invoke(input, config=None) -> str`

Ejecuta el modelo de forma sÃ­ncrona.

**Args:**
- `input`: `str`, `dict` (multimodal), o `List[BaseMessage]` (LangChain)
- `config`: Dict opcional con parÃ¡metros de generaciÃ³n

**Returns:**
- `str`: Respuesta del modelo (o `np.ndarray` para embeddings)

**Ejemplo:**
```python
response = model.invoke(
    "Explica Python",
    config={"temperature": 0.9, "max_tokens": 200}
)
```

---

### `UnifiedModelWrapper.ainvoke(input, config=None) -> str`

VersiÃ³n asÃ­ncrona de `invoke()`.

**Ejemplo:**
```python
response = await model.ainvoke("Hola")
```

---

### `UnifiedModelWrapper.stream(input, config=None) -> Iterator[str]`

Genera respuesta en streaming.

**Ejemplo:**
```python
for chunk in model.stream("Cuenta hasta 10"):
    print(chunk, end="")
```

---

### `UnifiedModelWrapper.batch(inputs: List) -> List[str]`

Procesa mÃºltiples inputs en batch.

**Ejemplo:**
```python
responses = model.batch(["query1", "query2", "query3"])
```

---

### `UnifiedModelWrapper.unload()`

Descarga el modelo de memoria manualmente.

**Ejemplo:**
```python
model.unload()
```

---

## Troubleshooting

### Error: "Model not found in config"

**Causa**: El modelo no existe en `models.yaml`

**SoluciÃ³n**:
```python
# Ver modelos disponibles
from core.unified_model_wrapper import list_available_models
print(list_available_models())

# O agregar modelo a config/models.yaml
```

---

### Error: "Ollama server not available"

**Causa**: Servidor Ollama no estÃ¡ corriendo o URL incorrecta

**SoluciÃ³n**:
```bash
# Verificar Ollama
curl http://<OLLAMA_HOST>:11434/api/tags

# O configurar variable de entorno
export OLLAMA_BASE_URL="http://localhost:11434"
```

---

### Error: "sentence-transformers not installed"

**Causa**: Dependencia faltante para embeddings

**SoluciÃ³n**:
```bash
pip install sentence-transformers
# O usar implementaciÃ³n directa (ya en v2.14)
```

---

### Modelo carga muy lento

**Causa**: Primera carga de modelo grande

**SoluciÃ³n**:
- Usar `load_on_demand: false` en config para precarga
- Reducir `n_ctx` para modelos GGUF
- Usar cuantizaciÃ³n mÃ¡s agresiva (Q4_K_M â†’ Q2_K)

---

### RAM se agota

**Causa**: Demasiados modelos cargados

**SoluciÃ³n**:
```python
# Ver modelos cargados
registry = ModelRegistry()
loaded = registry.get_loaded_models()

# Descargar los que no usas
for model in loaded:
    if model not in ["lfm2"]:  # Mantener solo esenciales
        registry.unload_model(model)
```

---

## MigraciÃ³n desde model_pool

### Antes (v2.13 - model_pool)

```python
from core.model_pool import ModelPool

pool = ModelPool(config)
solar = pool.get("expert")
response = solar.generate(
    prompt="Hola",
    temperature=0.7,
    max_tokens=100
)
pool.release("expert")
```

### DespuÃ©s (v2.14 - Unified Wrapper)

```python
from core.unified_model_wrapper import get_model

solar = get_model("solar_long")  # legacy mapping: expert â†’ solar_long
response = solar.invoke(
    "Hola",
    config={"temperature": 0.7, "max_tokens": 100}
)
# Auto-release (garbage collection automÃ¡tico)
```

### Legacy Mappings

Para compatibilidad temporal, existe mapeo automÃ¡tico:

```yaml
# config/models.yaml
legacy_mappings:
  backend: "config"
  expert: solar_long
  expert_short: solar_short
  expert_long: solar_long
  tiny: lfm2
  multimodal: qwen3_vl
```

```python
# Funciona con nombres legacy
expert = get_model("expert")  # â†’ Resuelve a solar_long
tiny = get_model("tiny")      # â†’ Resuelve a lfm2
```

**RecomendaciÃ³n**: Migrar a nombres nuevos para claridad.

---

## Mejores PrÃ¡cticas

### 1. YAML sobre CÃ³digo

âŒ **No hacer**:
```python
# Hard-coded en Python
solar = Llama(model_path="/path/to/model", n_ctx=512, ...)
```

âœ… **Hacer**:
```yaml
# config/models.yaml
solar_custom:
  backend: "gguf"
  model_path: "/path/to/model"
  n_ctx: 512
```
```python
solar = get_model("solar_custom")
```

---

### 2. Usar Config para ParÃ¡metros Runtime

âŒ **No hacer**:
```python
# Hard-coded temperature
response = model.invoke("texto")
```

âœ… **Hacer**:
```python
# Flexible via config
response = model.invoke(
    "texto",
    config={"temperature": 0.9, "max_tokens": 200}
)
```

---

### 3. Lazy Loading para Modelos Grandes

```yaml
qwen3_vl:
  load_on_demand: true  # Solo carga cuando se necesita
  priority: 7  # Descarga primero si hay presiÃ³n de RAM
```

---

### 4. ValidaciÃ³n de Input

```python
def safe_invoke(model, input_text):
    try:
        return model.invoke(input_text)
    except Exception as e:
        logger.error(f"Error en {model.name}: {e}")
        return "Lo siento, ocurriÃ³ un error."
```

---

## Roadmap v2.15+

- [ ] **Backend PyTorchCheckpoint real** (actualmente solo config)
- [ ] **Auto-tuning de parÃ¡metros** basado en hardware
- [ ] **Distributed inference** (multi-GPU)
- [ ] **Model versioning** en YAML
- [ ] **TelemetrÃ­a integrada** (latencia, RAM, tokens/s)
- [ ] **Hot-reload de config** sin reinicio

---

## Contribuir

Para agregar un nuevo backend:

1. Crear clase en `core/unified_model_wrapper.py`:
   ```python
   class MiBackendWrapper(UnifiedModelWrapper):
       def _load_model(self) -> Any:
           # Tu lÃ³gica
           pass
       
       def _invoke_sync(self, input, config) -> str:
           # Tu lÃ³gica
           pass
   ```

2. Registrar en `ModelRegistry.get_model()`:
   ```python
   elif backend == "mi_backend":
       wrapper = MiBackendWrapper(name, config)
   ```

3. Agregar tests en `tests/test_unified_wrapper_integration.py`

4. Documentar en este archivo

---

## Licencia

MIT License - SARAi v2.14  
Â© 2025 iagenerativa

---

**Ãšltima actualizaciÃ³n**: 1 de noviembre de 2025  
**Autor**: GitHub Copilot + SARAi Team  
**Estado**: âœ… PRODUCCIÃ“N
