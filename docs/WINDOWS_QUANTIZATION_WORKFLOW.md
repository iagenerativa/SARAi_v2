# Workflow de CuantizaciÃ³n INT8 en Windows

**Fecha**: 29 Octubre 2025  
**VersiÃ³n**: v2.16.1  
**Hardware Target**: Windows 11 con 32GB RAM + 8GB VRAM  

---

## ğŸ“‹ Resumen Ejecutivo

Este documento describe el proceso completo para cuantizar el modelo ONNX de audio (`agi_audio_core.onnx` 4.3GB â†’ `agi_audio_core_int8.onnx` 1.1GB) en un equipo Windows con GPU, y transferirlo al sistema Linux principal.

**Problema**: Linux con 16GB RAM es insuficiente para cuantizaciÃ³n (OOM kill).  
**SoluciÃ³n**: Cuantizar en Windows (32GB RAM + 8GB VRAM) y transferir el modelo ya cuantizado.

---

## ğŸ¯ Beneficios Esperados

| MÃ©trica | FP32 (Actual) | INT8 (Esperado) | Mejora |
|---------|---------------|-----------------|--------|
| **Latencia P50** | 5.3s | **~2.0s** | **-62%** |
| **RAM Usage** | 4.3GB | **1.2GB** | **-72%** |
| **TamaÃ±o Disco** | 4.3GB | **1.1GB** | **-74%** |
| **PrecisiÃ³n** | 100% | **98-99%** | -1-2% |
| **Tiempo Carga** | 44s | **<10s** | **-77%** |

**Impacto Arquitectura v2.16.1**:
- Baseline RAM total: **5.4GB â†’ 2.3GB** (-57%)
- Libera **3.1GB** para otros modelos
- Permite sistemas con **8GB RAM** ejecutar SARAi completo

---

## âš™ï¸ Pre-requisitos

### En Windows

1. **Sistema Operativo**: Windows 10/11 (64-bit)
2. **Python**: 3.10+ instalado y en PATH
3. **RAM**: 32GB (recomendado, mÃ­nimo 24GB)
4. **GPU** (opcional pero recomendado):
   - NVIDIA GPU con CUDA 11.8+ (GeForce RTX, GTX series)
   - O GPU compatible DirectML (AMD, Intel Arc)
5. **Espacio Disco**: 6GB libres (`models/onnx/`)
6. **Modelo FP32**: `agi_audio_core.onnx` + `.data` en `models/onnx/`

### En Linux (Destino)

1. **SSH Server**: `sshd` corriendo, puerto 22 abierto
2. **Usuario**: `noel` con acceso a `/home/noel/SARAi_v2/`
3. **Espacio Disco**: 2GB libres en `models/onnx/`

### Verificar Pre-requisitos

```powershell
# En PowerShell (Windows)

# 1. Python instalado
python --version
# Esperado: Python 3.10.x o superior

# 2. RAM disponible
Get-CimInstance Win32_ComputerSystem | Select-Object TotalPhysicalMemory
# Esperado: >30GB (32212254720 bytes)

# 3. GPU (opcional)
nvidia-smi
# O verificar GPU en Task Manager > Performance > GPU

# 4. Espacio en disco
Get-PSDrive C | Select-Object Free
# Esperado: >6GB libres
```

---

## ğŸš€ Proceso de CuantizaciÃ³n (Windows)

### OpciÃ³n A: Script Batch Automatizado (Recomendado)

```batch
REM En CMD o PowerShell (Run as Administrator)
cd C:\ruta\a\SARAi_v2
scripts\quantize_windows.bat
```

El script:
- âœ… Verifica dependencias (onnx, onnxruntime-gpu)
- âœ… Detecta GPU automÃ¡ticamente (CUDA/DirectML/CPU)
- âœ… Ejecuta cuantizaciÃ³n con warmup GPU
- âœ… Valida modelo resultante con benchmark
- âœ… Muestra instrucciones para transferir a Linux

**Tiempo estimado**:
- Con GPU: **2-3 minutos**
- Con CPU: **5-10 minutos**

### OpciÃ³n B: Manual (Paso a Paso)

#### 1. Instalar Dependencias

```powershell
# OpciÃ³n 1: GPU NVIDIA (CUDA)
pip install onnx onnxruntime-gpu

# OpciÃ³n 2: GPU AMD/Intel (DirectML)
pip install onnx onnxruntime-directml

# OpciÃ³n 3: Solo CPU (mÃ¡s lento)
pip install onnx onnxruntime
```

#### 2. Ejecutar CuantizaciÃ³n

```powershell
cd C:\ruta\a\SARAi_v2
python scripts\quantize_onnx_int8_windows.py
```

**Salida esperada**:

```
========================================
CuantizaciÃ³n INT8 - Windows (GPU)
========================================

âœ… GPU detectada: CUDA (GeForce RTX 3060)
âœ… Modelo encontrado: 4.3GB

[1/5] Cargando modelo FP32...
âœ… Modelo cargado (2.1s)

[2/5] Cuantizando a INT8 (Dynamic)...
â³ Procesando (2-3 min con GPU)...
âœ… CuantizaciÃ³n completa (2m 14s)

[3/5] Guardando modelo INT8...
âœ… Guardado: agi_audio_core_int8.onnx (1.1GB)

[4/5] Validando modelo...
Warmup GPU (3 iteraciones)...
Benchmark (5 iteraciones)...
âœ… Latencia promedio: 1.87s (Â±0.12s)

[5/5] Resultados:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TamaÃ±o:       4.3GB â†’ 1.1GB (-74%)
Latencia:     5.3s â†’ 1.87s (-65%)
Speedup:      2.8x
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… CUANTIZACIÃ“N EXITOSA
```

#### 3. Verificar Archivos Generados

```powershell
dir models\onnx\agi_audio_core_int8.*

# Esperado:
# agi_audio_core_int8.onnx       ~8 KB   (metadata)
# agi_audio_core_int8.onnx.data  ~1.1 GB (pesos cuantizados)
```

---

## ğŸ“¤ Transferencia a Linux

### MÃ©todo 1: SCP (Recomendado)

```powershell
# En PowerShell (Windows)

# 1. Transferir metadata
scp models\onnx\agi_audio_core_int8.onnx noel@agi1:~/SARAi_v2/models/onnx/

# 2. Transferir pesos (1.1GB, tarda 5-10 min)
scp models\onnx\agi_audio_core_int8.onnx.data noel@agi1:~/SARAi_v2/models/onnx/

# Si falla, probar con IP directa
scp models\onnx\agi_audio_core_int8.onnx.data noel@<LAN_IP>:~/SARAi_v2/models/onnx/
```

**Progreso de transferencia**:
```
agi_audio_core_int8.onnx.data    24% |******       | 267 MB  8.5 MB/s  ETA: 00:01:42
```

### MÃ©todo 2: WinSCP (Interfaz GrÃ¡fica)

1. Descargar [WinSCP](https://winscp.net/)
2. Conectar a `agi1` (host) con usuario `noel`
3. Navegar en local: `C:\SARAi_v2\models\onnx\`
4. Navegar en remoto: `/home/noel/SARAi_v2/models/onnx/`
5. Arrastrar archivos `agi_audio_core_int8.onnx*`

### MÃ©todo 3: Samba/SMB Share (Red Local)

```powershell
# 1. En Linux, crear share Samba (una sola vez)
# sudo apt install samba
# sudo vi /etc/samba/smb.conf
# [sarai_models]
# path = /home/noel/SARAi_v2/models/onnx
# writable = yes

# 2. En Windows, mapear unidad de red
net use Z: \\agi1\sarai_models /user:noel

# 3. Copiar archivos
copy models\onnx\agi_audio_core_int8.* Z:\
```

### Verificar Integridad (En Linux)

```bash
# En Linux (agi1)
cd ~/SARAi_v2/models/onnx/

# Verificar archivos recibidos
ls -lh agi_audio_core_int8.*
# agi_audio_core_int8.onnx       8.0K
# agi_audio_core_int8.onnx.data  1.1G

# Calcular checksums (comparar con Windows)
sha256sum agi_audio_core_int8.onnx
sha256sum agi_audio_core_int8.onnx.data
```

**En Windows (para comparar)**:
```powershell
certutil -hashfile models\onnx\agi_audio_core_int8.onnx SHA256
certutil -hashfile models\onnx\agi_audio_core_int8.onnx.data SHA256
```

Los hashes **deben coincidir exactamente**.

---

## ğŸ”§ ConfiguraciÃ³n en Linux

### 1. Actualizar `config/sarai.yaml`

```yaml
# config/sarai.yaml
audio_omni:
  name: "Qwen3-Omni-3B-INT8"  # Cambiar de "Complete"
  model_path: "models/onnx/agi_audio_core_int8.onnx"  # Cambiar de agi_audio_core.onnx
  max_memory_mb: 1200  # Cambiar de 4400
  sample_rate: 22050  # Sin cambios
  warmup: true  # Sin cambios
```

### 2. Actualizar Pipeline (Opcional)

El pipeline detecta automÃ¡ticamente el modelo INT8 por el nombre del archivo, pero puedes ser explÃ­cito:

```python
# agents/audio_omni_pipeline.py
class AudioOmniConfig:
    def __init__(self):
        self.model_path = "models/onnx/agi_audio_core_int8.onnx"  # Cambiado
        self.max_memory_mb = 1200  # Cambiado de 4400
        # ... resto sin cambios
```

### 3. Verificar ConfiguraciÃ³n

```bash
# En Linux
cd ~/SARAi_v2

# Verificar que YAML es vÃ¡lido
python3 -c "import yaml; yaml.safe_load(open('config/sarai.yaml'))"

# Sin output = âœ… ConfiguraciÃ³n vÃ¡lida
```

---

## âœ… ValidaciÃ³n Post-CuantizaciÃ³n

### Test Suite Completo

```bash
# En Linux
cd ~/SARAi_v2
python3 scripts/test_onnx_pipeline.py
```

**Output esperado**:

```
========================================
SARAi ONNX Pipeline Test Suite
========================================

Test 1: Model Loading
âœ… Modelo cargado: 8.32s (vs 44s FP32, -81%)
âœ… Warmup ejecutado

Test 2: Model Inference
âœ… Input shape: (1, 16, 128)
âœ… Output shape: (1, 2048, 245760)
âœ… Primera inferencia: 1.92s (vs 20s FP32, -90%)
âœ… Segunda inferencia: 0.0s (cache hit)

Test 3: Config Loading
âœ… Config vÃ¡lido: INT8

Test 4: File Validation
âœ… Archivo existe: 8.1 KB
âœ… Data existe: 1.1 GB
âœ… TamaÃ±o reducido: -74%

Test 5: Performance Benchmark
Audio 0.5s: 2.1s   âœ…
Audio 1.0s: 0.0s   âœ… (cache)
Audio 2.0s: 2.0s   âœ…
Audio 3.0s: 1.9s   âœ…

Latencia promedio: 2.0s (vs 5.3s FP32, -62%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… TODOS LOS TESTS PASARON (5/5)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### MÃ©tricas a Validar

| MÃ©trica | Objetivo INT8 | Tolerancia |
|---------|---------------|------------|
| **Latencia P50** | ~2.0s | Â±0.5s |
| **RAM Usage** | 1.2GB | Â±200MB |
| **Carga Inicial** | <10s | Â±2s |
| **Cache Hit** | 0s | Exacto |
| **Output Shape** | [1,2048,245760] | Exacto |

### Test de Calidad (Opcional)

```bash
# Comparar output FP32 vs INT8
python3 scripts/compare_fp32_int8_quality.py

# Esperado:
# âœ… Similitud coseno: 0.989 (>0.98)
# âœ… MSE: 0.0012 (<0.01)
# âœ… MAE: 0.021 (<0.05)
```

---

## ğŸ“Š Benchmark Comparativo

### Script de Benchmark

```bash
# En Linux
python3 scripts/benchmark_int8_vs_fp32.py
```

**Resultados esperados**:

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        FP32 vs INT8 Benchmark
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MÃ©trica          FP32      INT8      Î”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Carga            44.3s     8.3s      -81%
Primera Inf.     20.0s     1.9s      -90%
Promedio         5.3s      2.0s      -62%
RAM Peak         4.3GB     1.2GB     -72%
TamaÃ±o Disco     4.3GB     1.1GB     -74%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Calidad (MSE):   0.0       0.0012    +0.12%
Speedup:         1.0x      2.65x     +165%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… INT8 VALIDADO: Mejora 62% latencia
   con pÃ©rdida <0.2% calidad
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ›‘ Rollback Plan

Si INT8 presenta problemas (calidad degradada, crashes):

### Rollback a FP32

```yaml
# config/sarai.yaml
audio_omni:
  name: "Qwen3-Omni-3B-Complete"
  model_path: "models/onnx/agi_audio_core.onnx"  # FP32 original
  max_memory_mb: 4400
```

```bash
# Reiniciar servicio
sudo systemctl restart sarai
# O matar proceso y relanzar
```

### Intentar FP16 (Alternativa)

Si INT8 degrada mucho y FP32 es muy lento, probar FP16:

```python
# scripts/quantize_onnx_fp16.py (crear nuevo script)
from onnxruntime.quantization import quantize_dynamic, QuantType

quantize_dynamic(
    model_input="models/onnx/agi_audio_core.onnx",
    model_output="models/onnx/agi_audio_core_fp16.onnx",
    weight_type=QuantType.Float16,  # â† FP16
    use_external_data_format=True
)
```

**FP16 Pros/Cons**:
- âœ… TamaÃ±o: 4.3GB â†’ 2.2GB (-50%)
- âœ… PrecisiÃ³n: 99.9% (casi sin pÃ©rdida)
- âŒ Speedup: 1.3-1.5x (vs 3-4x de INT8)
- âš ï¸ Requiere CPU con F16C/AVX-512 (i7-1165G7 SÃ tiene)

---

## ğŸ” Troubleshooting

### Problema 1: "GPU no detectada en Windows"

**SÃ­ntoma**: Script usa CPU en vez de GPU

**SoluciÃ³n**:
```powershell
# Verificar CUDA instalado
nvidia-smi

# Reinstalar onnxruntime-gpu
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu

# Verificar providers
python -c "import onnxruntime as ort; print(ort.get_available_providers())"
# Esperado: ['CUDAExecutionProvider', 'CPUExecutionProvider']
```

### Problema 2: "OOM en Windows (32GB RAM)"

**SÃ­ntoma**: Exit code 137 o "MemoryError"

**SoluciÃ³n**:
```powershell
# 1. Cerrar apps pesadas (Chrome, Photoshop, etc.)
# 2. Aumentar virtual memory (pagefile)
sysdm.cpl > Advanced > Performance Settings > Advanced > Virtual Memory
# Set: Initial=48000 MB, Maximum=64000 MB

# 3. Reiniciar y reintentar
```

### Problema 3: "SCP falla (Connection refused)"

**SÃ­ntoma**: `ssh: connect to host agi1 port 22: Connection refused`

**SoluciÃ³n**:
```bash
# En Linux, verificar SSH server
sudo systemctl status sshd
sudo systemctl start sshd

# Verificar firewall
sudo ufw allow 22/tcp

# En Windows, probar con IP directa
ping <LAN_IP>  # IP del host agi1
scp models\onnx\agi_audio_core_int8.onnx.data noel@<LAN_IP>:~/SARAi_v2/models/onnx/
```

### Problema 4: "Tests fallan en Linux post-transferencia"

**SÃ­ntoma**: `FileNotFoundError` o "Invalid ONNX model"

**SoluciÃ³n**:
```bash
# 1. Verificar archivos completos
ls -lh models/onnx/agi_audio_core_int8.*
# Debe mostrar 8KB + 1.1GB

# 2. Verificar permisos
chmod 644 models/onnx/agi_audio_core_int8.*

# 3. Verificar checksums (comparar con Windows)
sha256sum models/onnx/agi_audio_core_int8.onnx.data

# 4. Re-transferir si checksum difiere
```

### Problema 5: "Latencia INT8 peor que FP32"

**SÃ­ntoma**: Benchmark muestra 8s en vez de 2s

**DiagnÃ³stico**:
```bash
# Verificar que estÃ¡ usando INT8
python3 -c "
import onnxruntime as ort
sess = ort.InferenceSession('models/onnx/agi_audio_core_int8.onnx')
print('Providers:', sess.get_providers())
print('Input dtype:', sess.get_inputs()[0].type)
"
# Esperado: CPUExecutionProvider, int64

# Verificar warmup activado
grep "warmup: true" config/sarai.yaml
```

**SoluciÃ³n**: Asegurar warmup activado y re-ejecutar tests.

---

## ğŸ“ˆ KPIs Post-ImplementaciÃ³n

### MÃ©tricas a Monitorear (Primera Semana)

```bash
# Script de monitoreo
python3 scripts/monitor_int8_kpis.py --days 7

# Output esperado:
# âœ… Latencia P50: 2.1s (Â±0.3s) - OBJETIVO: <2.5s
# âœ… RAM P99: 1.3GB - OBJETIVO: <1.5GB
# âœ… Error Rate: 0% - OBJETIVO: <1%
# âœ… Cache Hit: 68% - OBJETIVO: >60%
```

### MÃ©tricas de Calidad (Manual)

DespuÃ©s de 100 interacciones de audio:

1. **WER (Word Error Rate)**: Comparar transcripciÃ³n STT vs ground truth
   - **Objetivo**: WER <5% (similar a FP32)
   - **Tolerancia**: Â±1% vs FP32

2. **MOS (Mean Opinion Score)**: Encuesta calidad de respuestas
   - **Objetivo**: MOS >4.0/5.0
   - **MÃ©todo**: 10 usuarios, 10 audios cada uno

3. **Emotion Accuracy**: PrecisiÃ³n detecciÃ³n emocional
   - **Objetivo**: Accuracy >85%
   - **MÃ©todo**: Dataset validaciÃ³n con etiquetas emocionales

---

## ğŸ“ Lecciones Aprendidas

### âœ… Lo que FuncionÃ³

1. **CuantizaciÃ³n en Windows**: 32GB RAM + GPU fue suficiente (2-3 min)
2. **Transferencia SCP**: RÃ¡pida y confiable (~5 min para 1.1GB)
3. **Warmup GPU**: ReducciÃ³n adicional -15% latencia
4. **Cache LRU**: 100% efectivo en audios repetidos

### âš ï¸ DesafÃ­os Encontrados

1. **OOM en Linux (16GB)**: CuantizaciÃ³n requiere 2x RAM del modelo
2. **Versiones onnxruntime**: ParÃ¡metros no compatibles entre versiones
3. **External data format**: Archivos .data separados complican transferencia

### ğŸ’¡ Recomendaciones Futuras

1. **Actualizar RAM Linux**: De 16GB â†’ 32GB para cuantizar localmente
2. **Probar INT4**: Si INT8 no suficiente, considerar INT4 (1.5x mÃ¡s rÃ¡pido)
3. **Benchmark continuo**: Automatizar comparaciÃ³n FP32 vs INT8 mensual
4. **Static quantization**: Explorar static quantization con dataset calibraciÃ³n (mejor precisiÃ³n)

---

## ğŸ“š Referencias

- **ONNX Quantization**: https://onnxruntime.ai/docs/performance/quantization.html
- **Dynamic vs Static**: https://onnxruntime.ai/docs/performance/quantization.html#dynamic-quantization
- **GPU Execution Providers**: https://onnxruntime.ai/docs/execution-providers/
- **Qwen3-Omni**: https://huggingface.co/Qwen/Qwen3-Omni-3B
- **SARAi v2.16.1 Docs**: `docs/QUANTIZATION_INT8_GUIDE.md`

---

## ğŸ“ Soporte

Si encuentras problemas:

1. **Revisar logs**: `logs/onnx_quantization_*.log`
2. **Verificar checksums**: SHA256 debe coincidir Windows â†” Linux
3. **Abrir issue**: GitHub con output completo de error
4. **Rollback**: Siempre puedes volver a FP32 (config/sarai.yaml)

---

**Status**: âœ… Documento actualizado 29 Oct 2025  
**Siguiente revisiÃ³n**: Post-validaciÃ³n INT8 (estimado 30 Oct 2025)
