# AnÃ¡lisis de DecisiÃ³n: Latencia vs Calidad

**Fecha**: 29 octubre 2025  
**Contexto**: Evaluar si Qwen3-Omni-30B (2.87s) justifica el aumento vs objetivo 190ms

---

## ğŸ“Š Comparativa CrÃ­tica

### Escenario A: Objetivo Original (190-240ms)

**Modelo hipotÃ©tico**: Qwen3-VL-4B-Instruct (si existiera en ONNX)

| MÃ©trica | Valor Esperado | Caso de Uso Ideal |
|---------|---------------|-------------------|
| **Latencia** | 190-240ms | âœ… ConversaciÃ³n fluida en tiempo real |
| **STT WER** | ~2.0% | âœ… TranscripciÃ³n aceptable |
| **TTS MOS** | 4.21 natural, 4.38 empatÃ­a | âœ… Voz natural |
| **RAM** | ~1.5GB (3B INT8) | âœ… Ligero |
| **ParÃ¡metros** | 3B | âš ï¸ Capacidad limitada |

**Limitaciones**:
- âŒ Menor capacidad de razonamiento (3B parÃ¡metros)
- âŒ Vocabulario mÃ¡s limitado
- âŒ Menos contexto para respuestas complejas
- âš ï¸ **Problema**: Este modelo NO existe como ONNX INT8 actualmente

---

### Escenario B: SituaciÃ³n Actual (Qwen3-Omni-30B)

| MÃ©trica | Valor Real | Impacto |
|---------|-----------|---------|
| **Latencia** | 2.87s (2870ms) | âŒ **15x mÃ¡s lenta** que objetivo |
| **STT WER** | â‰¤1.8% (esperado) | âœ… 10% mejor que 3B |
| **TTS MOS** | â‰¥4.32/4.50 (esperado) | âœ… ~3% mejor que 3B |
| **RAM** | 1.1GB INT8 | âœ… MÃ¡s ligero que 3B FP32 |
| **ParÃ¡metros** | 30B | âœ… **10x mÃ¡s capacidad** |

**Ventajas**:
- âœ… 10x mÃ¡s parÃ¡metros (razonamiento profundo)
- âœ… Mejor calidad STT/TTS (medible)
- âœ… Mayor vocabulario y contexto
- âœ… MÃ¡s robusto ante acentos/ruido

**Desventajas**:
- âŒ **Latencia INACEPTABLE para conversaciÃ³n en tiempo real**
- âŒ 2.87s = pausa perceptible (lÃ­mite humano ~200ms)
- âŒ Experiencia de usuario degradada

---

## ğŸ¯ Casos de Uso: Â¿CuÃ¡ndo Importa la Latencia?

### Caso 1: ConversaciÃ³n en Tiempo Real âš ï¸ CRÃTICO

**Escenario**: Usuario habla con SARAi como asistente de voz

```
Usuario: "Hola SARAi, Â¿quÃ© tiempo harÃ¡ maÃ±ana?"
         â””â”€ Silencio de 2.87 segundos â”€â”
SARAi:                                  "MaÃ±ana habrÃ¡ sol..."
```

**PercepciÃ³n del usuario**:
- âœ… 190ms: Natural, fluido âœ¨
- âš ï¸ 500ms: Aceptable, ligera pausa
- âŒ 1000ms: Lento, notable
- âŒ **2870ms: INACEPTABLE, parece roto** ğŸ’”

**Veredicto**: âŒ **Qwen3-Omni-30B NO sirve para conversaciÃ³n en tiempo real**

---

### Caso 2: Procesamiento Batch/AsÃ­ncrono âœ… VIABLE

**Escenario**: TranscripciÃ³n de archivos, procesamiento offline

```
Usuario sube 10 archivos de audio â†’ SARAi procesa en background
Latencia por archivo: 2.87s
Total: 28.7s para 10 archivos

Usuario NO espera activamente (proceso asÃ­ncrono)
```

**PercepciÃ³n del usuario**:
- âœ… Calidad superior (WER 1.8% vs 2.0%)
- âœ… No hay espera perceptible (background task)
- âœ… Mejor para transcripciones largas

**Veredicto**: âœ… **Qwen3-Omni-30B EXCELENTE para batch/offline**

---

### Caso 3: Swapping bajo Demanda (Tu Propuesta) ğŸ¤”

**Escenario**: SARAi mantiene modelo ligero por defecto, carga 30B solo cuando necesita calidad mÃ¡xima

```
Flujo Normal (95% casos):
â”œâ”€ Modelo ligero (190ms): ConversaciÃ³n casual
â””â”€ Swapping activado si:
    â”œâ”€ TranscripciÃ³n crÃ­tica (legal, mÃ©dica)
    â”œâ”€ TTS emocional complejo
    â””â”€ Usuario solicita mÃ¡xima calidad

Swapping Cost:
â”œâ”€ Carga modelo 30B: ~27s (one-time)
â”œâ”€ Inferencia: 2.87s
â”œâ”€ Descarga: ~1s
â””â”€ Total: ~31s overhead
```

**Viabilidad**:
- âœ… Permite usar ambos modelos segÃºn contexto
- âœ… Optimiza latencia (190ms) para casos comunes
- âœ… Reserva calidad (30B) para casos crÃ­ticos
- âš ï¸ Requiere detectar automÃ¡ticamente cuÃ¡ndo swappear
- âš ï¸ 31s overhead al activar 30B (aceptable si es poco frecuente)

**Veredicto**: âœ… **Estrategia hÃ­brida VIABLE** si:
1. Existe modelo 3B ONNX INT8 (190ms)
2. DetecciÃ³n automÃ¡tica de casos crÃ­ticos
3. Swapping <5% del tiempo total

---

## ğŸ’¡ Opciones Disponibles

### OpciÃ³n 1: Buscar/Crear Qwen3-VL-4B-Instruct ONNX INT8 â­ RECOMENDADO

**AcciÃ³n**:
1. Buscar en HuggingFace: `Qwen/Qwen3-VL-4B-Instruct-Instruct`
2. Si existe PyTorch: Convertir a ONNX + cuantizar INT8
3. Si NO existe: Usar alternativa (MeloTTS + Whisper-small)

**Pros**:
- âœ… Latencia objetivo (190ms) alcanzable
- âœ… Experiencia fluida en tiempo real
- âœ… Calidad suficiente para uso general

**Contras**:
- âš ï¸ Requiere trabajo de conversiÃ³n/bÃºsqueda
- âš ï¸ Calidad inferior a 30B (WER 2.0% vs 1.8%)

**Timeline**: 2-4 horas (si existe modelo base)

---

### OpciÃ³n 2: Sistema HÃ­brido (Dual-Model Swapping) ğŸ”„

**Arquitectura**:
```yaml
models:
  audio_light:
    name: "Qwen3-VL-4B-Instruct"  # Si se encuentra
    latency: 190ms
    use_case: "conversaciÃ³n general (95% casos)"
    ram: 1.5GB
  
  audio_heavy:
    name: "Qwen3-Omni-30B"
    latency: 2870ms
    use_case: "transcripciÃ³n crÃ­tica, TTS emocional complejo (5% casos)"
    ram: 1.1GB
    swap_triggers:
      - user_request: "mÃ¡xima calidad"
      - context: "legal|mÃ©dico|tÃ©cnico"
      - emotion_intensity: >0.8
```

**Pros**:
- âœ… Mejor de ambos mundos
- âœ… Latencia baja por defecto
- âœ… Calidad mÃ¡xima cuando importa

**Contras**:
- âš ï¸ Complejidad arquitectural (2 modelos)
- âš ï¸ Requiere modelo ligero (no lo tenemos aÃºn)
- âš ï¸ Swapping overhead (31s al cambiar)

**Timeline**: 1 dÃ­a (con modelo ligero disponible)

---

### OpciÃ³n 3: Mantener Solo Qwen3-Omni-30B âŒ NO RECOMENDADO

**Uso exclusivo**:
- âœ… TranscripciÃ³n batch
- âœ… Procesamiento offline
- âŒ ConversaciÃ³n en tiempo real (ROTO)

**Veredicto**: Solo viable si SARAi **NO necesita conversaciÃ³n fluida**

---

## ğŸ” InvestigaciÃ³n Necesaria

### Â¿Existe Qwen3-VL-4B-Instruct en ONNX?

Necesito verificar:

1. **HuggingFace Search**:
   ```bash
   # Buscar modelos Qwen Omni disponibles
   https://huggingface.co/models?search=qwen+omni
   ```

2. **Alternativas REALISTAS** (latencia <300ms, <4GB RAM):
   
   **OpciÃ³n A - Balance Ã“ptimo** (RECOMENDADO):
   - **Whisper-small** (244M, STT): ~80ms, WER ~3-4%
   - **Piper TTS** (o MeloTTS): ~60ms, MOS ~4.0
   - **Total**: ~140ms âœ…, ~1.5GB RAM âœ…
   
   **OpciÃ³n B - MÃ¡s Ligero**:
   - **Whisper-tiny** (39M, STT): ~30ms, WER ~5-6% (peor)
   - **espeak-ng** (sÃ­ntesis rÃ¡pida): ~20ms, MOS ~3.0 (robÃ³tico)
   - **Total**: ~50ms âœ…, ~500MB RAM âœ… (pero calidad baja)
   
   **OpciÃ³n C - Calidad Media-Alta**:
   - **Whisper-medium** (769M, STT): ~150ms, WER ~2.5%
   - **VITS** (TTS): ~100ms, MOS ~4.2
   - **Total**: ~250ms âš ï¸, ~2.5GB RAM âœ…
   
   **âŒ NO VIABLE** (descartado por latencia/RAM):
   - Whisper-large-v3 (1.5B): ~400ms, WER ~1.8% (muy lento)
   - Higgs Audio V2 (5.77B): ~800ms, MOS ~4.8 (VRAM >6GB)

3. **ConversiÃ³n manual**:
   - Si existe Qwen3-VL-4B-Instruct en PyTorch â†’ Convertir ONNX
   - Timeline: 4-6 horas

---

## ğŸ“‹ RecomendaciÃ³n Final

### Para ConversaciÃ³n en Tiempo Real: âŒ QWEN3-OMNI-30B NO ES VIABLE

**RazÃ³n**: 2.87s es **14.5x mÃ¡s lenta** que el lÃ­mite de fluidez humana (~200ms)

### Estrategia Recomendada: SISTEMA HÃBRIDO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SARAi Dual-Speed Audio Architecture        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  DEFAULT (95% casos):                       â”‚
â”‚  â”œâ”€ Modelo Ligero (3B o alternativa)        â”‚
â”‚  â”œâ”€ Latencia: 190-240ms âœ…                  â”‚
â”‚  â””â”€ Calidad: Suficiente para uso general   â”‚
â”‚                                             â”‚
â”‚  SWAP TO HEAVY (5% casos crÃ­ticos):        â”‚
â”‚  â”œâ”€ Qwen3-Omni-30B                         â”‚
â”‚  â”œâ”€ Latencia: 2870ms (aceptable offline)   â”‚
â”‚  â”œâ”€ Triggers: transcripciÃ³n legal/mÃ©dica   â”‚
â”‚  â””â”€ Calidad: MÃ¡xima (WER 1.8%, MOS 4.5)    â”‚
â”‚                                             â”‚
â”‚  Swapping Cost: 31s (amortizado en >10     â”‚
â”‚  inferencias de calidad)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PrÃ³ximos Pasos**:

1. **INMEDIATO**: Buscar/crear modelo ligero (3B ONNX INT8)
   - Timeline: 4-6 horas
   - Prioridad: CRÃTICA

2. **CORTO PLAZO**: Implementar lÃ³gica de swapping
   - Timeline: 1 dÃ­a
   - Prioridad: ALTA

3. **FALLBACK**: Si no existe 3B, usar alternativa:
   - Whisper-small (STT) + MeloTTS (TTS)
   - Timeline: 2-3 horas
   - Calidad: 85-90% del Qwen-3B

---

## ğŸ¯ Respuesta Directa a Tu Pregunta

> "Â¿Justifica la mejora del sistema el aumento de latencia aunque solo sea para swapping?"

**Respuesta**: âœ… **SÃ, pero SOLO como modelo secundario** (5% del tiempo)

**JustificaciÃ³n**:
- âŒ **NO** para conversaciÃ³n por defecto (2.87s es inaceptable)
- âœ… **SÃ** para transcripciÃ³n crÃ­tica bajo demanda
- âœ… **SÃ** para procesamiento batch/offline
- âœ… **SÃ** como "modo alta calidad" activable

**CondiciÃ³n crÃ­tica**: Necesitas un modelo ligero (190ms) como default.

**Prioridad absoluta**: Conseguir/crear Qwen3-VL-4B-Instruct ONNX INT8 o alternativa equivalente.

---

## ğŸš€ Plan de AcciÃ³n Inmediato

## ğŸ“Š Tabla Comparativa de Alternativas REALISTAS

| Modelo/Pipeline | Latencia | RAM | STT WER | TTS MOS | Uso Recomendado | Viable? |
|----------------|----------|-----|---------|---------|-----------------|---------|
| **Qwen3-Omni-30B INT8** | 2870ms | 1.1GB | 1.8% âœ… | 4.5 âœ… | Batch/offline, calidad mÃ¡xima | âš ï¸ NO para real-time |
| **Whisper-small + Piper** | 190ms âœ… | 1.9GB | 3.5% âš ï¸ | 4.0 âš ï¸ | ConversaciÃ³n fluida, uso general | âœ… **RECOMENDADO** |
| **Whisper-medium + VITS** | 250ms âš ï¸ | 2.5GB | 2.5% âœ… | 4.2 âœ… | Balance calidad/latencia | âœ… Viable |
| **Whisper-tiny + espeak-ng** | 50ms âœ… | 0.5GB | 6% âŒ | 3.0 âŒ | Prototipado rÃ¡pido | âŒ Calidad muy baja |
| **Whisper-large-v3 + Higgs V2** | 1200ms âŒ | 8GB âŒ | 1.5% âœ… | 4.8 âœ… | SOTA (si tienes GPU) | âŒ Latencia/RAM excesiva |
| **Qwen3-VL-4B-Instruct** (si existiera) | 190ms âœ… | 1.5GB | 2.0% âœ… | 4.3 âœ… | Ideal (modelo unificado) | âš ï¸ NO existe en ONNX |

**Leyenda**:
- âœ… Excelente (cumple objetivo)
- âš ï¸ Aceptable (trade-off necesario)
- âŒ Inaceptable (no viable)

---

## ğŸ¯ DecisiÃ³n Recomendada: Sistema HÃ­brido REALISTA

### Arquitectura Dual-Speed con Modelos EXISTENTES

```yaml
# config/audio_models.yaml

audio:
  default_tier: "fast"  # Para 95% de casos
  
  models:
    fast:  # ConversaciÃ³n en tiempo real
      stt: "openai/whisper-small"
      tts: "rhasspy/piper-tts"
      latency_target: 190ms
      quality: "aceptable"
      ram: 1.9GB
      use_cases:
        - conversaciÃ³n casual
        - comandos de voz
        - asistente domÃ©stico
    
    quality:  # Procesamiento crÃ­tico
      stt: "Qwen3-Omni-30B-A3B-Instruct (STT component)"
      tts: "Qwen3-Omni-30B-A3B-Instruct (TTS component)"
      latency_target: 2870ms
      quality: "excelente"
      ram: 1.1GB
      swap_triggers:
        - user_command: "transcribe con mÃ¡xima calidad"
        - context_type: ["legal", "mÃ©dico", "tÃ©cnico"]
        - emotion_required: high  # TTS emocional complejo
      use_cases:
        - transcripciÃ³n legal/mÃ©dica
        - TTS emocional (empatÃ­a, tristeza profunda)
        - documentaciÃ³n tÃ©cnica
```

**ImplementaciÃ³n**:

```python
# core/audio_router.py

class AudioRouter:
    """Router inteligente entre modelo rÃ¡pido y calidad"""
    
    def __init__(self):
        self.fast_pipeline = AudioLightPipeline()  # Whisper-small + Piper
        self.quality_pipeline = AudioOmniPipeline()  # Qwen3-Omni-30B
        self.current_tier = "fast"
    
    def process(self, audio: bytes, context: dict) -> dict:
        """
        Decide quÃ© pipeline usar segÃºn contexto
        
        Args:
            audio: Audio bytes
            context: {
                'user_command': str,
                'context_type': str,
                'emotion_required': str,
                'is_critical': bool
            }
        
        Returns:
            Procesamiento del pipeline seleccionado
        """
        # DetecciÃ³n automÃ¡tica de necesidad de calidad
        needs_quality = (
            context.get('is_critical', False) or
            context.get('context_type') in ['legal', 'mÃ©dico', 'tÃ©cnico'] or
            context.get('emotion_required') == 'high' or
            'mÃ¡xima calidad' in context.get('user_command', '').lower()
        )
        
        if needs_quality:
            print(f"ğŸ”„ Swapping a modelo de CALIDAD (latencia ~3s, justificado)")
            return self.quality_pipeline.process_audio(audio)
        else:
            # 95% de casos: latencia Ã³ptima
            return self.fast_pipeline.process_audio(audio)
```

**Beneficios**:
1. âœ… **95% del tiempo**: Latencia 190ms (conversaciÃ³n fluida)
2. âœ… **5% crÃ­tico**: Calidad mÃ¡xima (WER 1.8%, MOS 4.5)
3. âœ… **Sin frustraciÃ³n**: Usuario sabe cuÃ¡ndo esperar (swap explÃ­cito)
4. âœ… **Modelos existentes**: No requiere buscar Qwen-3B inexistente

**Trade-offs aceptados**:
- âš ï¸ Calidad ligeramente inferior en modo rÃ¡pido (WER 3.5% vs 1.8%)
- âš ï¸ Necesita detectar contextos crÃ­ticos (reglas o ML)
- âš ï¸ Swapping overhead (31s al activar calidad)

---

## ğŸš€ Plan de AcciÃ³n ACTUALIZADO

### PASO 1: Investigar disponibilidad (30 min)
```bash
# Buscar en HuggingFace
- Qwen/Qwen3-VL-4B-Instruct-Instruct
- Qwen/Qwen-Audio-3B
- Alternativas: speechbrain, coqui-ai/TTS

# Si NO existe en ONNX:
- Verificar si existe PyTorch â†’ Convertir
- Fallback: Whisper-small + MeloTTS
```

### PASO 2: Implementar modelo ligero (4-6h)

**OpciÃ³n RECOMENDADA**: Whisper-small + Piper TTS

```python
# agents/audio_light_pipeline.py - Pipeline ligero para conversaciÃ³n

class AudioLightPipeline:
    """
    Pipeline ligero para conversaciÃ³n en tiempo real
    - STT: Whisper-small (244M) ~80ms
    - TTS: Piper (60M) ~60ms
    - TOTAL: ~140ms âœ…
    """
    
    def __init__(self):
        # STT: Whisper-small (mejor balance latencia/calidad)
        import whisper
        self.stt_model = whisper.load_model("small")  # 244M params
        
        # TTS: Piper (TTS rÃ¡pido, calidad aceptable)
        from piper import PiperVoice
        self.tts_model = PiperVoice.load("es_ES-davefx-medium")  # 60M params
    
    def process_audio(self, audio_bytes: bytes) -> dict:
        """
        Procesa audio en tiempo real con latencia <200ms
        
        Returns:
            {
                'text': str,           # TranscripciÃ³n
                'response_audio': bytes,  # Respuesta TTS
                'latency_ms': float,   # Latencia total
                'metadata': {
                    'stt_wer_expected': 3.5,  # % error esperado
                    'tts_mos_expected': 4.0,  # Calidad voz
                    'model': 'whisper-small + piper'
                }
            }
        """
        import time
        start = time.time()
        
        # 1. STT con Whisper-small (~80ms)
        import librosa
        audio, sr = librosa.load(audio_bytes, sr=16000)
        result = self.stt_model.transcribe(audio)
        text = result["text"]
        
        # 2. [AQUÃ IrÃ­a LFM2 para lÃ³gica/RAG/empatÃ­a]
        # Por ahora, eco simple para testing
        response_text = f"Entendido: {text}"
        
        # 3. TTS con Piper (~60ms)
        response_audio = self.tts_model.synthesize(response_text)
        
        latency_ms = (time.time() - start) * 1000
        
        return {
            'text': text,
            'response_audio': response_audio,
            'latency_ms': latency_ms,
            'metadata': {
                'stt_wer_expected': 3.5,
                'tts_mos_expected': 4.0,
                'model': 'whisper-small + piper'
            }
        }
```

**Benchmarks esperados**:
```
Latencia STT:        ~80ms  (Whisper-small CPU)
Latencia TTS:        ~60ms  (Piper CPU)
Latencia LFM2:       ~50ms  (si se integra para lÃ³gica)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:               ~190ms âœ… (dentro de objetivo)

RAM:
- Whisper-small:     ~1GB
- Piper:             ~200MB
- LFM2 (opcional):   ~700MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:               ~1.9GB âœ… (viable en 16GB)

Calidad:
- WER (STT):         ~3-4% (aceptable, no SOTA)
- MOS (TTS):         ~4.0  (natural, no emocional)
```

**Ventajas vs Qwen3-Omni-30B**:
- âœ… Latencia: 190ms vs 2870ms (**15x mÃ¡s rÃ¡pido**)
- âœ… ConversaciÃ³n fluida en tiempo real
- âš ï¸ Calidad: 3.5% WER vs 1.8% (peor, pero aceptable)

**Desventajas**:
- âš ï¸ Menor calidad que Qwen-30B (WER +100%, MOS -8%)
- âš ï¸ Sin empatÃ­a nativa en TTS (Piper es neutro)
- âš ï¸ Pipeline desagregado (mÃ¡s puntos de fallo)

**Alternativas descartadas**:
```
âŒ Whisper-large-v3 + Higgs Audio V2:
   - Latencia: ~1200ms (4x objetivo)
   - RAM: ~8GB (inaceptable)
   - Calidad: SOTA pero inviable para real-time

âŒ Whisper-tiny + espeak-ng:
   - Latencia: ~50ms âœ…
   - RAM: ~500MB âœ…
   - Calidad: Muy baja (WER 6%, MOS 3.0 robÃ³tico)
```

### PASO 3: Configurar swapping (1 dÃ­a)
```python
# core/model_pool.py
class AudioModelPool:
    def get_audio_model(self, quality_tier: str):
        if quality_tier == "fast":
            return self.load("qwen-3b-int8")  # 190ms
        elif quality_tier == "quality":
            return self.load("qwen-30b-int8")  # 2870ms
```

---

**ConclusiÃ³n ACTUALIZADA**: 

1. **Qwen3-Omni-30B es EXCELENTE**, pero **NO para conversaciÃ³n en tiempo real** (2.87s inaceptable)

2. **NO persigas Whisper-large-v3 + Higgs V2** (latencia ~1.2s, RAM ~8GB, overkill)

3. **SÃ implementa sistema hÃ­brido**:
   - **DEFAULT**: Whisper-small + Piper (~190ms, WER 3.5%, MOS 4.0) âœ…
   - **SWAP**: Qwen3-Omni-30B cuando se justifica (transcripciÃ³n crÃ­tica) âœ…

4. **Timeline realista**:
   - Implementar pipeline ligero: **4-6 horas**
   - Configurar router de swapping: **2-3 horas**
   - Testing integraciÃ³n: **2 horas**
   - **TOTAL: 1 dÃ­a de trabajo**

Â¿Procedo a implementar el **AudioRouter** y el **AudioLightPipeline** (Whisper-small + Piper)?
