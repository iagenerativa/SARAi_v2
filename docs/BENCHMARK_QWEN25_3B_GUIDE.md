# Gu√≠a de Benchmark: Qwen3-VL-4B-Instruct vs Qwen3-Omni-30B

**Fecha**: 29 octubre 2025  
**Objetivo**: Validar emp√≠ricamente si Qwen3-VL-4B-Instruct alcanza latencia <240ms

---

## üéØ Hip√≥tesis a Validar

### Teor√≠a

| Modelo | Par√°metros | Latencia Esperada | WER Esperado | MOS Esperado |
|--------|------------|-------------------|--------------|--------------|
| **Qwen3-VL-4B-Instruct** | 3B | **190-240ms** ‚úÖ | 2.0% | 4.21/4.38 |
| **Qwen3-Omni-30B** | 30B | **2870ms** ‚ùå | 1.8% | 4.32/4.50 |

**Pregunta cr√≠tica**: ¬øLa latencia te√≥rica de 3B (~190ms) se cumple en la pr√°ctica?

---

## üì• Paso 1: Obtener el Modelo ONNX

### Opci√≥n A: Descargar desde HuggingFace (si existe)

```bash
# Buscar modelo en HuggingFace
https://huggingface.co/models?search=Qwen3-VL-4B-Instruct

# Si existe versi√≥n ONNX directa:
huggingface-cli download Qwen/Qwen3-VL-4B-Instruct-ONNX \
    --local-dir models/onnx/ \
    --include "*.onnx" "*.onnx.data"
```

### Opci√≥n B: Convertir desde PyTorch (si solo existe PyTorch)

```bash
# 1. Descargar modelo PyTorch
huggingface-cli download Qwen/Qwen3-VL-4B-Instruct-Instruct \
    --local-dir models/pytorch/Qwen3-VL-4B-Instruct/

# 2. Convertir a ONNX
python scripts/convert_to_onnx.py \
    --model models/pytorch/Qwen3-VL-4B-Instruct/ \
    --output models/onnx/Qwen3-VL-4B-Instruct.onnx \
    --optimize

# 3. Cuantizar a INT8 (opcional, para mejorar latencia)
python scripts/quantize_int8.py \
    --model models/onnx/Qwen3-VL-4B-Instruct.onnx \
    --output models/onnx/Qwen3-VL-4B-Instruct-int8.onnx
```

### Opci√≥n C: Archivo compartido

Si lo tienes de otra fuente:

```bash
# Copiar a la ubicaci√≥n correcta
cp /path/to/Qwen3-VL-4B-Instruct.onnx models/onnx/
cp /path/to/Qwen3-VL-4B-Instruct.onnx.data models/onnx/  # Si existe archivo .data

# Verificar integridad
ls -lh models/onnx/Qwen3-VL-4B-Instruct*
```

---

## üß™ Paso 2: Ejecutar Benchmark Individual

### Test del Modelo 3B (cuando lo tengas)

```bash
cd /home/noel/SARAi_v2

# Benchmark completo
python scripts/benchmark_audio_latency.py \
    --model models/onnx/Qwen3-VL-4B-Instruct.onnx \
    --iterations 20

# Output esperado:
# ‚è±Ô∏è  Tiempos de Carga:
#    Carga modelo:  XX.XXs
#    Warmup (avg):  XX.XXs
# 
# ‚ö° Latencia de Inferencia:
#    P50 promedio:  XXX.Xms  <-- ESTO ES CR√çTICO
#    P99 promedio:  XXX.Xms
# 
# üéØ Objetivo: <240ms
#    ‚úÖ CUMPLE o ‚ùå NO CUMPLE
```

### Test del Modelo 30B (baseline actual)

```bash
# Para comparaci√≥n
python scripts/benchmark_audio_latency.py \
    --model models/onnx/agi_audio_core_int8.onnx \
    --iterations 20

# Resultado esperado: P50 ~2870ms ‚ùå
```

---

## üìä Paso 3: Comparaci√≥n Directa

### Benchmark Lado a Lado

```bash
python scripts/benchmark_audio_latency.py --compare \
    --model-a models/onnx/Qwen3-VL-4B-Instruct.onnx \
    --model-b models/onnx/agi_audio_core_int8.onnx \
    --iterations 20
```

**Output esperado**:

```
üìä TABLA COMPARATIVA
================================================================================

M√©trica                   Modelo A             Modelo B             Diferencia
--------------------------------------------------------------------------------
Modelo                    Qwen3-VL-4B-Instruct      agi_audio_core_int8  N/A
Carga (s)                 XX.XX                26.88                ‚¨áÔ∏è -X.XX (-X%)
Warmup (s)                XX.XX                47.35                ‚¨áÔ∏è -X.XX (-X%)
Latencia P50 (ms)         XXX.X                2872.0               ‚¨áÔ∏è -XXXX (-XX%) üéØ
Latencia P99 (ms)         XXX.X                XXXX.X               ‚¨áÔ∏è -XXXX (-XX%)
Cumple <240ms             ‚úÖ S√ç/‚ùå NO          ‚ùå NO                N/A
Throughput (inf/s)        X.XX                 0.35                 ‚¨ÜÔ∏è +X.XX (+XX%)

================================================================================

üéØ VEREDICTO:
   ‚úÖ/‚ùå Modelo A (Qwen3-VL-4B-Instruct) CUMPLE/NO CUMPLE objetivo (<240ms)
   ‚ùå Modelo B (agi_audio_core_int8) NO cumple
   ‚Üí Recomendaci√≥n: ...
```

---

## ‚úÖ Paso 4: Decisi√≥n Basada en Datos

### Escenario A: 3B CUMPLE (<240ms) ‚úÖ

**Si P50 de Qwen3-VL-4B-Instruct < 240ms**:

```
‚úÖ DECISI√ìN: Usar Qwen3-VL-4B-Instruct como modelo principal

Pr√≥ximos pasos:
1. Integrar Qwen3-VL-4B-Instruct.onnx en audio_omni_pipeline.py
2. Validar calidad (WER/MOS) con audio real
3. Mantener Qwen3-Omni-30B como modelo opcional de m√°xima calidad
4. Implementar sistema h√≠brido:
   - DEFAULT: Qwen3-VL-4B-Instruct (latencia √≥ptima)
   - SWAP: Qwen3-Omni-30B (calidad m√°xima bajo demanda)

Timeline: 4-6 horas de integraci√≥n
```

### Escenario B: 3B NO CUMPLE (>240ms) ‚ùå

**Si P50 de Qwen3-VL-4B-Instruct > 240ms**:

```
‚ùå DECISI√ìN: Modelo 3B tampoco es viable para real-time

Pr√≥ximos pasos:
1. Implementar pipeline desagregado:
   - Whisper-small (STT): ~80ms
   - Piper TTS: ~60ms
   - Total: ~140ms ‚úÖ

2. Mantener ambos Qwen como opciones de calidad:
   - Qwen3-VL-4B-Instruct: Calidad media (batch)
   - Qwen3-Omni-30B: Calidad m√°xima (cr√≠tico)

3. Arquitectura tri-tier:
   - FAST: Whisper-small + Piper (~140ms)
   - QUALITY: Qwen3-VL-4B-Instruct (~XXXms, si <1s)
   - MAX: Qwen3-Omni-30B (~2870ms)

Timeline: 1-2 d√≠as de implementaci√≥n
```

### Escenario C: 3B MEJORA pero NO SUFICIENTE (240-500ms) ‚ö†Ô∏è

**Si 240ms < P50 < 500ms**:

```
‚ö†Ô∏è DECISI√ìN: Modelo 3B es mejor que 30B pero a√∫n lento

An√°lisis:
- ¬øEs aceptable 300-400ms de latencia? (l√≠mite perceptible)
- Usuario puede tolerar leve pausa vs pipeline desagregado
- Calidad integral (STT+TTS unificado) vs componentes separados

Opciones:
A) Aceptar latencia 240-500ms si calidad justifica
B) Usar 3B como tier medio + pipeline ligero para real-time
C) Optimizar 3B m√°s (cuantizaci√≥n agresiva, pruning)

Timeline: Decisi√≥n subjetiva basada en UX testing
```

---

## üîç M√©tricas a Capturar

### Durante el Benchmark

1. **Latencia P50/P90/P99**: Distribuci√≥n de tiempos de respuesta
2. **Carga del modelo**: Tiempo inicial (one-time cost)
3. **Warmup**: Compilaci√≥n de kernels (one-time)
4. **Throughput**: Inferencias por segundo
5. **RAM usage**: Consumo de memoria (usar `htop` en paralelo)

### Post-Benchmark (con audio real)

6. **WER (Word Error Rate)**: Calidad de transcripci√≥n
7. **MOS (Mean Opinion Score)**: Calidad de TTS (requiere evaluadores)
8. **Robustez ante ruido**: Test con audio ruidoso
9. **Empat√≠a en TTS**: Capacidad de prosodia emocional

---

## üìù Template de Reporte de Resultados

```markdown
# Resultados Benchmark: Qwen3-VL-4B-Instruct

**Fecha**: [Fecha del test]
**Hardware**: i7 quad-core, 16GB RAM, CPU-only
**ONNX Runtime**: [versi√≥n]

## Modelo Testeado

- Nombre: Qwen3-VL-4B-Instruct
- Path: models/onnx/Qwen3-VL-4B-Instruct.onnx
- Tama√±o: [X.X GB]
- Formato: ONNX [FP32/INT8]

## Resultados de Latencia

| M√©trica | Valor | Objetivo | ‚úÖ/‚ùå |
|---------|-------|----------|-------|
| Carga modelo | XX.XXs | N/A | - |
| Warmup | XX.XXs | N/A | - |
| **Latencia P50** | **XXX.Xms** | **<240ms** | **‚úÖ/‚ùå** |
| Latencia P90 | XXX.Xms | <300ms | ‚úÖ/‚ùå |
| Latencia P99 | XXX.Xms | <400ms | ‚úÖ/‚ùå |
| Throughput | X.XX inf/s | >4 inf/s | ‚úÖ/‚ùå |

## Comparaci√≥n vs Qwen3-Omni-30B

| M√©trica | 3B | 30B | Mejora |
|---------|----|----|--------|
| Latencia P50 | XXXms | 2870ms | -XX% |
| RAM | X.XGB | 1.1GB | +/-X% |
| Calidad (esperada) | WER 2.0% | WER 1.8% | -10% |

## Decisi√≥n

[‚úÖ VIABLE / ‚ùå NO VIABLE / ‚ö†Ô∏è REQUIERE AN√ÅLISIS]

**Justificaci√≥n**:
[Explicar decisi√≥n basada en datos]

**Pr√≥ximos pasos**:
1. [Acci√≥n 1]
2. [Acci√≥n 2]
3. [Acci√≥n 3]
```

---

## üöÄ Checklist de Validaci√≥n

Antes de tomar decisi√≥n final:

- [ ] Modelo ONNX descargado y verificado
- [ ] Benchmark de latencia ejecutado (‚â•20 iteraciones)
- [ ] Comparaci√≥n con Qwen3-Omni-30B realizada
- [ ] RAM usage medido (<4GB objetivo)
- [ ] Latencia P50 < 240ms validada (o documentada si no cumple)
- [ ] Test con audio real (no solo dummy input)
- [ ] Decisi√≥n documentada en reporte
- [ ] Timeline de implementaci√≥n definido

---

## üí° Tips de Optimizaci√≥n (si latencia es borderline)

Si Qwen3-VL-4B-Instruct est√° cerca pero no cumple (<300ms):

### 1. Cuantizaci√≥n Agresiva

```bash
# INT8 si est√° en FP32
python scripts/quantize_int8.py --model Qwen3-VL-4B-Instruct.onnx

# O incluso INT4 (p√©rdida de calidad m√≠nima en algunos modelos)
python scripts/quantize_int4.py --model Qwen3-VL-4B-Instruct.onnx
```

**Ganancia esperada**: -20-30% latencia

### 2. Graph Optimization Extra

```python
# En el benchmark, a√±adir:
sess_options.add_session_config_entry("session.disable_prepacking", "0")
sess_options.add_session_config_entry("session.use_ort_model_bytes_directly", "1")
```

**Ganancia esperada**: -5-10% latencia

### 3. Reducir Resoluci√≥n de Audio

```python
# Si modelo acepta menor sample rate
# 16kHz ‚Üí 8kHz (solo si calidad no se degrada mucho)
```

**Ganancia esperada**: -15-25% latencia

---

## ‚ùì FAQ

**P: ¬øQu√© hago si el modelo no existe en ONNX?**

R: Opciones:
1. Contactar a Qwen/Alibaba para versi√≥n ONNX oficial
2. Convertir desde PyTorch (requiere c√≥digo del modelo)
3. Usar alternativa (Whisper-small + Piper)

**P: ¬ø240ms es un l√≠mite estricto?**

R: No, es guideline. 190-300ms es aceptable para conversaci√≥n. >500ms ya se siente lento.

**P: ¬øY si 3B tampoco cumple?**

R: Implementar pipeline desagregado (Whisper + Piper) garantiza <200ms.

**P: ¬øPuedo usar GPU para acelerar?**

R: S√≠, pero entonces la comparaci√≥n con 30B debe ser tambi√©n en GPU (fair comparison).

---

**Siguiente paso**: Cuando tengas el modelo ONNX, ejecuta:

```bash
python scripts/benchmark_audio_latency.py \
    --model models/onnx/Qwen3-VL-4B-Instruct.onnx \
    --iterations 20
```

Y comparte los resultados. Tomaremos la decisi√≥n basada en **datos reales**, no teor√≠a. üéØ
