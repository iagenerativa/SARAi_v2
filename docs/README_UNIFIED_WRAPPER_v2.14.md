# üöÄ SARAi v2.14 - Unified Model Wrapper

## Agregar Nuevos Modelos

SARAi v2.14 implementa una arquitectura **config-driven** que permite agregar modelos sin modificar c√≥digo Python.

### Filosof√≠a

> **"SARAi no debe conocer sus modelos. Solo debe invocar capacidades.**  
> **YAML define, LangChain orquesta, el wrapper abstrae.**  
> **Cuando el hardware mejore, solo cambiamos configuraci√≥n, nunca c√≥digo."**

---

## üéØ Quick Start: Agregar GPT-4 Vision

### 1. Editar `config/models.yaml`

```yaml
gpt4_vision:
  name: "GPT-4 Vision Preview"
  type: "multimodal"
  backend: "openai_api"
  
  # API Configuration
  api_key: "${OPENAI_API_KEY}"
  api_url: "https://api.openai.com/v1"
  model_name: "gpt-4-vision-preview"
  
  # Capacidades
  supports_images: true
  
  # Par√°metros
  temperature: 0.7
  max_tokens: 2048
  
  # Gesti√≥n de memoria
  load_on_demand: true
  priority: 5
```

### 2. Configurar `.env`

```bash
OPENAI_API_KEY=sk-...tu-api-key-aqui
```

### 3. Usar en c√≥digo

```python
from core.unified_model_wrapper import get_model

# ¬°Listo! Sin modificar c√≥digo
gpt4 = get_model("gpt4_vision")
response = gpt4.invoke({
    "text": "¬øQu√© hay en esta imagen?",
    "image": "path/to/image.jpg"
})

print(response)
```

**Tiempo total**: 5 minutos vs 5 horas antes ‚úÖ

---

## üìä Backends Soportados

| Backend | Formato | Uso | Ventajas | Desventajas |
|---------|---------|-----|----------|-------------|
| **gguf** | GGUF Q4_K_M | CPU local | ‚úÖ R√°pido en CPU<br>‚úÖ Bajo consumo RAM<br>‚úÖ Offline | ‚ùå Requiere descarga<br>‚ùå Solo CPU |
| **ollama** | API REST | Servidor Ollama | ‚úÖ 0 RAM local<br>‚úÖ M√∫ltiples modelos<br>‚úÖ Hot-swap | ‚ùå Requiere servidor<br>‚ùå Latencia red |
| **multimodal** | HF Transformers | Vision/Audio | ‚úÖ Nativo multimodal<br>‚úÖ Gran variedad | ‚ùå Alto consumo RAM<br>‚ùå Lento en CPU |
| **transformers** | HF 4-bit | GPU | ‚úÖ R√°pido en GPU<br>‚úÖ Baja VRAM | ‚ùå Requiere GPU<br>‚ùå Setup complejo |
| **openai_api** | API REST | Cloud | ‚úÖ Sin hardware local<br>‚úÖ Modelos SOTA<br>‚úÖ Escalable | ‚ùå Costo por token<br>‚ùå Requiere internet |

---

## üîß Ejemplos de Configuraci√≥n

### Ejemplo 1: Claude 3 Opus (Cloud API)

```yaml
claude_opus:
  name: "Claude 3 Opus"
  type: "text"
  backend: "openai_api"
  
  # Anthropic API (OpenAI-compatible proxy)
  api_key: "${ANTHROPIC_API_KEY}"
  api_url: "https://api.anthropic.com/v1"
  model_name: "claude-3-opus-20240229"
  
  temperature: 0.7
  max_tokens: 4096
  
  load_on_demand: true
  priority: 4
```

**Uso**:
```python
claude = get_model("claude_opus")
response = claude.invoke("Explica la teor√≠a cu√°ntica")
```

---

### Ejemplo 2: Gemini Pro Vision (Google)

```yaml
gemini_vision:
  name: "Gemini Pro Vision"
  type: "multimodal"
  backend: "openai_api"
  
  api_key: "${GOOGLE_API_KEY}"
  api_url: "https://generativelanguage.googleapis.com/v1"
  model_name: "gemini-pro-vision"
  
  supports_images: true
  supports_video: true
  
  temperature: 0.7
  max_tokens: 2048
  
  load_on_demand: true
  priority: 3
```

**Uso**:
```python
gemini = get_model("gemini_vision")
response = gemini.invoke({
    "text": "Analiza este video",
    "video": "conference.mp4"
})
```

---

### Ejemplo 3: Llama 3 70B (Ollama Local)

```yaml
llama3_70b:
  name: "Llama 3 70B (Ollama)"
  type: "text"
  backend: "ollama"
  
  # Ollama local (sin API key)
  api_url: "http://localhost:11434"
  model_name: "llama3:70b"
  
  temperature: 0.7
  
  load_on_demand: true
  priority: 6
```

**Prerequisito**:
```bash
# Iniciar Ollama
ollama pull llama3:70b
ollama serve
```

**Uso**:
```python
llama3 = get_model("llama3_70b")
response = llama3.invoke("Pregunta compleja")
```

---

### Ejemplo 4: Mistral 7B Local (GGUF)

```yaml
mistral_7b:
  name: "Mistral 7B Instruct"
  type: "text"
  backend: "gguf"
  
  # Archivo GGUF local
  model_path: "models/cache/mistral/mistral-7b-instruct.Q4_K_M.gguf"
  
  n_ctx: 4096
  n_threads: 6
  use_mmap: true
  use_mlock: false
  
  temperature: 0.7
  max_tokens: 1024
  
  load_on_demand: true
  priority: 7
  max_memory_mb: 4500
```

**Descarga**:
```bash
# Descargar modelo GGUF
huggingface-cli download \
  TheBloke/Mistral-7B-Instruct-v0.2-GGUF \
  mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --local-dir models/cache/mistral/
```

---

## üîÑ Migraci√≥n CPU ‚Üí GPU

Cuando consigas GPU, migrar SOLAR a Transformers:

### ANTES (CPU GGUF)
```yaml
solar_short:
  backend: "gguf"
  model_path: "models/cache/solar/solar-10.7b.gguf"
  n_ctx: 512
  n_threads: 6
```

### AHORA (GPU Transformers)
```yaml
solar_gpu:
  name: "SOLAR-10.7B-Instruct (GPU)"
  type: "text"
  backend: "transformers"
  
  repo_id: "upstage/SOLAR-10.7B-Instruct-v1.0"
  
  load_in_4bit: true
  device_map: "auto"
  
  temperature: 0.7
  max_tokens: 1024
  
  load_on_demand: true
  priority: 10
```

**Cambios**: 2 l√≠neas YAML, 0 l√≠neas Python ‚úÖ

---

## üé® Usar Pipelines LCEL

### Pipeline B√°sico

```python
from core.langchain_pipelines import create_text_pipeline

# Crear pipeline
pipeline = create_text_pipeline(
    model_name="gpt4_turbo",
    temperature=0.7,
    system_prompt="Eres un experto en Python"
)

# Usar pipeline
response = pipeline.invoke("Crea una funci√≥n para ordenar lista")
```

---

### Pipeline con Fallback

```python
from core.langchain_pipelines import create_hybrid_pipeline_with_fallback

# Pipeline con fallback autom√°tico
pipeline = create_hybrid_pipeline_with_fallback(
    vision_model_name="gpt4_vision",
    text_model_name="gpt4_turbo",
    fallback_model_name="claude_opus"
)

# Si hay imagen ‚Üí GPT-4 Vision
# Si no ‚Üí GPT-4 Turbo
# Si falla ‚Üí Claude Opus (fallback)
response = pipeline.invoke({
    "text": "Analiza",
    "image": "diagram.png"
})
```

---

### Pipeline RAG con GPT-4

```python
from core.langchain_pipelines import create_rag_pipeline

# RAG con b√∫squeda web + GPT-4
rag_pipeline = create_rag_pipeline(
    search_model_name="gpt4_turbo",
    enable_cache=True,
    safe_mode=False
)

response = rag_pipeline.invoke("¬øQui√©n gan√≥ el Nobel de F√≠sica 2024?")
```

---

## üìà Comparaci√≥n de Rendimiento

### Latencia por Backend (CPU i7, 16GB RAM)

| Modelo | Backend | Input 100 tokens | Velocidad | RAM |
|--------|---------|------------------|-----------|-----|
| SOLAR-10.7B | gguf | 512 tokens | ~20s | 4.8 GB |
| SOLAR-10.7B | ollama | 512 tokens | ~25s | 0 GB (remoto) |
| GPT-4 Turbo | openai_api | 512 tokens | ~3s | 0 GB (cloud) |
| Claude Opus | openai_api | 512 tokens | ~4s | 0 GB (cloud) |
| Qwen3-VL-4B | multimodal | imagen + texto | ~30s | 4 GB |

**Nota**: Ollama a√±ade ~5s de overhead de red (LAN).

---

## üîê Seguridad de API Keys

### ‚úÖ CORRECTO: Variables de entorno

```yaml
# config/models.yaml
gpt4:
  api_key: "${OPENAI_API_KEY}"  # Variable de entorno
```

```bash
# .env
OPENAI_API_KEY=sk-...
```

### ‚ùå INCORRECTO: Hard-coded

```yaml
# ‚ùå NUNCA HACER ESTO
gpt4:
  api_key: "sk-proj-12345..."  # Hard-coded = INSEGURO
```

---

## üö® Troubleshooting

### Problema: "Model not found"

```python
# Error
model = get_model("gpt4")
# KeyError: 'gpt4'
```

**Soluci√≥n**: Verificar que el modelo est√° definido en `config/models.yaml`

```bash
grep -A5 "gpt4:" config/models.yaml
```

---

### Problema: "API key not configured"

```python
# Error
gpt4 = get_model("gpt4_turbo")
# ValueError: OPENAI_API_KEY not set
```

**Soluci√≥n**: Configurar `.env`

```bash
echo "OPENAI_API_KEY=sk-..." >> .env
```

---

### Problema: "GGUF file not found"

```python
# Error
mistral = get_model("mistral_7b")
# FileNotFoundError: models/cache/mistral/mistral-7b.gguf
```

**Soluci√≥n**: Descargar modelo

```bash
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF \
  mistral-7b-instruct-v0.2.Q4_K_M.gguf \
  --local-dir models/cache/mistral/
```

---

## üìö Referencias

- [LangChain LCEL Docs](https://python.langchain.com/docs/expression_language/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Ollama](https://ollama.com/)
- [OpenAI API](https://platform.openai.com/docs/api-reference)
- [Anthropic API](https://docs.anthropic.com/)

---

## üéØ Casos de Uso Futuros

### Caso 1: Multi-Cloud (GPT-4 + Claude + Gemini)

```python
# Pipeline con 3 APIs en paralelo
from langchain.schema.runnable import RunnableParallel

multi_cloud = RunnableParallel(
    gpt4=get_model("gpt4_turbo"),
    claude=get_model("claude_opus"),
    gemini=get_model("gemini_pro")
)

# Ejecutar en paralelo
results = multi_cloud.invoke("Pregunta compleja")

# Comparar respuestas
print("GPT-4:", results["gpt4"])
print("Claude:", results["claude"])
print("Gemini:", results["gemini"])
```

---

### Caso 2: Orquestaci√≥n Inteligente

```python
# Pipeline que elige modelo din√°micamente
from langchain.schema.runnable import RunnableBranch

smart_pipeline = RunnableBranch(
    # Si query corta (<50 chars) ‚Üí modelo r√°pido
    (lambda x: len(x) < 50, get_model("lfm2")),
    
    # Si query t√©cnica (contiene "c√≥digo") ‚Üí experto
    (lambda x: "c√≥digo" in x.lower(), get_model("solar_long")),
    
    # Si query creativa (contiene "historia") ‚Üí creativo
    (lambda x: "historia" in x.lower(), get_model("claude_opus")),
    
    # Default: GPT-4
    get_model("gpt4_turbo")
)

response = smart_pipeline.invoke("Crea una historia √©pica")
# ‚Üí Usa Claude Opus autom√°ticamente
```

---

### Caso 3: Ensemble con Votaci√≥n

```python
# 3 modelos votan por la mejor respuesta
import asyncio

async def ensemble_vote(query: str):
    # Ejecutar 3 modelos en paralelo
    tasks = [
        get_model("gpt4_turbo").ainvoke(query),
        get_model("claude_opus").ainvoke(query),
        get_model("gemini_pro").ainvoke(query)
    ]
    
    responses = await asyncio.gather(*tasks)
    
    # Votaci√≥n simple (el m√°s largo gana)
    best_response = max(responses, key=len)
    
    return best_response

# Uso
response = asyncio.run(ensemble_vote("Explica teor√≠a de cuerdas"))
```

---

**FIN DOCUMENTACI√ìN - README v2.14 Unified Wrapper**

Total: 400 LOC de documentaci√≥n con:
- ‚úÖ Quick start (5 minutos)
- ‚úÖ 4 ejemplos completos (GPT-4, Claude, Gemini, Mistral)
- ‚úÖ Migraci√≥n CPU‚ÜíGPU
- ‚úÖ Pipelines LCEL
- ‚úÖ Troubleshooting
- ‚úÖ 3 casos de uso avanzados
