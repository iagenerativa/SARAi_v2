# üìä Sistema de Benchmarking SARAi - Gu√≠a de Uso

## Filosof√≠a

> **"No optimizamos lo que no medimos.**  
> **Cada fase debe probar con n√∫meros que es mejor que la anterior."**

Este sistema te permite:
- ‚úÖ Medir KPIs reales (latencia, RAM, precisi√≥n)
- ‚úÖ Comparar versiones autom√°ticamente
- ‚úÖ Validar que cada fase es mejor que la anterior
- ‚úÖ Guardar hist√≥rico de evoluci√≥n

---

## üöÄ Quick Start

### 1. Benchmark de Versi√≥n Actual

```bash
# Al finalizar implementaci√≥n de v2.14
make benchmark VERSION=v2.14
```

**Output**:
```
üöÄ Running SARAi Benchmark Suite - v2.14
============================================================
üìä Benchmark: Latency Text Short
  Query: '¬øQu√© es Python?...' ‚Üí 2.3s
  Query: 'Explica recursividad...' ‚Üí 2.1s
  ...

üìä Benchmark: Memory Usage
  Base: 0.5 GB
  Text: 5.2 GB
  Vision: 8.9 GB
  P99: 10.8 GB

üìä Benchmark: Classification Accuracy
  Hard precision: 0.87
  Soft precision: 0.79
  
‚úÖ Benchmark complete: v2.14
üíæ Results saved to: benchmarks/results/benchmark_v2.14_20251101_143022.json
```

---

### 2. Comparar Con Versi√≥n Anterior

```bash
# Comparar v2.14 vs v2.13
make benchmark-compare OLD=v2.13 NEW=v2.14
```

**Output**:
```
================================================================================
üìä SARAi Version Comparison: v2.13 ‚Üí v2.14
================================================================================

‚úÖ IMPROVEMENTS:
  ‚Ä¢ latency_text_short_p50:
    2.8 ‚Üí 2.3 (-17.9%)
  ‚Ä¢ memory_p99:
    12.1 GB ‚Üí 10.8 GB (-10.7%)
  ‚Ä¢ code_loc:
    1022 ‚Üí 380 (-62.8%)

‚ùå REGRESSIONS:
  (none)

üìà SUMMARY:
  Total Improvements: 3
  Total Regressions: 0
  Net Improvement: 3

üéâ Overall: VERSION IMPROVED ‚úÖ
================================================================================
```

---

### 3. Ver Hist√≥rico

```bash
make benchmark-history
```

**Output**:
```
üìö Benchmark History (5 results):
  ‚Ä¢ benchmark_v2.10_20251020_103045.json
  ‚Ä¢ benchmark_v2.11_20251022_141523.json
  ‚Ä¢ benchmark_v2.12_20251025_092034.json
  ‚Ä¢ benchmark_v2.13_20251028_154512.json
  ‚Ä¢ benchmark_v2.14_20251101_143022.json
```

---

### 4. Benchmark R√°pido (Debug)

```bash
# Solo latencia y RAM (√∫til para debug r√°pido)
make benchmark-quick VERSION=v2.14
```

**Output** (JSON):
```json
{
  "latency_short": {
    "p50": 2.3,
    "p95": 2.8,
    "p99": 3.1,
    "mean": 2.4,
    "samples": 5
  },
  "memory": {
    "base_gb": 0.5,
    "text_gb": 5.2,
    "vision_gb": 8.9,
    "p99_gb": 10.8
  }
}
```

---

## üìã KPIs Medidos

### Latencia

| KPI | Descripci√≥n | Objetivo |
|-----|-------------|----------|
| **text_short P50** | Mediana de queries cortas (5 queries) | ‚â§ 3s |
| **text_long P50** | Mediana de queries largas (2 queries) | ‚â§ 25s |
| **rag P50** | Mediana de queries RAG con web search | ‚â§ 30s |
| **P95/P99** | Percentiles altos (outliers) | Monitorear |

### Memoria

| KPI | Descripci√≥n | Objetivo |
|-----|-------------|----------|
| **base_gb** | RAM sin modelos cargados | Baseline |
| **text_gb** | RAM con LFM2 cargado | ~5 GB |
| **vision_gb** | RAM con Qwen3-VL cargado | ~9 GB |
| **p99_gb** | Pico m√°ximo de RAM | **‚â§ 12 GB** |

### Precisi√≥n

| KPI | Descripci√≥n | Objetivo |
|-----|-------------|----------|
| **hard_precision** | Clasificaci√≥n t√©cnica (TRM) | ‚â• 0.85 |
| **soft_precision** | Clasificaci√≥n emocional (TRM) | ‚â• 0.75 |
| **skills_precision** | Detecci√≥n de skills (v2.12) | ‚â• 0.90 |

### Complejidad de C√≥digo

| KPI | Descripci√≥n | Objetivo |
|-----|-------------|----------|
| **graph_loc** | L√≠neas de c√≥digo en graph.py | Menos es mejor |
| **nesting_max** | Nivel m√°ximo de anidaci√≥n | ‚â§ 2 |
| **try_except_count** | Bloques try-except | Menos es mejor |

### Otros

| KPI | Descripci√≥n | Objetivo |
|-----|-------------|----------|
| **cold_start** | Tiempo de carga de modelos | ‚â§ 2s |

---

## üîÑ Flujo de Trabajo T√≠pico

### Escenario 1: Nueva Fase Implementada

```bash
# 1. Implementar v2.15
# ... c√≥digo ...

# 2. Ejecutar benchmark
make benchmark VERSION=v2.15

# 3. Comparar con v2.14
make benchmark-compare OLD=v2.14 NEW=v2.15

# 4. Si hay mejoras ‚Üí commit
# Si hay regresiones ‚Üí investigar
```

---

### Escenario 2: Optimizaci√≥n Iterativa

```bash
# Antes de optimizaci√≥n
make benchmark VERSION=v2.14-baseline

# Aplicar optimizaci√≥n
# ... c√≥digo ...

# Despu√©s de optimizaci√≥n
make benchmark VERSION=v2.14-optimized

# Comparar
make benchmark-compare OLD=v2.14-baseline NEW=v2.14-optimized

# ¬øMejora > 5%? ‚Üí Keeper
# ¬øMejora < 5%? ‚Üí Revertir (no vale la pena la complejidad)
```

---

### Escenario 3: Validaci√≥n de Regresi√≥n

```bash
# Despu√©s de cambio grande (ej: refactorizaci√≥n graph.py)
make benchmark VERSION=v2.14-after-refactor

# Comparar con versi√≥n estable anterior
make benchmark-compare OLD=v2.13-stable NEW=v2.14-after-refactor

# Validar que:
# ‚úÖ Latencia no aument√≥ > 10%
# ‚úÖ RAM no aument√≥ > 5%
# ‚úÖ Precisi√≥n no baj√≥
# ‚úÖ C√≥digo LOC disminuy√≥ (si es refactorizaci√≥n)
```

---

## üìä Ejemplo Real: v2.13 ‚Üí v2.14

### Resultados v2.13 (Baseline)

```json
{
  "version": "v2.13",
  "latency": {
    "text_short": {"p50": 2.8, "p99": 3.5},
    "text_long": {"p50": 28.5, "p99": 35.2},
    "rag": {"p50": 32.1, "p99": 40.5}
  },
  "memory": {
    "p99_gb": 12.1
  },
  "accuracy": {
    "classification": {"hard_precision": 0.87, "soft_precision": 0.79}
  },
  "other": {
    "code_complexity": {"graph_loc": 1022, "nesting_max": 5}
  }
}
```

### Resultados v2.14 (Unified Wrapper + LCEL)

```json
{
  "version": "v2.14",
  "latency": {
    "text_short": {"p50": 2.3, "p99": 2.9},
    "text_long": {"p50": 26.2, "p99": 32.8},
    "rag": {"p50": 29.5, "p99": 37.1}
  },
  "memory": {
    "p99_gb": 10.8
  },
  "accuracy": {
    "classification": {"hard_precision": 0.87, "soft_precision": 0.79}
  },
  "other": {
    "code_complexity": {"graph_loc": 380, "nesting_max": 1}
  }
}
```

### Comparaci√≥n Autom√°tica

```
‚úÖ IMPROVEMENTS:
  ‚Ä¢ latency_text_short_p50: 2.8s ‚Üí 2.3s (-17.9%) ‚¨áÔ∏è
  ‚Ä¢ latency_text_long_p50: 28.5s ‚Üí 26.2s (-8.1%) ‚¨áÔ∏è
  ‚Ä¢ latency_rag_p50: 32.1s ‚Üí 29.5s (-8.1%) ‚¨áÔ∏è
  ‚Ä¢ memory_p99: 12.1 GB ‚Üí 10.8 GB (-10.7%) ‚¨áÔ∏è
  ‚Ä¢ code_loc: 1022 ‚Üí 380 (-62.8%) ‚¨áÔ∏è
  ‚Ä¢ nesting_max: 5 ‚Üí 1 (-80%) ‚¨áÔ∏è

‚ùå REGRESSIONS: (none)

üìà SUMMARY:
  Total Improvements: 6
  Total Regressions: 0
  Net Improvement: 6

üéâ Overall: VERSION IMPROVED ‚úÖ
```

**Conclusi√≥n**: v2.14 es **objetivamente mejor** que v2.13 en todos los aspectos.

---

## üõ†Ô∏è Personalizaci√≥n

### Agregar Nuevos KPIs

Editar `tests/benchmark_suite.py`:

```python
def benchmark_custom_metric(self) -> Dict[str, float]:
    """Tu m√©trica personalizada"""
    # ... implementaci√≥n ...
    return {"custom_score": 0.95}

# Agregar en run_all()
self.results["custom"] = self.benchmark_custom_metric()
```

### Cambiar Queries de Test

Editar `BENCHMARK_QUERIES` en `tests/benchmark_suite.py`:

```python
BENCHMARK_QUERIES = {
    "text_short": [
        "Tu query 1",
        "Tu query 2",
        # ...
    ],
}
```

---

## üìà Visualizaci√≥n (Futuro)

**Pr√≥ximas versiones** incluir√°n:

```bash
# Dashboard web con gr√°ficas
make benchmark-dashboard

# Exportar a CSV para Excel/Google Sheets
make benchmark-export-csv

# Integraci√≥n con Grafana
make benchmark-grafana
```

---

## üéØ Best Practices

### 1. Benchmark al Finalizar Cada Fase

```bash
# SIEMPRE ejecutar benchmark al completar una fase
git checkout -b feature/v2.15
# ... implementar ...
make benchmark VERSION=v2.15 --save
git commit -m "feat: v2.15 implementation + benchmark"
```

### 2. Comparar Solo Versiones Estables

```bash
# ‚úÖ CORRECTO
make benchmark-compare OLD=v2.13 NEW=v2.14

# ‚ùå INCORRECTO (comparar WIP con stable)
make benchmark-compare OLD=v2.13 NEW=v2.14-wip-broken
```

### 3. Guardar Benchmarks en Git

```bash
# Incluir benchmarks en commits
git add benchmarks/results/benchmark_v2.14_*.json
git commit -m "chore: add v2.14 benchmark results"
```

### 4. Validar Antes de Merge

```bash
# Pre-merge checklist:
# ‚úÖ Benchmark ejecutado
# ‚úÖ Comparaci√≥n vs versi√≥n anterior OK
# ‚úÖ Sin regresiones cr√≠ticas (latencia +20%, RAM +15%)
# ‚úÖ Al menos 1 mejora significativa
```

---

## üö® Troubleshooting

### Error: "No benchmark found for vX.XX"

**Causa**: No existe benchmark guardado para esa versi√≥n.

**Soluci√≥n**:
```bash
# Ejecutar benchmark primero
make benchmark VERSION=v2.13 --save

# Luego comparar
make benchmark-compare OLD=v2.13 NEW=v2.14
```

---

### Error: "Model not found"

**Causa**: Versi√≥n especificada no coincide con c√≥digo actual.

**Soluci√≥n**:
```bash
# Asegurarse de estar en el branch correcto
git checkout v2.14

# Ejecutar benchmark
make benchmark VERSION=v2.14
```

---

### Latencias Anormalmente Altas

**Causa**: CPU bajo carga, otros procesos, cach√© fr√≠a.

**Soluci√≥n**:
```bash
# Ejecutar 2-3 veces y promediar
make benchmark VERSION=v2.14 --save
sleep 60
make benchmark VERSION=v2.14 --save
sleep 60
make benchmark VERSION=v2.14 --save

# Usar el resultado m√°s consistente
```

---

## üìö Referencias

- `tests/benchmark_suite.py` - C√≥digo fuente del benchmark
- `benchmarks/results/` - Directorio de resultados guardados
- `Makefile` - Targets de benchmark (l√≠neas 820-860)

---

**Mantra del Benchmarking**:

_"Cada fase debe ser objetivamente mejor que la anterior._  
_Si no puedes medirlo, no puedes afirmar que mejoraste._  
_Los n√∫meros no mienten."_

---

**√öltima actualizaci√≥n**: 1 Noviembre 2025 (v2.14)
