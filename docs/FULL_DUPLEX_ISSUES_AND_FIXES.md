# ğŸš¨ Full-Duplex: Problemas CrÃ­ticos y Soluciones

## ğŸ“‹ Problemas Identificados

### 1. âŒ Sistema Bloqueante (No Interactivo)

**Problema**: 
```python
# output_thread.py lÃ­nea 369
self._wait_for_user_silence()  # BLOQUEA TODO
```

**Flujo actual**:
```
Usuario habla â†’ [ESPERA 5s] â†’ STT â†’ [ESPERA silencio] â†’ TTS â†’ [BLOQUEA 1-2s] â†’ Audio
                     â†‘                      â†‘                       â†‘
                 BLOQUEANTE            BLOQUEANTE              BLOQUEANTE
```

**Impacto**:
- âŒ ConversaciÃ³n por turnos rÃ­gidos
- âŒ Silencios absurdos
- âŒ No puede interrumpir
- âŒ No parece natural

---

### 2. âŒ TTS Bloquea STT

**Problema**:
```python
# output_thread.py lÃ­nea 372-374
audio_output = self.melo_tts.synthesize(response_text)  # 600-700ms BLOQUEO
# Durante este tiempo, InputThread NO puede procesar audio del usuario
```

**Consecuencia**:
- Usuario habla â†’ Se pierde
- STT estÃ¡ congelado mientras TTS trabaja
- Primeras palabras del usuario desaparecen

---

### 3. âŒ DetecciÃ³n de Emociones Silenciosa

**Problema**:
```python
# input_thread.py lÃ­nea 264
tone_result = self.audio_emotion.analyze(audio_data, sr=self.sample_rate)
# Si falla, no muestra error, solo pone "silencio"
```

**Debug necesario**:
```bash
# Â¿EstÃ¡ cargado EmotionAudioLite?
# Â¿Tiene el modelo descargado?
# Â¿El audio tiene el formato correcto?
```

---

### 4. âŒ LoRA Ignora Experiencia

**Problema**:
```python
# lora_router.py - NO usa tone_result en la predicciÃ³n
decision = self.lora_router.predict(embedding)  # Solo BERT, no tono
```

**Falta**:
- Integrar tono emocional en LoRA
- Usar ToneMemoryBuffer para contexto
- Adaptar respuesta segÃºn emociÃ³n detectada

---

### 5. âŒ Latencia STT Inicial

**Problema**:
- Vosk tarda ~1-2s en arrancar el recognizer
- Primeros chunks de audio se pierden
- Usuario tiene que repetir

---

## ğŸ¯ Soluciones Propuestas

### FASE 1: Hacer Sistema No-Bloqueante (CRÃTICO)

#### 1.1 Eliminar `_wait_for_user_silence()` Bloqueante

**Antes**:
```python
self._wait_for_user_silence()  # Bloquea hasta 5s
audio_output = self.melo_tts.synthesize(response_text)  # Bloquea 600ms
```

**DespuÃ©s**:
```python
# TTS en thread separado, NO bloquea STT
threading.Thread(target=self._async_tts_and_play, args=(response_text,), daemon=True).start()
```

#### 1.2 TTS AsÃ­ncrono con InterrupciÃ³n

```python
def _async_tts_and_play(self, text: str):
    """TTS en background, puede ser interrumpido si usuario habla"""
    
    # 1. Generar audio
    audio = self.melo_tts.synthesize(text)
    
    # 2. Reproducir en chunks pequeÃ±os (100ms)
    chunk_size = 4410  # 100ms a 44.1kHz
    for i in range(0, len(audio), chunk_size):
        
        # COMPROBAR: Â¿Usuario empezÃ³ a hablar?
        if self.user_speaking:
            print("âš ï¸ Usuario interrumpiÃ³, deteniendo TTS")
            return  # ABORTAR reproducciÃ³n
        
        # Reproducir chunk
        chunk = audio[i:i+chunk_size]
        self._play_audio_chunk(chunk)
```

**Beneficio**:
- âœ… Usuario puede interrumpir en cualquier momento
- âœ… STT siempre activo
- âœ… ConversaciÃ³n fluida

---

### FASE 2: Optimizar Latencias

#### 2.1 Pre-warm Vosk Recognizer

```python
# input_thread.py - load_components()
def load_components(self):
    self.vosk_stt = VoskSTTStreaming()
    self.vosk_session = VoskStreamingSession(self.vosk_stt)
    
    # PRE-WARM: Procesar 100ms de silencio
    dummy_audio = np.zeros(1600, dtype=np.float32)  # 100ms silencio
    self.vosk_session.feed_audio(dummy_audio)
    
    print("âœ… Vosk pre-warmed, listo para audio real")
```

#### 2.2 TTS Streaming (GeneraciÃ³n + ReproducciÃ³n SimultÃ¡nea)

```python
# MeloTTS no soporta streaming nativo, pero podemos:
# 1. Dividir texto en frases
# 2. Sintetizar primera frase
# 3. Mientras reproduce frase 1, sintetizar frase 2
# 4. Reducir latencia percibida ~50%
```

---

### FASE 3: Arreglar DetecciÃ³n de Emociones

#### 3.1 Debug EmotionAudioLite

```python
# input_thread.py - load_components()
def load_components(self):
    try:
        self.audio_emotion = EmotionAudioLite()
        
        # TEST: Audio de prueba
        test_audio = np.random.randn(16000).astype(np.float32) * 0.01
        test_result = self.audio_emotion.analyze(test_audio, sr=16000)
        
        print(f"âœ… EmotionAudio funcionando: {test_result.label}")
        
    except Exception as e:
        print(f"âŒ EmotionAudio FALLÃ“: {e}")
        print("   Instalando dependencias...")
        # Fallback o instalaciÃ³n automÃ¡tica
```

#### 3.2 Logging de Emociones Detallado

```python
# input_thread.py - lÃ­nea 264
tone_result = self.audio_emotion.analyze(audio_data, sr=self.sample_rate)

# AÃ‘ADIR DEBUG:
print(f"ğŸ­ EmociÃ³n: {tone_result.label} ({tone_result.confidence:.2f})")
print(f"   Valence: {tone_result.valence:.2f}, Arousal: {tone_result.arousal:.2f}")
```

---

### FASE 4: Integrar Tono en LoRA

#### 4.1 Expandir Input de LoRA

**Antes**:
```python
decision = self.lora_router.predict(embedding)  # Solo BERT (768D)
```

**DespuÃ©s**:
```python
# Concatenar BERT + Tono emocional
tone_features = np.array([
    tone_result.valence,
    tone_result.arousal,
    tone_result.confidence
])

combined_input = np.concatenate([embedding, tone_features])  # 771D
decision = self.lora_router.predict(combined_input)
```

#### 4.2 Adaptar Respuesta segÃºn EmociÃ³n

```python
# Si usuario estÃ¡ triste â†’ Respuesta mÃ¡s empÃ¡tica
if tone_result.label == "triste":
    # Activar estilo "empathy_support" en ToneBridge
    self.tone_bridge.set_active_style("empathy_support")

# Si usuario estÃ¡ enojado â†’ Respuesta mÃ¡s calmada
elif tone_result.label == "enojado":
    self.tone_bridge.set_active_style("calming_neutral")
```

---

### FASE 5: Mejorar VAD (Voice Activity Detection)

#### 5.1 VAD MÃ¡s Inteligente

**Problema actual**:
```python
# input_thread.py - lÃ­nea 194
energy = np.sqrt(np.mean(audio_float32 ** 2))
if energy > self.vad_energy_threshold:  # Muy simple
```

**Mejora**:
```python
# Usar webrtcvad (mÃ¡s robusto)
import webrtcvad

vad = webrtcvad.Vad(2)  # Agresividad 0-3
is_speech = vad.is_speech(audio_bytes, sample_rate)
```

**Beneficio**:
- âœ… Detecta voz vs ruido de fondo
- âœ… Menos falsos positivos
- âœ… No corta por ruidos ambientales

---

## ğŸ“Š Comparativa: Antes vs DespuÃ©s

| Aspecto | Antes (Bloqueante) | DespuÃ©s (AsÃ­ncrono) |
|---------|-------------------|---------------------|
| **InterrupciÃ³n** | âŒ Imposible | âœ… Inmediata |
| **Latencia respuesta** | 2-3s | 0.6-0.8s |
| **STT mientras TTS** | âŒ Bloqueado | âœ… Activo |
| **Emociones** | âŒ No funciona | âœ… Detectadas |
| **Naturalidad** | â­â­ (2/5) | â­â­â­â­â­ (5/5) |

---

## ğŸš€ Plan de ImplementaciÃ³n

### Prioridad 1 (HOY):
1. âœ… Ajustar VAD timeouts (YA HECHO)
2. ğŸ”§ TTS asÃ­ncrono no-bloqueante
3. ğŸ”§ Debug detecciÃ³n de emociones

### Prioridad 2 (MAÃ‘ANA):
4. ğŸ”§ Pre-warm Vosk
5. ğŸ”§ Integrar tono en LoRA
6. ğŸ”§ Sistema de interrupciÃ³n

### Prioridad 3 (OPCIONAL):
7. ğŸ”§ TTS streaming por frases
8. ğŸ”§ webrtcvad reemplazo
9. ğŸ”§ Fillers mientras procesa

---

## ğŸ¯ Objetivo Final

**ConversaciÃ³n Natural**:
```
Usuario: "Hola [pausa] Â¿cÃ³mo estÃ¡s?"
          â†“ (STT detecta en tiempo real)
SARAi:   [Empieza a responder ANTES de que termine]
         "Hola! Estoy bien, gracias por..."
          â†“
Usuario: [INTERRUMPE] "Espera, tengo una pregunta"
          â†“
SARAi:   [DETIENE audio inmediatamente]
         [PROCESA nueva pregunta]
```

**CaracterÃ­sticas clave**:
- âœ… Respuesta rÃ¡pida (< 1s percibido)
- âœ… Puede ser interrumpida
- âœ… Detecta emociones
- âœ… Adapta tono
- âœ… No bloquea STT
- âœ… ConversaciÃ³n fluida

---

**PrÃ³ximo paso**: Â¿Empezamos con TTS asÃ­ncrono + debug de emociones?
