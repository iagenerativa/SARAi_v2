# SARAi v2.16 - Resumen de Benchmarks

## üìä Stack v2.16 FINAL - Configuraci√≥n H√≠brida

Fecha: 28 de octubre de 2025

### Modelos Seleccionados

| Modelo | Backend | Tama√±o | RAM (GB) | Tok/s | Rol |
|--------|---------|--------|----------|-------|-----|
| **SOLAR-10.7B** (solar:10.7b) | **Ollama** | 6.1 GB | 6.1 | **3.36** | Expert (Hard Skills) |
| **LFM2-1.2B** (Q4_K_M) | **llama-cpp-python** | 0.68 GB | 1.18 | **23.10** | Tiny Tier (Soft Skills) |
| **Qwen3-VL-4B-Instruct** (Q5_K_M) | **llama-cpp-python** | 2.27 GB | 2.36 | **7.97** | Omni-Loop (Reflexi√≥n + Audio) |
| **TOTAL** | - | **9.05 GB** | **9.64 GB** | - | - |

### ‚úÖ KPIs v2.16 CUMPLIDOS

- ‚úÖ **RAM Total**: 9.64 GB < 12 GB target
- ‚úÖ **Expert Speed**: 3.36 tok/s ‚â• 3.0 target
- ‚úÖ **Tiny Speed**: 23.10 tok/s >> 8.0 target (2.9x mejor)
- ‚úÖ **Omni Speed**: 7.97 tok/s ‚â• 7.0 target
- ‚úÖ **Almacenamiento**: 9.05 GB (razonable para v2.16)

### Comparativa: Nativo vs H√≠brido

#### Estrategia 100% Nativa (llama-cpp-python)

| M√©trica | Valor | Estado |
|---------|-------|--------|
| RAM Total | 14.09 GB | ‚ùå Excede l√≠mite |
| SOLAR Speed | 2.58 tok/s | ‚ùå Lento |
| LFM2 Speed | 23.10 tok/s | ‚úÖ Excelente |
| Qwen Speed | 7.97 tok/s | ‚úÖ Bueno |

**Problema**: SOLAR consume 10.55 GB RAM solo.

#### Estrategia H√≠brida (Ollama + llama-cpp) ‚úÖ

| M√©trica | Valor | Estado |
|---------|-------|--------|
| RAM Total | 9.64 GB | ‚úÖ Dentro del l√≠mite |
| SOLAR Speed | 3.36 tok/s | ‚úÖ 30% m√°s r√°pido |
| LFM2 Speed | 23.10 tok/s | ‚úÖ Mantiene rendimiento |
| Qwen Speed | 7.97 tok/s | ‚úÖ Mantiene rendimiento |

**Beneficio**: Ollama optimiza SOLAR (menos RAM, m√°s velocidad).

### Tiempos de Carga

| Modelo | Tiempo de Carga | Backend |
|--------|----------------|---------|
| SOLAR-10.7B | ~54s | Ollama (optimizado) |
| LFM2-1.2B | 6.5s | llama-cpp-python |
| Qwen-Omni-3B | 20.5s | llama-cpp-python |

**Total cold-start**: ~81s (con carga secuencial)

### Capacidades Multimodales

| Modelo | Texto | Audio | Imagen |
|--------|-------|-------|--------|
| SOLAR-10.7B | ‚úÖ | ‚ùå | ‚ùå |
| LFM2-1.2B | ‚úÖ | ‚ùå | ‚ùå |
| Qwen-Omni-3B | ‚úÖ | ‚ö†Ô∏è Experimental | ‚ö†Ô∏è Limitado |

**Nota**: Qwen-Omni-3B tiene arquitectura para audio/imagen pero requiere:
- **Audio**: Procesamiento con torchaudio/librosa antes del LLM
- **Imagen**: Projector CLIP separado (2.6GB adicionales)

### Decisi√≥n de Arquitectura v2.16

```yaml
# Stack h√≠brido recomendado
models:
  expert:
    name: "solar:10.7b"
    backend: "ollama"  # Optimizado
    ram_gb: 6.1
    tok_s: 3.36
    
  tiny:
    name: "LFM2-1.2B-Q4_K_M.gguf"
    backend: "llama-cpp-python"  # Control nativo
    ram_gb: 1.18
    tok_s: 23.10
    
  omni_loop:
    name: "Qwen3-VL-4B-Instruct-Q5_K_M.gguf"
    backend: "llama-cpp-python"  # Multimodal futuro
    ram_gb: 2.36
    tok_s: 7.97
```

### Roadmap Multimodal

**v2.16** (Actual):
- ‚úÖ Stack de texto completo
- ‚úÖ Wrapper nativo para Qwen-Omni
- ‚è≥ Audio/imagen experimental

**v2.17** (Futuro):
- üéØ Projector CLIP para im√°genes
- üéØ Pipeline de audio con torchaudio
- üéØ TTS (Text-to-Speech) para respuestas de voz

### Archivos de Resultados

- **Benchmark nativo**: `logs/model_benchmark_1761688531.json`
- **Tests quick**: 
  - `scripts/quick_test_solar.sh` (SOLAR)
  - `scripts/quick_test_lfm2.sh` (LFM2)
- **Wrapper nativo**: `agents/omni_native.py`
- **Test h√≠brido**: `scripts/test_hybrid_stack.py`

### Conclusi√≥n

**El stack h√≠brido es la mejor opci√≥n para v2.16**:

‚úÖ Cumple todos los KPIs de RAM y velocidad  
‚úÖ Aprovecha optimizaciones de Ollama para SOLAR  
‚úÖ Mantiene control nativo para LFM2 y Qwen-Omni  
‚úÖ Base s√≥lida para expansi√≥n multimodal en v2.17  

**RAM Total**: 9.64 GB (80% del l√≠mite de 12GB)  
**Margen para expansi√≥n**: 2.36 GB disponibles para skills adicionales o projector CLIP
