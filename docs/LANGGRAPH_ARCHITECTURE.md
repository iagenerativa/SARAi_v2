# Arquitectura LangGraph de SARAi v2.11

**Respuesta rÃ¡pida**: **SÃ, SARAi estÃ¡ 100% optimizado con LangGraph** desde el inicio del proyecto.

---

## ðŸ—ï¸ Arquitectura Completa

SARAi usa **LangGraph** (del ecosistema LangChain) como orquestador central de todo el flujo de procesamiento. No es un "aÃ±adido posterior", es el **nÃºcleo arquitectÃ³nico** del sistema.

### Stack LangChain Completo

```yaml
# requirements.txt
langchain>=0.1.0          # Framework base
langchain-core>=0.1.0     # Abstracciones core (Runnable, etc.)
langgraph>=0.0.40         # OrquestaciÃ³n de estado
```

---

## ðŸ“Š Grafo de Estado (StateGraph)

El flujo completo de SARAi estÃ¡ definido como un **StateGraph** en `core/graph.py`:

```python
from langgraph.graph import StateGraph, END

class State(TypedDict):
    """Estado compartido en el flujo LangGraph"""
    input: str                    # Query del usuario
    hard: float                   # Score tÃ©cnico (TRM)
    soft: float                   # Score emocional (TRM)
    web_query: float              # Score bÃºsqueda web (v2.10)
    alpha: float                  # Peso hard (MCP)
    beta: float                   # Peso soft (MCP)
    agent_used: str               # "expert" | "tiny" | "rag"
    response: str                 # Respuesta final
    feedback: float               # Feedback implÃ­cito
    rag_metadata: dict            # Metadata RAG (v2.10)
```

### Flujo de Nodos (v2.10)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ENTRADA (Input)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   classify    â”‚  TRM-Router + EmbeddingGemma
                   â”‚ (hard/soft/   â”‚  Clasifica intenciÃ³n
                   â”‚  web_query)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚      mcp      â”‚  Meta Control Plane
                   â”‚  (Î±, Î² pesos) â”‚  Calcula pesos dinÃ¡micos
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ ROUTING LOGIC  â”‚
                  â”‚ (condicional)  â”‚
                  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”˜
                       â†“   â†“   â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                  â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚generate_expertâ”‚  â”‚ execute_rag   â”‚  â”‚generate_tiny â”‚
â”‚  (SOLAR LLM)  â”‚  â”‚(BÃºsqueda web) â”‚  â”‚  (LFM2 LLM)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   feedback    â”‚  Logger asÃ­ncrono
                   â”‚  (implÃ­cito)  â”‚  Aprendizaje continuo
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                          END
```

---

## ðŸ”€ Routing Condicional (add_conditional_edges)

El poder de LangGraph estÃ¡ en el **routing dinÃ¡mico** basado en el estado:

```python
# core/graph.py
workflow.add_conditional_edges(
    "mcp",
    self._route_to_agent,
    {
        "expert": "generate_expert",
        "tiny": "generate_tiny",
        "rag": "execute_rag"
    }
)

def _route_to_agent(self, state: State) -> str:
    """
    LÃ³gica de decisiÃ³n inteligente:
    1. Si web_query > 0.7 â†’ RAG (bÃºsqueda web)
    2. Si alpha > 0.7 â†’ Expert (SOLAR)
    3. Else â†’ Tiny (LFM2)
    """
    if state.get("web_query", 0.0) > 0.7:
        return "rag"
    
    if state["alpha"] > 0.7:
        return "expert"
    
    return "tiny"
```

**Ventajas sobre flujos lineales**:
- âœ… **Decisiones dinÃ¡micas**: No todos los queries pasan por todos los LLMs
- âœ… **Eficiencia RAM**: Solo carga el modelo necesario
- âœ… **Latencia optimizada**: Rutas cortas (tiny) vs largas (expert)
- âœ… **Extensibilidad**: AÃ±adir nodos nuevos sin romper flujo

---

## ðŸ§© IntegraciÃ³n de Componentes

Cada componente de SARAi es un **nodo LangGraph**:

### 1. Nodo: classify (TRM-Router)

```python
def _classify_intent(self, state: State) -> dict:
    """Nodo: Clasificar hard/soft/web_query intent"""
    user_input = state["input"]
    
    # TRM-Router usa embeddings para clasificar
    embedding = self.embedding_model.encode(user_input)
    scores = self.trm_classifier.invoke(embedding_tensor)
    
    return {
        "hard": scores["hard"],
        "soft": scores["soft"],
        "web_query": scores.get("web_query", 0.0)
    }
```

**Output**: Actualiza `state["hard"]`, `state["soft"]`, `state["web_query"]`

### 2. Nodo: mcp (Meta Control Plane)

```python
def _compute_weights(self, state: State) -> dict:
    """Nodo: Calcular pesos Î±/Î² con MCP"""
    alpha, beta = self.mcp.compute_weights(
        state["hard"], 
        state["soft"]
    )
    
    return {"alpha": alpha, "beta": beta}
```

**Output**: Actualiza `state["alpha"]`, `state["beta"]`

### 3. Nodo: generate_expert (SOLAR)

```python
def _generate_expert(self, state: State) -> dict:
    """Nodo: Generar respuesta tÃ©cnica con SOLAR"""
    from core.model_pool import get_model_pool
    
    pool = get_model_pool()
    solar = pool.get("expert_short")  # o "expert_long"
    
    response = solar(state["input"], max_tokens=512)
    
    return {
        "response": response["choices"][0]["text"],
        "agent_used": "expert"
    }
```

**Output**: Actualiza `state["response"]`, `state["agent_used"]`

### 4. Nodo: execute_rag (BÃºsqueda Web + LLM)

```python
# agents/rag_agent.py
def create_rag_node(model_pool):
    """Factory para nodo RAG compatible con LangGraph"""
    
    def rag_node(state: State) -> dict:
        # 1. BÃºsqueda web
        results = cached_search(state["input"])
        
        # 2. SÃ­ntesis con LLM
        prompt = build_rag_prompt(state["input"], results)
        llm = model_pool.get("expert_long")
        response = llm(prompt)
        
        # 3. AuditorÃ­a
        log_web_query(state["input"], results, response)
        
        return {
            "response": response,
            "agent_used": "rag",
            "rag_metadata": {
                "snippets_count": len(results),
                "source": "searxng"
            }
        }
    
    return rag_node
```

**Output**: Actualiza `state["response"]`, `state["rag_metadata"]`

---

## ðŸŽ¯ Ventajas de LangGraph en SARAi

| Aspecto | Sin LangGraph | Con LangGraph (SARAi) |
|---------|---------------|------------------------|
| **Control de flujo** | if/else anidados, difÃ­cil seguir | Grafo visual, fÃ¡cil debuggear |
| **Estado compartido** | Pasar dictionaries manualmente | TypedDict automÃ¡tico |
| **Routing** | LÃ³gica dispersa en funciones | `add_conditional_edges` declarativo |
| **Debugging** | Print statements, difÃ­cil trace | LangSmith, visualizaciÃ³n de grafo |
| **Testing** | Mockear cada funciÃ³n individualmente | Testear nodos independientes |
| **Extensibilidad** | Modificar flujo = refactor masivo | AÃ±adir nodo = 3 lÃ­neas |
| **Persistencia** | Implementar manualmente | Checkpointing built-in |
| **ParalelizaciÃ³n** | Threading manual complejo | `add_parallel_edges` automÃ¡tico |

---

## ðŸ”„ Flujo Completo (Ejemplo Real)

**Input**: "Â¿CÃ³mo estÃ¡ el clima en Tokio?"

```
1. classify:
   - hard: 0.2
   - soft: 0.1
   - web_query: 0.9  â† DETECTA bÃºsqueda web

2. mcp:
   - alpha: 0.3
   - beta: 0.7
   (No importa porque web_query > 0.7)

3. _route_to_agent:
   - DecisiÃ³n: "rag"  â† Va a bÃºsqueda web

4. execute_rag:
   - cached_search("Â¿CÃ³mo estÃ¡ el clima en Tokio?")
   - Encuentra 5 snippets de weather.com
   - SÃ­ntesis con SOLAR
   - response: "SegÃºn weather.com, el clima actual en Tokio es..."
   - rag_metadata: {snippets_count: 5, source: "searxng"}

5. feedback:
   - log_web_query(...) â†’ AuditorÃ­a HMAC
   - Feedback implÃ­cito detectado (continuaciÃ³n de diÃ¡logo)

6. END
```

**Latencia total**: ~25-30s (P50 RAG)

---

## ðŸ§ª Testing con LangGraph

LangGraph facilita enormemente el testing:

```python
# tests/test_graph.py
def test_web_query_routing():
    """Verifica que web_query > 0.7 enruta a RAG"""
    
    orchestrator = SARAiOrchestrator()
    
    # Estado inicial
    state = {
        "input": "Â¿CÃ³mo estÃ¡ el clima en Madrid?",
        "hard": 0.2,
        "soft": 0.1,
        "web_query": 0.9,  # â† Forzar web_query alto
        "alpha": 0.5,
        "beta": 0.5
    }
    
    # Ejecutar solo el routing
    route = orchestrator._route_to_agent(state)
    
    assert route == "rag"  # âœ… Correcto
```

**Sin LangGraph**, tendrÃ­as que mockear todo el flujo completo.

---

## ðŸ“š Recursos LangChain/LangGraph

- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **LangChain Core**: https://python.langchain.com/docs/
- **LangSmith** (debugging): https://smith.langchain.com/
- **StateGraph Tutorial**: https://langchain-ai.github.io/langgraph/tutorials/introduction/

---

## ðŸŽ“ PatrÃ³n de DiseÃ±o: State Machine como Grafo

SARAi implementa el patrÃ³n **Finite State Machine** (FSM) usando **StateGraph**:

```
Estados:
- CLASSIFY: Analiza intenciÃ³n
- MCP: Calcula pesos
- EXPERT: Genera con SOLAR
- TINY: Genera con LFM2
- RAG: Busca en web + sintetiza
- FEEDBACK: Aprende

Transiciones:
- CLASSIFY â†’ MCP (siempre)
- MCP â†’ EXPERT (si alpha > 0.7)
- MCP â†’ TINY (si alpha <= 0.7 y web_query <= 0.7)
- MCP â†’ RAG (si web_query > 0.7)
- [EXPERT|TINY|RAG] â†’ FEEDBACK (siempre)
- FEEDBACK â†’ END (siempre)
```

**Ventaja clave**: Cada estado (nodo) es **stateless** individualmente, pero el grafo mantiene el estado global.

---

## ðŸš€ ConclusiÃ³n

**SARAi NO es un proyecto "con LangGraph aÃ±adido"**.

**SARAi ES un proyecto LangGraph desde su concepciÃ³n**.

Todo el sistema (TRM, MCP, LLMs, RAG, feedback) estÃ¡ orquestado mediante:
- âœ… **StateGraph**: Flujo de estado
- âœ… **TypedDict**: Estado tipado
- âœ… **add_conditional_edges**: Routing inteligente
- âœ… **Nodos independientes**: Modularidad extrema
- âœ… **Runnable Protocol**: Interop con LangChain

**Si quitas LangGraph de SARAi, el sistema deja de existir**.

---

**Ãšltima actualizaciÃ³n**: 2025-10-28  
**VersiÃ³n SARAi**: v2.11  
**LangGraph versiÃ³n**: >=0.0.40
