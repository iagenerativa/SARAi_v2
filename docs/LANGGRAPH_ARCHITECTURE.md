# Arquitectura LangGraph de SARAi v2.11

**Respuesta r√°pida**: **S√ç, SARAi est√° 100% optimizado con LangGraph** desde el inicio del proyecto.

---

## üèóÔ∏è Arquitectura Completa

SARAi usa **LangGraph** (del ecosistema LangChain) como orquestador central de todo el flujo de procesamiento. No es un "a√±adido posterior", es el **n√∫cleo arquitect√≥nico** del sistema.

### Stack LangChain Completo

```yaml
# requirements.txt
langchain>=0.1.0          # Framework base
langchain-core>=0.1.0     # Abstracciones core (Runnable, etc.)
langgraph>=0.0.40         # Orquestaci√≥n de estado
```

---

## üìä Grafo de Estado (StateGraph)

El flujo completo de SARAi est√° definido como un **StateGraph** en `core/graph.py`:

```python
from langgraph.graph import StateGraph, END

class State(TypedDict):
    """Estado compartido en el flujo LangGraph"""
    input: str                    # Query del usuario
    hard: float                   # Score t√©cnico (TRM)
    soft: float                   # Score emocional (TRM)
    web_query: float              # Score b√∫squeda web (v2.10)
    alpha: float                  # Peso hard (MCP)
    beta: float                   # Peso soft (MCP)
    agent_used: str               # "expert" | "tiny" | "rag"
    response: str                 # Respuesta final
    feedback: float               # Feedback impl√≠cito
    rag_metadata: dict            # Metadata RAG (v2.10)
```

### Flujo de Nodos (v2.10)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       ENTRADA (Input)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ   classify    ‚îÇ  TRM-Router + EmbeddingGemma
                   ‚îÇ (hard/soft/   ‚îÇ  Clasifica intenci√≥n
                   ‚îÇ  web_query)   ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ      mcp      ‚îÇ  Meta Control Plane
                   ‚îÇ  (Œ±, Œ≤ pesos) ‚îÇ  Calcula pesos din√°micos
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ ROUTING LOGIC  ‚îÇ
                  ‚îÇ (condicional)  ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì   ‚Üì   ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                  ‚Üì                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇgenerate_expert‚îÇ  ‚îÇ execute_rag   ‚îÇ  ‚îÇgenerate_tiny ‚îÇ
‚îÇ  (SOLAR LLM)  ‚îÇ  ‚îÇ(B√∫squeda web) ‚îÇ  ‚îÇ  (LFM2 LLM)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                  ‚îÇ                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ   feedback    ‚îÇ  Logger as√≠ncrono
                   ‚îÇ  (impl√≠cito)  ‚îÇ  Aprendizaje continuo
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                          END
```

---

## üîÄ Routing Condicional (add_conditional_edges)

El poder de LangGraph est√° en el **routing din√°mico** basado en el estado:

```python
# core/graph.py
workflow.add_conditional_edges(
    "mcp",
    self._route_to_agent,
    {
        "expert": "generate_expert",
        "tiny": "generate_tiny",
        "rag": "execute_rag"
    }
)

def _route_to_agent(self, state: State) -> str:
    """
    L√≥gica de decisi√≥n inteligente:
    1. Si web_query > 0.7 ‚Üí RAG (b√∫squeda web)
    2. Si alpha > 0.7 ‚Üí Expert (SOLAR)
    3. Else ‚Üí Tiny (LFM2)
    """
    if state.get("web_query", 0.0) > 0.7:
        return "rag"
    
    if state["alpha"] > 0.7:
        return "expert"
    
    return "tiny"
```

**Ventajas sobre flujos lineales**:
- ‚úÖ **Decisiones din√°micas**: No todos los queries pasan por todos los LLMs
- ‚úÖ **Eficiencia RAM**: Solo carga el modelo necesario
- ‚úÖ **Latencia optimizada**: Rutas cortas (tiny) vs largas (expert)
- ‚úÖ **Extensibilidad**: A√±adir nodos nuevos sin romper flujo

---

## üß© Integraci√≥n de Componentes

Cada componente de SARAi es un **nodo LangGraph**:

### 1. Nodo: classify (TRM-Router)

```python
def _classify_intent(self, state: State) -> dict:
    """Nodo: Clasificar hard/soft/web_query intent"""
    user_input = state["input"]
    
    # TRM-Router usa embeddings para clasificar
    embedding = self.embedding_model.encode(user_input)
    scores = self.trm_classifier.invoke(embedding_tensor)
    
    return {
        "hard": scores["hard"],
        "soft": scores["soft"],
        "web_query": scores.get("web_query", 0.0)
    }
```

**Output**: Actualiza `state["hard"]`, `state["soft"]`, `state["web_query"]`

### 2. Nodo: mcp (Meta Control Plane)

```python
def _compute_weights(self, state: State) -> dict:
    """Nodo: Calcular pesos Œ±/Œ≤ con MCP"""
    alpha, beta = self.mcp.compute_weights(
        state["hard"], 
        state["soft"]
    )
    
    return {"alpha": alpha, "beta": beta}
```

**Output**: Actualiza `state["alpha"]`, `state["beta"]`

### 3. Nodo: generate_expert (SOLAR)

```python
def _generate_expert(self, state: State) -> dict:
    """Nodo: Generar respuesta t√©cnica con SOLAR"""
    from core.model_pool import get_model_pool
    
    pool = get_model_pool()
    solar = pool.get("expert_short")  # o "expert_long"
    
    response = solar(state["input"], max_tokens=512)
    
    return {
        "response": response["choices"][0]["text"],
        "agent_used": "expert"
    }
```

**Output**: Actualiza `state["response"]`, `state["agent_used"]`

### 4. Nodo: execute_rag (B√∫squeda Web + LLM)

```python
# agents/rag_agent.py
def create_rag_node(model_pool):
    """Factory para nodo RAG compatible con LangGraph"""
    
    def rag_node(state: State) -> dict:
        # 1. B√∫squeda web
        results = cached_search(state["input"])
        
        # 2. S√≠ntesis con LLM
        prompt = build_rag_prompt(state["input"], results)
        llm = model_pool.get("expert_long")
        response = llm(prompt)
        
        # 3. Auditor√≠a
        log_web_query(state["input"], results, response)
        
        return {
            "response": response,
            "agent_used": "rag",
            "rag_metadata": {
                "snippets_count": len(results),
                "source": "searxng"
            }
        }
    
    return rag_node
```

**Output**: Actualiza `state["response"]`, `state["rag_metadata"]`

---

## üéØ Ventajas de LangGraph en SARAi

| Aspecto | Sin LangGraph | Con LangGraph (SARAi) |
|---------|---------------|------------------------|
| **Control de flujo** | if/else anidados, dif√≠cil seguir | Grafo visual, f√°cil debuggear |
| **Estado compartido** | Pasar dictionaries manualmente | TypedDict autom√°tico |
| **Routing** | L√≥gica dispersa en funciones | `add_conditional_edges` declarativo |
| **Debugging** | Print statements, dif√≠cil trace | LangSmith, visualizaci√≥n de grafo |
| **Testing** | Mockear cada funci√≥n individualmente | Testear nodos independientes |
| **Extensibilidad** | Modificar flujo = refactor masivo | A√±adir nodo = 3 l√≠neas |
| **Persistencia** | Implementar manualmente | Checkpointing built-in |
| **Paralelizaci√≥n** | Threading manual complejo | `add_parallel_edges` autom√°tico |

---

## üîÑ Flujo Completo (Ejemplo Real)

**Input**: "¬øC√≥mo est√° el clima en Tokio?"

```
1. classify:
   - hard: 0.2
   - soft: 0.1
   - web_query: 0.9  ‚Üê DETECTA b√∫squeda web

2. mcp:
   - alpha: 0.3
   - beta: 0.7
   (No importa porque web_query > 0.7)

3. _route_to_agent:
   - Decisi√≥n: "rag"  ‚Üê Va a b√∫squeda web

4. execute_rag:
   - cached_search("¬øC√≥mo est√° el clima en Tokio?")
   - Encuentra 5 snippets de weather.com
   - S√≠ntesis con SOLAR
   - response: "Seg√∫n weather.com, el clima actual en Tokio es..."
   - rag_metadata: {snippets_count: 5, source: "searxng"}

5. feedback:
   - log_web_query(...) ‚Üí Auditor√≠a HMAC
   - Feedback impl√≠cito detectado (continuaci√≥n de di√°logo)

6. END
```

**Latencia total**: ~25-30s (P50 RAG)

---

## üß™ Testing con LangGraph

LangGraph facilita enormemente el testing:

```python
# tests/test_graph.py
def test_web_query_routing():
    """Verifica que web_query > 0.7 enruta a RAG"""
    
    orchestrator = SARAiOrchestrator()
    
    # Estado inicial
    state = {
        "input": "¬øC√≥mo est√° el clima en Madrid?",
        "hard": 0.2,
        "soft": 0.1,
        "web_query": 0.9,  # ‚Üê Forzar web_query alto
        "alpha": 0.5,
        "beta": 0.5
    }
    
    # Ejecutar solo el routing
    route = orchestrator._route_to_agent(state)
    
    assert route == "rag"  # ‚úÖ Correcto
```

**Sin LangGraph**, tendr√≠as que mockear todo el flujo completo.

---

## üìö Recursos LangChain/LangGraph

- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **LangChain Core**: https://python.langchain.com/docs/
- **LangSmith** (debugging): https://smith.langchain.com/
- **StateGraph Tutorial**: https://langchain-ai.github.io/langgraph/tutorials/introduction/

---

## üéì Patr√≥n de Dise√±o: State Machine como Grafo

SARAi implementa el patr√≥n **Finite State Machine** (FSM) usando **StateGraph**:

```
Estados:
- CLASSIFY: Analiza intenci√≥n
- MCP: Calcula pesos
- EXPERT: Genera con SOLAR
- TINY: Genera con LFM2
- RAG: Busca en web + sintetiza
- FEEDBACK: Aprende

Transiciones:
- CLASSIFY ‚Üí MCP (siempre)
- MCP ‚Üí EXPERT (si alpha > 0.7)
- MCP ‚Üí TINY (si alpha <= 0.7 y web_query <= 0.7)
- MCP ‚Üí RAG (si web_query > 0.7)
- [EXPERT|TINY|RAG] ‚Üí FEEDBACK (siempre)
- FEEDBACK ‚Üí END (siempre)
```

**Ventaja clave**: Cada estado (nodo) es **stateless** individualmente, pero el grafo mantiene el estado global.

---

## üöÄ Conclusi√≥n

**SARAi NO es un proyecto "con LangGraph a√±adido"**.

**SARAi ES un proyecto LangGraph desde su concepci√≥n**.

Todo el sistema (TRM, MCP, LLMs, RAG, feedback) est√° orquestado mediante:
- ‚úÖ **StateGraph**: Flujo de estado
- ‚úÖ **TypedDict**: Estado tipado
- ‚úÖ **add_conditional_edges**: Routing inteligente
- ‚úÖ **Nodos independientes**: Modularidad extrema
- ‚úÖ **Runnable Protocol**: Interop con LangChain

**Si quitas LangGraph de SARAi, el sistema deja de existir**.

---

**√öltima actualizaci√≥n**: 2025-10-28  
**Versi√≥n SARAi**: v2.11  
**LangGraph versi√≥n**: >=0.0.40
