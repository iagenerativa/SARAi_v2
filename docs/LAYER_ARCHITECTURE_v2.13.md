# ğŸ—ï¸ Arquitectura de 3 Layers - SARAi v2.13

## ğŸ“Š VisiÃ³n General

SARAi utiliza una arquitectura de **3 capas** para procesar informaciÃ³n de forma modular:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INPUT (Audio/Texto)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LAYER 1: I/O       â”‚
              â”‚   (Input/Output)     â”‚
              â”‚                      â”‚
              â”‚ â€¢ Captura audio      â”‚
              â”‚ â€¢ STT (Vosk)         â”‚
              â”‚ â€¢ Emotion detection  â”‚
              â”‚ â€¢ VAD (Sherpa)       â”‚
              â”‚ â€¢ TTS (MeloTTS)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LAYER 2: Memory    â”‚
              â”‚   (Contexto/Tono)    â”‚
              â”‚                      â”‚
              â”‚ â€¢ Tone Memory Buffer â”‚
              â”‚ â€¢ Persistencia JSONL â”‚
              â”‚ â€¢ Historial emocionalâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LAYER 3: Fluidity  â”‚
              â”‚   (Transiciones)     â”‚
              â”‚                      â”‚
              â”‚ â€¢ Tone Bridge        â”‚
              â”‚ â€¢ Style inference    â”‚
              â”‚ â€¢ Filler hints       â”‚
              â”‚ â€¢ Smooth transitions â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    GRAPH (Core)      â”‚
              â”‚  TRM â†’ MCP â†’ LLM     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Layer 1: I/O (Input/Output)

### PropÃ³sito
**Captura y entrega de datos** (audio, texto, imagen).

### Componentes

#### 1. `true_fullduplex.py` - Multiprocessing I/O âš¡
**FunciÃ³n**: Sistema full-duplex real con 3 procesos independientes

**CaracterÃ­sticas**:
- âœ… Sin GIL compartido (paralelismo REAL)
- âœ… Latencia interrupciÃ³n <10ms
- âœ… Audio duplex nativo (PortAudio)

**Procesos**:
```python
Process 1: Input  â†’ STT (Vosk) + Emotion Detection
Process 2: Output â†’ LLM (SOLAR/LFM2) + TTS (MeloTTS)
Process 3: Main   â†’ Orchestration + User I/O
```

#### 2. `audio_emotion_lite.py` - DetecciÃ³n Emocional ğŸ­
**FunciÃ³n**: AnÃ¡lisis ligero de tono/emociÃ³n desde audio

**Features extraÃ­das**:
- Pitch (media, std, jitter)
- EnergÃ­a (RMS)
- MFCC (13 coeficientes)
- Spectral features (centroid, rolloff, flux)
- Formantes (F1, F2)

**Modelo**: RandomForest (CPU-friendly)

**Emociones detectadas**:
- neutral
- happy
- sad
- angry
- fearful

**API**:
```python
from core.layer1_io.audio_emotion_lite import detect_emotion

# Desde archivo de audio
emotion_scores = detect_emotion("audio.wav")
# â†’ {"neutral": 0.1, "happy": 0.7, "sad": 0.1, "angry": 0.05, "fearful": 0.05}

# Desde array numpy
emotion_scores = detect_emotion(audio_array, sr=16000)
```

**Output**:
```python
{
    "label": "happy",           # EmociÃ³n dominante
    "scores": {...},            # Scores por emociÃ³n
    "valence": 0.8,            # Positivo/negativo
    "arousal": 0.6,            # EnergÃ­a/activaciÃ³n
    "confidence": 0.7          # Confianza del modelo
}
```

#### 3. `vosk_streaming.py` - STT Streaming ğŸ¤
**FunciÃ³n**: Speech-to-Text en tiempo real con Vosk

**CaracterÃ­sticas**:
- Modelo: vosk-model-small-es-0.42 (91 MB)
- Latency: ~50ms
- CPU-only, sin GPU requerida

#### 4. `sherpa_vad.py` - Voice Activity Detection ğŸ”Š
**FunciÃ³n**: Detecta cuando el usuario estÃ¡ hablando

**Uso**: Evita procesar silencio innecesario

#### 5. `lora_router.py` - Routing DinÃ¡mico de Modelos ğŸ”€
**FunciÃ³n**: Enruta a diferentes checkpoints LoRA segÃºn contexto

**Ejemplo**:
```python
# Ruta a checkpoint optimizado para cÃ³digo
lora_checkpoint = lora_router.route("Escribe cÃ³digo Python")
# â†’ "state/lora_programming.pt"
```

---

## ğŸ§  Layer 2: Memory (Contexto/Tono)

### PropÃ³sito
**Persistencia de contexto y tono emocional** para mantener coherencia en conversaciones largas.

### Componentes

#### 1. `tone_memory.py` - Memoria de Tono ğŸ“
**FunciÃ³n**: Buffer persistente de eventos de tono

**CaracterÃ­sticas**:
- Persistencia JSONL (no se pierde al reiniciar)
- Buffer en memoria (deque, max 256 entries)
- Thread-safe (con locks)

**API**:
```python
from core.layer2_memory.tone_memory import ToneMemoryBuffer

tone_memory = ToneMemoryBuffer()

# Agregar entrada
tone_memory.append({
    "label": "happy",
    "valence": 0.8,
    "arousal": 0.6,
    "timestamp": time.time()
})

# Obtener entradas recientes
recent_tones = tone_memory.recent(limit=10)
# â†’ [{"label": "happy", "valence": 0.8, ...}, ...]

# Iterar sobre historial
for entry in tone_memory.iter_recent(limit=20):
    print(entry["label"], entry["valence"])
```

**Persistencia**:
```bash
# Archivo: state/layer2_tone_memory.jsonl
{"label": "happy", "valence": 0.8, "arousal": 0.6, "timestamp": 1730000000.0}
{"label": "neutral", "valence": 0.5, "arousal": 0.5, "timestamp": 1730000010.0}
{"label": "sad", "valence": 0.2, "arousal": 0.4, "timestamp": 1730000020.0}
```

**Uso en MCP**:
```python
# Ajustar Î±/Î² segÃºn historial de tono
historical_tone = tone_memory.recent(limit=5)
avg_valence = sum(t["valence"] for t in historical_tone) / len(historical_tone)

if avg_valence < 0.3:  # Usuario frustrado
    beta = min(beta + 0.15, 1.0)  # Aumentar empatÃ­a
    alpha = 1.0 - beta
```

---

## ğŸŒŠ Layer 3: Fluidity (Transiciones)

### PropÃ³sito
**Transiciones suaves de tono** y coordinaciÃ³n entre layers.

### Componentes

#### 1. `tone_bridge.py` - Puente de Tono ğŸŒ‰
**FunciÃ³n**: Mapea valence/arousal a estilos de respuesta

**Smoothing**: Exponential moving average (Î±=0.25)

**API**:
```python
from core.layer3_fluidity.tone_bridge import ToneStyleBridge, ToneProfile

bridge = ToneStyleBridge(smoothing=0.25)

# Actualizar con nuevo tono
profile = bridge.update(
    label="happy",
    valence=0.8,
    arousal=0.6
)

print(profile.style)        # â†’ "energetic_positive"
print(profile.filler_hint)  # â†’ "match_energy_positive"
```

**Estilos inferidos**:

| Valence | Arousal | Style | Filler Hint |
|---------|---------|-------|-------------|
| â‰¥0.65 | â‰¥0.6 | energetic_positive | match_energy_positive |
| â‰¥0.65 | <0.6 | warm_positive | calm_positive_fillers |
| â‰¤0.35 | â‰¥0.6 | urgent_support | short_assurance_fillers |
| â‰¤0.35 | <0.6 | soft_support | soothing_fillers |
| mid | â‰¥0.7 | focused_alert | steadying_fillers |
| mid | â‰¤0.3 | low_energy | gentle_engagement |
| mid | mid | neutral_support | neutral_fillers |

**Uso en modulaciÃ³n**:
```python
# Obtener estilo actual
profile = bridge.snapshot()

if profile.style == "urgent_support":
    # Usuario estresado â†’ respuesta breve y tranquilizadora
    prompt_style = "brief and reassuring"
elif profile.style == "energetic_positive":
    # Usuario energÃ©tico â†’ match energÃ­a
    prompt_style = "enthusiastic and upbeat"
else:
    # Neutral
    prompt_style = "balanced and clear"
```

#### 2. `sherpa_coordinator.py` - Coordinador VAD ğŸ›ï¸
**FunciÃ³n**: Coordina Voice Activity Detection entre layers

**Uso**: Sincroniza detecciÃ³n de voz con procesamiento de audio

---

## ğŸ”— IntegraciÃ³n con Graph

### Flujo Completo

```python
# 1. Layer1: Captura audio + detecciÃ³n emocional
audio_bytes = capture_audio()
emotion = detect_emotion(audio_bytes)

# 2. Layer2: Guardar en memoria de tono
tone_memory.append({
    "label": emotion["label"],
    "valence": emotion["valence"],
    "arousal": emotion["arousal"]
})

# 3. Layer3: Inferir estilo
profile = tone_bridge.update(
    label=emotion["label"],
    valence=emotion["valence"],
    arousal=emotion["arousal"]
)

# 4. Graph: Ajustar MCP segÃºn tono
mcp_weights = mcp.compute_weights(scores, context, tone_profile=profile)

# 5. Graph: Generar respuesta con estilo apropiado
response = llm.generate(query, style=profile.style)

# 6. Layer1: TTS con tono apropiado
audio_output = tts.synthesize(response, emotion=profile.last_label)
```

### Modificaciones en `core/graph.py`

#### Paso 1: Detectar emociÃ³n (Layer1)
```python
from core.layer1_io.audio_emotion_lite import detect_emotion

def classify_intent_node(state: State):
    # Si input es audio, detectar emociÃ³n
    if state.get("input_type") == "audio" and state.get("audio_path"):
        emotion = detect_emotion(state["audio_path"])
        state["emotion"] = emotion
    
    # ClasificaciÃ³n TRM como antes
    scores = trm_router.invoke(state["input"])
    return {"hard": scores["hard"], "soft": scores["soft"]}
```

#### Paso 2: Guardar en memoria (Layer2)
```python
from core.layer2_memory.tone_memory import ToneMemoryBuffer

tone_memory = ToneMemoryBuffer()

def log_emotion_node(state: State):
    if state.get("emotion"):
        tone_memory.append({
            "label": state["emotion"]["label"],
            "valence": state["emotion"]["valence"],
            "arousal": state["emotion"]["arousal"]
        })
    return state
```

#### Paso 3: Ajustar MCP (Layer2 â†’ MCP)
```python
def compute_weights_node(state: State):
    # Obtener tono histÃ³rico
    recent_tones = tone_memory.recent(limit=5)
    
    # Calcular Î±/Î² con ajuste de tono
    alpha, beta = mcp.compute_weights(
        scores={"hard": state["hard"], "soft": state["soft"]},
        context=state["input"],
        tone_history=recent_tones
    )
    
    return {"alpha": alpha, "beta": beta}
```

#### Paso 4: Aplicar estilo (Layer3)
```python
from core.layer3_fluidity.tone_bridge import ToneStyleBridge

tone_bridge = ToneStyleBridge()

def modulate_soft_node(state: State):
    # Actualizar bridge con emociÃ³n actual
    if state.get("emotion"):
        profile = tone_bridge.update(
            label=state["emotion"]["label"],
            valence=state["emotion"]["valence"],
            arousal=state["emotion"]["arousal"]
        )
        state["tone_style"] = profile.style
        state["filler_hint"] = profile.filler_hint
    
    # ModulaciÃ³n con estilo apropiado
    style_prompt = get_style_prompt(state.get("tone_style", "neutral_support"))
    
    prompt = f"""Reformula con tono {style_prompt}.

Respuesta tÃ©cnica:
{state['hard_response']}

Usuario ({state.get('emotion', {}).get('label', 'neutral')}):
{state['input']}

Reformula manteniendo datos tÃ©cnicos."""
    
    lfm2 = model_pool.get("tiny")
    state["response"] = lfm2.generate(prompt)
    
    return state
```

---

## ğŸ“Š Estado Compartido en Graph

### Campos aÃ±adidos al State

```python
class State(TypedDict):
    # ... campos existentes ...
    
    # Layer1
    input_type: str              # "text" | "audio" | "image"
    audio_path: Optional[str]    # Path al audio si aplica
    emotion: Optional[Dict]      # Resultado de detect_emotion()
    
    # Layer3
    tone_style: Optional[str]    # "energetic_positive", etc.
    filler_hint: Optional[str]   # "match_energy_positive", etc.
```

---

## ğŸ§ª Testing de Layers

### Tests Layer1
```python
def test_emotion_detection():
    """Verifica detecciÃ³n de emociÃ³n"""
    emotion = detect_emotion("test_audio_happy.wav")
    assert emotion["label"] in ["neutral", "happy", "sad", "angry", "fearful"]
    assert 0.0 <= emotion["valence"] <= 1.0
    assert 0.0 <= emotion["arousal"] <= 1.0
```

### Tests Layer2
```python
def test_tone_memory_persistence():
    """Verifica persistencia de memoria de tono"""
    tone_memory = ToneMemoryBuffer()
    tone_memory.append({"label": "happy", "valence": 0.8})
    
    # Reiniciar buffer (simula restart)
    tone_memory2 = ToneMemoryBuffer()
    recent = tone_memory2.recent(limit=1)
    
    assert len(recent) > 0
    assert recent[-1]["label"] == "happy"
```

### Tests Layer3
```python
def test_tone_bridge_smoothing():
    """Verifica smoothing de transiciones"""
    bridge = ToneStyleBridge(smoothing=0.25)
    
    # Entrada sÃºbita de tristeza
    profile = bridge.update("sad", valence=0.2, arousal=0.4)
    
    # Valence no debe cambiar abruptamente (smoothing)
    assert 0.35 < profile.valence_avg < 0.5  # No salta a 0.2
```

---

## ğŸ¯ KPIs de Layers

| KPI | Objetivo | MÃ©todo |
|-----|----------|--------|
| Latencia emotion detection | <100ms | Profiling con audio test |
| Accuracy emotion | >75% | ValidaciÃ³n con dataset etiquetado |
| Tone memory hit rate | >80% | % de queries con historial |
| Smoothing effectiveness | Î”<0.2 | Cambio mÃ¡ximo entre updates |

---

## ğŸš€ Roadmap de IntegraciÃ³n

### Fase 2.1: Layer1 I/O âœ…
- [x] Componentes existentes
- [ ] IntegraciÃ³n en graph.py
- [ ] Tests de emotion detection

### Fase 2.2: Layer2 Memory
- [x] ToneMemoryBuffer implementado
- [ ] IntegraciÃ³n con MCP
- [ ] Tests de persistencia

### Fase 2.3: Layer3 Fluidity
- [x] ToneBridge implementado
- [ ] IntegraciÃ³n en modulaciÃ³n
- [ ] Tests de smoothing

### Fase 2.4: IntegraciÃ³n Completa
- [ ] State extendido
- [ ] Nodos del grafo modificados
- [ ] Tests end-to-end

---

## ğŸ“š Referencias

- Layer1 I/O: `core/layer1_io/`
- Layer2 Memory: `core/layer2_memory/`
- Layer3 Fluidity: `core/layer3_fluidity/`
- Graph: `core/graph.py`

---

**ConclusiÃ³n**: La arquitectura de 3 layers permite a SARAi procesar audio con **contexto emocional**, mantener **memoria de tono** y producir **transiciones suaves** en las respuestas.
