# An√°lisis de Memoria: SOLAR + LFM2 + Omni Simult√°neos

## üéØ Pregunta

¬øPodemos tener **SOLAR + LFM2 + Omni todos en memoria** ahora que SOLAR puede usar servidor Ollama remoto?

## üÜï Actualizaci√≥n (29 Oct 2025)

**GGUF de Qwen2.5-Omni disponible**: `unsloth/Qwen2.5-Omni-7B-GGUF` (Q4_K_M, 4.68 GB).

**DECISI√ìN ESTRAT√âGICA**: Upgrade de 3B a **7B** por funci√≥n cr√≠tica multimodal en AGI.

- **Opci√≥n 1 (Solo texto)**: GGUF Q4_K_M = 4.9 GB (compatible con llama-cpp-python)
- **Opci√≥n 2 (Multimodal)**: Transformers 4bit = 5.1 GB (audio+imagen+video+TTS)
- **Decisi√≥n SARAi v2.16**: Arquitectura h√≠brida (GGUF para texto, Transformers bajo demanda para multimodal)

Los c√°lculos a continuaci√≥n asumen **7B GGUF Q4_K_M** (4.9 GB) para multimodal estrat√©gico.

**Justificaci√≥n**: Con SOLAR offloaded (11.6 GB liberados), podemos priorizar calidad multimodal (+10-18% performance en todas las modalidades).

---

## üìä An√°lisis de Consumo de RAM

### Escenario 1: SOLAR GGUF Nativo (Anterior)

| Componente | RAM (GB) | Notas |
|------------|----------|-------|
| **SOLAR-10.7B GGUF** | 11.8 GB | GGUF Q4_K_M local, n_ctx=2048 |
| **LFM2-1.2B** | 0.7 GB | Tiny tier, n_ctx=2048 |
| **Qwen-Omni-7B** | 4.9 GB | **7B Q4_K_M (Unsloth)**, solo texto |
| **EmbeddingGemma** | 0.15 GB | Siempre en memoria |
| **TRM-Router + Mini** | 0.05 GB | Clasificadores |
| **Sistema + Python** | 1.5 GB | OS + runtime |
| **TOTAL** | **19.1 GB** | ‚ùå **EXCEDE 16GB** (OOM garantizado) |

**Conclusi√≥n Anterior**: Solo pod√≠amos cargar **2 de 3 LLMs** simult√°neos (Expert + Tiny O Expert + Omni).

---

### Escenario 2: SOLAR Ollama HTTP (NUEVO v2.16)

| Componente | RAM (GB) | Notas |
|------------|----------|-------|
| **SOLAR Cliente HTTP** | 0.2 GB | Solo cliente requests + metadata |
| **LFM2-1.2B** | 0.7 GB | Tiny tier, n_ctx=2048 |
| **Qwen-Omni-7B** | 4.9 GB | **7B Q4_K_M (Unsloth)**, solo texto |
| **EmbeddingGemma** | 0.15 GB | Siempre en memoria |
| **TRM-Router + Mini** | 0.05 GB | Clasificadores |
| **Sistema + Python** | 1.5 GB | OS + runtime |
| **TOTAL** | **7.5 GB** | ‚úÖ **SOLO 47% de 16GB** |

**Conclusi√≥n Nueva v2.16**: ‚úÖ **S√ç, podemos tener los 3 LLMs "disponibles"** con **8.5 GB libres** (53% de RAM libre).

**Nota**: Si Omni usa Transformers (multimodal completo), RAM = 5.1 GB ‚Üí Total 7.7 GB (52% libre).

---

## üöÄ Beneficios de la Estrategia H√≠brida

### 1. RAM Liberada

```
ANTES (GGUF nativo):    16.3 GB ‚Üí OOM
AHORA (Ollama HTTP):     4.7 GB ‚Üí 70% libre
AHORRO:                 11.6 GB (-71%)
```

### 2. Capacidad Simult√°nea

**Antes**:
- ‚úÖ SOLAR + LFM2 = 12.5 GB (OK)
- ‚ùå SOLAR + Omni = 13.9 GB (borderline)
- ‚ùå SOLAR + LFM2 + Omni = 16.3 GB (OOM)

**Ahora**:
- ‚úÖ SOLAR (HTTP) + LFM2 + Omni = 4.7 GB (sobra RAM)
- ‚úÖ Podr√≠amos a√±adir m√°s modelos peque√±os si fuera necesario

### 3. Latencia Adicional Aceptable

| M√©trica | GGUF Local | Ollama HTTP (LAN) | Œî |
|---------|------------|-------------------|---|
| **Primera inferencia** | ~30ms | ~80ms | +50ms |
| **Tokens/s** | 2.11-2.20 | 2.0-2.5 | ~5% m√°s lento |
| **Prompt eval (512 tok)** | ~15s | ~16s | +1s |

**Conclusi√≥n**: +50ms de latencia es **ACEPTABLE** para ganar 11.6 GB de RAM.

---

## üèóÔ∏è Arquitectura Propuesta

### Model Pool Optimizado

```python
# core/model_pool.py (pseudo-c√≥digo)

class ModelPool:
    """
    Pool h√≠brido: HTTP para modelos grandes, GGUF para peque√±os
    """
    
    def __init__(self):
        # Modelos SIEMPRE en memoria (peque√±os)
        self.embedding = load_embedding_gemma()    # 150 MB
        self.trm_router = load_trm_router()        # 50 MB
        
        # Modelos cargados bajo demanda
        self.cache = {
            "solar": None,      # HTTP (200 MB cliente)
            "lfm2": None,       # GGUF (700 MB)
            "omni": None        # GGUF (2.1 GB)
        }
    
    def get(self, model_name: str):
        """Carga modelo si no est√° en cache"""
        if model_name == "solar":
            # Usar cliente HTTP (carga instant√°nea)
            if self.cache["solar"] is None:
                from agents.solar_ollama import SolarOllama
                self.cache["solar"] = SolarOllama()
            return self.cache["solar"]
        
        elif model_name == "lfm2":
            # GGUF local (700 MB)
            if self.cache["lfm2"] is None:
                self.cache["lfm2"] = load_lfm2_gguf()
            return self.cache["lfm2"]
        
        elif model_name == "omni":
            # GGUF local (2.1 GB)
            if self.cache["omni"] is None:
                self.cache["omni"] = load_omni_gguf()
            return self.cache["omni"]
    
    def get_memory_usage(self) -> Dict[str, float]:
        """Retorna uso de RAM por modelo"""
        return {
            "embedding": 0.15,
            "trm_router": 0.05,
            "solar": 0.2 if self.cache["solar"] else 0,
            "lfm2": 0.7 if self.cache["lfm2"] else 0,
            "omni": 2.1 if self.cache["omni"] else 0,
            "system": 1.5,
            "total": sum(...)
        }
```

### Pol√≠tica de Carga

**NUNCA descargar modelos** (nueva estrategia):

```python
# Antes (v2.15): Descarga si RAM > 12GB
if psutil.virtual_memory().available / (1024**3) < 4.0:
    self.unload("omni")  # Liberar RAM

# Ahora (v2.16): TODOS en memoria simult√°neamente
# Total: 4.7 GB < 12 GB l√≠mite ‚Üí nunca necesitamos descargar
```

---

## üé≠ Casos de Uso Mejorados

### Caso 1: Multimodal + Razonamiento

**Antes**: No pod√≠amos tener Omni + SOLAR juntos (13.9 GB)

```python
# Usuario env√≠a imagen + pregunta t√©cnica
user_input = {
    "image": "foto.jpg",
    "text": "Explica qu√© sale en esta gr√°fica de red"
}

# Necesit√°bamos descargar Omni despu√©s de procesar imagen
omni = model_pool.get("omni")
description = omni.process_image(user_input["image"])
model_pool.unload("omni")  # ‚ùå Descarga para liberar RAM

solar = model_pool.get("solar")  # ‚ùå Carga GGUF (11.8 GB)
response = solar.generate(f"Contexto: {description}\n\n{user_input['text']}")
```

**Ahora**: Ambos disponibles simult√°neamente

```python
# ‚úÖ Omni y SOLAR disponibles al mismo tiempo
omni = model_pool.get("omni")      # Ya en memoria (2.1 GB)
solar = model_pool.get("solar")    # HTTP instant√°neo (0.2 GB)

description = omni.process_image(user_input["image"])
response = solar.generate(f"Contexto: {description}\n\n{user_input['text']}")
# Total RAM: 2.1 + 0.2 = 2.3 GB ‚Üí ¬°11.3 GB libres!
```

### Caso 2: Pipeline H√≠brido (Soft ‚Üí Hard)

**Antes**: LFM2 ‚Üí SOLAR requer√≠a descarga intermedia

```python
# Modulaci√≥n de tono
lfm2 = model_pool.get("lfm2")
tone = lfm2.generate("Reformula con empat√≠a: ...")
model_pool.unload("lfm2")  # ‚ùå Descarga

solar = model_pool.get("solar")  # ‚ùå Carga GGUF
response = solar.generate(f"Con este tono: {tone}, responde: ...")
```

**Ahora**: Pipeline sin descarga

```python
# ‚úÖ LFM2 y SOLAR disponibles simult√°neamente
lfm2 = model_pool.get("lfm2")    # 0.7 GB
solar = model_pool.get("solar")  # 0.2 GB HTTP

tone = lfm2.generate("Reformula con empat√≠a: ...")
response = solar.generate(f"Con este tono: {tone}, responde: ...")
# Total: 0.9 GB ‚Üí ¬°Sin descarga!
```

### Caso 3: Voz + RAG + Expert

**Antes**: Imposible (Omni 2.1 GB + SOLAR 11.8 GB + LFM2 0.7 GB = 14.6 GB)

**Ahora**: Perfectamente viable

```python
# ‚úÖ Los 3 modelos disponibles
omni = model_pool.get("omni")    # Voz (2.1 GB)
solar = model_pool.get("solar")  # Expert (0.2 GB HTTP)
lfm2 = model_pool.get("lfm2")    # Modulaci√≥n (0.7 GB)

# Pipeline completo
audio_input = user_records_voice()
text = omni.transcribe(audio_input)           # Omni STT
facts = solar.generate(f"Datos sobre: {text}")  # Expert
response = lfm2.modulate(facts, tone="friendly")  # Modulaci√≥n

# Total: 3.0 GB ‚Üí ¬°13 GB libres para b√∫squeda web!
```

---

## ‚ö†Ô∏è Trade-offs y Consideraciones

### 1. Dependencia de Red

| Aspecto | GGUF Local | Ollama HTTP |
|---------|------------|-------------|
| **Red requerida** | No | S√≠ (LAN) |
| **Falla si...** | Disco lleno | Servidor ca√≠do |
| **Latencia** | Constante | Variable (red) |
| **Ancho de banda** | 0 MB/s | ~20 KB/s (tokens) |

**Mitigaci√≥n**: Fallback autom√°tico a GGUF si Ollama no disponible

```python
def get_solar_with_fallback():
    try:
        return SolarOllama()  # Intento 1: HTTP
    except ConnectionError:
        return SolarNative()  # Intento 2: GGUF local
```

### 2. Latencia de Red

**LAN (subred privada)**:
- Ping: ~1ms
- Overhead HTTP: ~50ms por request
- Streaming: Token-por-token sin lag perceptible
- **Conclusi√≥n**: ‚úÖ Aceptable

**WAN (Internet)**:
- Ping: 20-100ms
- Overhead HTTP: 100-300ms por request
- **Conclusi√≥n**: ‚ö†Ô∏è Solo para development, no producci√≥n

### 3. Escalabilidad del Servidor

**Servidor Ollama (<OLLAMA_HOST>)**:
- Capacidad: ~2-3 requests simult√°neas (depende de RAM)
- Uso actual: Solo desarrollo de SARAi
- **Conclusi√≥n**: ‚úÖ OK para 1-2 desarrolladores

**Si crece el equipo**:
- Opci√≥n A: Servidor m√°s grande
- Opci√≥n B: Volver a GGUF local en producci√≥n
- Opci√≥n C: Ollama en cluster (m√∫ltiples servidores)

---

## üéØ Recomendaci√≥n Final

### S√ç, podemos tener SOLAR + LFM2 + Omni simult√°neos ‚úÖ

**Configuraci√≥n recomendada**:

```yaml
# config/sarai.yaml

models:
  solar:
    backend: "ollama_http"  # Nuevo
    fallback: "gguf_native" # Si HTTP falla
    
  lfm2:
    backend: "gguf_native"  # Peque√±o, mejor local
    
  omni:
    backend: "gguf_native"  # Multimodal, mejor local

memory:
  max_ram_gb: 12
  max_concurrent_llms: 3  # NUEVO (antes era 2)
  strategy: "hybrid_http_gguf"
```

**Uso de RAM esperado**:
- **M√≠nimo**: 2.5 GB (solo sistema + embeddings)
- **T√≠pico**: 4.7 GB (todos los modelos cargados)
- **M√°ximo**: 6.0 GB (con skills + cache adicional)
- **Margen**: 10 GB libres (62% de RAM disponible)

### Ventajas

1. ‚úÖ **Latencia reducida**: Sin descarga/carga intermedia
2. ‚úÖ **Pipelines complejos**: Multimodal + Expert + Soft
3. ‚úÖ **RAG + Voz**: Espacio para b√∫squeda web + audio
4. ‚úÖ **Desarrollo r√°pido**: No esperar cargas de 11.8 GB

### Desventajas (mitigables)

1. ‚ö†Ô∏è Depende de red LAN (fallback a GGUF)
2. ‚ö†Ô∏è +50ms latencia HTTP (aceptable)
3. ‚ö†Ô∏è Servidor √∫nico (escalar si crece equipo)

---

## üöÄ Implementaci√≥n Sugerida

### 1. Actualizar Model Pool

```python
# core/model_pool.py

MAX_CONCURRENT_LLMS = 3  # Cambiar de 2 a 3
```

### 2. Configurar Estrategia H√≠brida

```python
# core/model_pool.py

def get_solar():
    """Estrategia h√≠brida con fallback"""
    prefer_http = os.getenv("SOLAR_PREFER_HTTP", "true").lower() == "true"
    
    if prefer_http:
        try:
            return SolarOllama()
        except ConnectionError:
            logger.warning("Ollama no disponible, usando GGUF local")
            return SolarNative()
    else:
        return SolarNative()
```

### 3. A√±adir a `.env`

```bash
# Preferir HTTP en desarrollo
SOLAR_PREFER_HTTP=true

# Permitir 3 LLMs simult√°neos
MAX_CONCURRENT_LLMS=3
```

### 4. Validar con Benchmark

```python
# scripts/test_concurrent_models.py

def test_all_models_loaded():
    pool = ModelPool()
    
    # Cargar los 3
    solar = pool.get("solar")
    lfm2 = pool.get("lfm2")
    omni = pool.get("omni")
    
    # Verificar RAM
    usage = pool.get_memory_usage()
    assert usage["total"] < 12.0, "Excede l√≠mite de RAM"
    
    # Probar inferencia simult√°nea
    assert solar.generate("test") is not None
    assert lfm2.generate("test") is not None
    assert omni.process_image(dummy_image) is not None
    
    print(f"‚úÖ RAM usada: {usage['total']:.1f} GB < 12 GB")
```

---

## üìä Comparativa Final

| M√©trica | Antes (GGUF) | Ahora (HTTP) | Mejora |
|---------|--------------|--------------|--------|
| **Modelos simult√°neos** | 2 | 3 | +50% |
| **RAM total** | 12.5 GB | 4.7 GB | -62% |
| **RAM libre** | 3.5 GB | 11.3 GB | +223% |
| **Latencia SOLAR** | 30ms | 80ms | +50ms |
| **Carga inicial** | 60s | <1s | -98% |
| **Pipelines complejos** | ‚ùå | ‚úÖ | Habilitado |

**Conclusi√≥n**: El trade-off de **+50ms latencia** por **+11.6 GB RAM libre** es **EXCELENTE**.

---

**Respuesta corta**: 

# ‚úÖ S√ç, con Ollama HTTP podemos tener SOLAR + LFM2 + Omni-3B simult√°neos

- **RAM usada**: 4.7 GB (vs 16.3 GB antes)
- **RAM libre**: 11.3 GB (70% disponible)
- **Latencia extra**: +50ms (aceptable)
- **Beneficio**: Pipelines multimodales complejos sin descargas
