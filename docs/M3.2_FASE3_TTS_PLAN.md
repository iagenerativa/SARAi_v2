# M3.2 Fase 3 - TTS Integration: Implementation Plan

**Status**: üî¥ **PLANNED**  
**Dependencies**: M3.2 Fase 2 (‚úÖ COMPLETE)  
**Estimated Duration**: 1 day  
**Target Date**: Oct 29, 2025  
**LOC Estimate**: ~800 l√≠neas (500 prod + 250 tests + 50 docs)

---

## üéØ Objetivo

Implementar s√≠ntesis de voz (Text-to-Speech) con **modulaci√≥n emocional** usando los par√°metros pros√≥dicos generados en Fase 2, completando el ciclo **Voice ‚Üí LLM ‚Üí Voice** con empat√≠a nativa.

---

## üìã Alcance de la Fase

### ‚úÖ Incluido

1. **Motor TTS**: Integraci√≥n de Qwen2.5-Omni-3B en modo TTS
2. **Prosody Application**: Aplicar par√°metros de `emotion_integration.py`
3. **Nodo LangGraph**: `generate_tts` en el flujo principal
4. **Audio Encoding**: WAV 16kHz mono (compatible con Omni-3B)
5. **Testing**: 15 tests (unit + integration + benchmark)
6. **Fallback Chain**: TTS Omni ‚Üí pyttsx3 ‚Üí texto puro

### ‚ùå Excluido (Fases posteriores)

- ~~ONNX Q4 optimization~~ (M3.2 Fase 5)
- ~~Multi-speaker voices~~ (Futuro)
- ~~Voice cloning~~ (Futuro)
- ~~Streaming TTS~~ (Futuro)

---

## üèóÔ∏è Arquitectura Propuesta

### Componente Principal: TTSEngine

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         TTSEngine                               ‚îÇ
‚îÇ                  (agents/tts_engine.py)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  synthesize()         ‚îÇ
           ‚îÇ  (Text + Prosody)     ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ              ‚îÇ              ‚îÇ
   [Priority 1]   [Priority 2]   [Priority 3]
        ‚îÇ              ‚îÇ              ‚îÇ
        ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Qwen2.5      ‚îÇ ‚îÇ pyttsx3      ‚îÇ ‚îÇ Text Only    ‚îÇ
‚îÇ Omni-3B      ‚îÇ ‚îÇ (OS Native)  ‚îÇ ‚îÇ (Fallback)   ‚îÇ
‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ
‚îÇ TTS Native   ‚îÇ ‚îÇ espeak-ng    ‚îÇ ‚îÇ No Audio     ‚îÇ
‚îÇ Emotional    ‚îÇ ‚îÇ SAPI (Win)   ‚îÇ ‚îÇ Response     ‚îÇ
‚îÇ Multi-lang   ‚îÇ ‚îÇ nsss (Mac)   ‚îÇ ‚îÇ String       ‚îÇ
‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ              ‚îÇ              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Audio Encoding ‚îÇ
              ‚îÇ WAV 16kHz mono ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ LangGraph State       ‚îÇ
           ‚îÇ state["audio_output"] ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ M√≥dulos a Implementar

### 1. `agents/tts_engine.py` (~350 LOC)

**Prop√≥sito**: Motor TTS unificado con fallback chain y prosody application.

```python
# agents/tts_engine.py

from dataclasses import dataclass
from typing import Optional, Dict, Any, Literal
import logging
import io
import wave
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class TTSConfig:
    """Configuraci√≥n del motor TTS"""
    engine: Literal["omni", "pyttsx3", "disabled"] = "omni"
    sample_rate: int = 16000
    channels: int = 1  # Mono
    bit_depth: int = 16
    fallback_enabled: bool = True
    cache_enabled: bool = True

@dataclass
class AudioOutput:
    """Resultado de s√≠ntesis TTS"""
    audio_bytes: bytes          # Audio WAV raw
    sample_rate: int            # Hz (16000)
    duration_ms: int            # Duraci√≥n en ms
    engine_used: str            # "omni" | "pyttsx3" | "text_only"
    emotion_applied: str        # Emoci√≥n modulada
    prosody_params: Dict[str, Any]  # Params aplicados

class TTSEngine:
    """
    Motor TTS con 3 niveles de fallback:
    1. Qwen2.5-Omni-3B (nativo, emocional)
    2. pyttsx3 (OS nativo, b√°sico)
    3. Texto puro (sin audio)
    
    Features:
    - Prosody application from emotion_integration
    - Multi-language support (es, en via Omni)
    - Audio caching (perceptual hash)
    - Lazy loading (Omni model on-demand)
    """
    
    def __init__(self, config: TTSConfig = TTSConfig()):
        self.config = config
        self._omni_model = None  # Lazy load
        self._pyttsx3_engine = None  # Lazy load
        self.cache = {}  # {text_hash: AudioOutput}
    
    def synthesize(self,
                   text: str,
                   emotion: str = "neutral",
                   prosody_params: Optional[Dict] = None,
                   lang: str = "es") -> AudioOutput:
        """
        Sintetiza audio con modulaci√≥n emocional
        
        Args:
            text: Texto a sintetizar
            emotion: Emoci√≥n (HAPPY, SAD, ANGRY, etc.)
            prosody_params: Par√°metros de emotion_integration
                {
                    "pitch_shift": +3.0,      # Semitones
                    "speed_factor": 1.15,     # Velocidad
                    "energy_boost": 1.2,      # Amplitud
                    "trajectory": [...],      # Curva emocional
                    "pauses": [...]           # Pausas dram√°ticas
                }
            lang: Idioma ISO 639-1 (es, en)
        
        Returns:
            AudioOutput con audio WAV + metadata
        """
        # 1. Check cache
        if self.config.cache_enabled:
            cached = self._get_from_cache(text, emotion)
            if cached:
                logger.info(f"‚úÖ TTS cache hit: {text[:30]}...")
                return cached
        
        # 2. Try Priority 1: Omni-3B
        try:
            audio_output = self._synthesize_omni(
                text, emotion, prosody_params, lang
            )
            logger.info(f"‚úÖ TTS Omni success: {text[:30]}...")
            
            # Cache result
            if self.config.cache_enabled:
                self._save_to_cache(text, emotion, audio_output)
            
            return audio_output
        
        except Exception as e:
            logger.warning(f"Omni TTS failed: {e}, fallback to pyttsx3")
        
        # 3. Try Priority 2: pyttsx3
        if self.config.fallback_enabled:
            try:
                audio_output = self._synthesize_pyttsx3(
                    text, prosody_params
                )
                logger.info(f"‚úÖ TTS pyttsx3 success: {text[:30]}...")
                return audio_output
            
            except Exception as e:
                logger.warning(f"pyttsx3 TTS failed: {e}, text-only fallback")
        
        # 4. Ultimate fallback: text only
        return AudioOutput(
            audio_bytes=b"",
            sample_rate=0,
            duration_ms=0,
            engine_used="text_only",
            emotion_applied="none",
            prosody_params={}
        )
    
    def _synthesize_omni(self,
                         text: str,
                         emotion: str,
                         prosody_params: Optional[Dict],
                         lang: str) -> AudioOutput:
        """
        S√≠ntesis con Qwen2.5-Omni-3B
        
        Omni-3B soporta TTS nativo con:
        - Tokens especiales: <|tts|>, <|emotion:{emotion}|>
        - Multi-idioma: es, en (nativos)
        - Prosodia: pitch, speed, energy via model config
        """
        # Lazy load model
        if self._omni_model is None:
            from agents.omni_pipeline import load_omni_model
            self._omni_model = load_omni_model()
            logger.info("‚úÖ Omni-3B loaded for TTS")
        
        # Construir prompt con emoci√≥n
        tts_prompt = self._build_omni_prompt(text, emotion, lang)
        
        # Aplicar prosody params si est√°n disponibles
        if prosody_params:
            # Configurar Omni model generation parameters
            gen_config = {
                "pitch_shift": prosody_params.get("pitch_shift", 0.0),
                "speed": prosody_params.get("speed_factor", 1.0),
                "energy": prosody_params.get("energy_boost", 1.0)
            }
        else:
            gen_config = {}
        
        # Generar audio
        audio_array = self._omni_model.generate_audio(
            tts_prompt,
            lang=lang,
            **gen_config
        )
        
        # Convertir a WAV bytes
        audio_bytes = self._array_to_wav(audio_array)
        
        return AudioOutput(
            audio_bytes=audio_bytes,
            sample_rate=self.config.sample_rate,
            duration_ms=int(len(audio_array) / self.config.sample_rate * 1000),
            engine_used="omni",
            emotion_applied=emotion,
            prosody_params=prosody_params or {}
        )
    
    def _synthesize_pyttsx3(self,
                           text: str,
                           prosody_params: Optional[Dict]) -> AudioOutput:
        """
        S√≠ntesis con pyttsx3 (OS nativo)
        
        Limitaciones:
        - No soporta emoci√≥n nativa
        - Prosody limitada (solo speed, volume)
        - Calidad inferior a Omni
        """
        # Lazy load engine
        if self._pyttsx3_engine is None:
            import pyttsx3
            self._pyttsx3_engine = pyttsx3.init()
            logger.info("‚úÖ pyttsx3 loaded for TTS fallback")
        
        # Aplicar prosody b√°sica
        if prosody_params:
            speed = int(200 * prosody_params.get("speed_factor", 1.0))
            volume = prosody_params.get("energy_boost", 1.0)
            
            self._pyttsx3_engine.setProperty('rate', speed)
            self._pyttsx3_engine.setProperty('volume', volume)
        
        # Generar audio a archivo temporal
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
            self._pyttsx3_engine.save_to_file(text, tmp.name)
            self._pyttsx3_engine.runAndWait()
            
            # Leer audio generado
            with open(tmp.name, 'rb') as f:
                audio_bytes = f.read()
        
        # Limpiar archivo temporal
        import os
        os.unlink(tmp.name)
        
        return AudioOutput(
            audio_bytes=audio_bytes,
            sample_rate=22050,  # pyttsx3 default
            duration_ms=int(len(audio_bytes) / 22050 * 1000),
            engine_used="pyttsx3",
            emotion_applied="neutral",  # pyttsx3 no soporta emociones
            prosody_params=prosody_params or {}
        )
    
    def _build_omni_prompt(self, text: str, emotion: str, lang: str) -> str:
        """
        Construye prompt para Omni-3B TTS
        
        Formato: <|tts|>{text}<|emotion:{emotion}|><|lang:{lang}|>
        """
        return f"<|tts|>{text}<|emotion:{emotion.lower()}|><|lang:{lang}|>"
    
    def _array_to_wav(self, audio_array: np.ndarray) -> bytes:
        """Convierte numpy array a WAV bytes"""
        buffer = io.BytesIO()
        
        with wave.open(buffer, 'wb') as wav_file:
            wav_file.setnchannels(self.config.channels)
            wav_file.setsampwidth(self.config.bit_depth // 8)
            wav_file.setframerate(self.config.sample_rate)
            
            # Convertir float32 ‚Üí int16
            audio_int16 = (audio_array * 32767).astype(np.int16)
            wav_file.writeframes(audio_int16.tobytes())
        
        buffer.seek(0)
        return buffer.read()
    
    def _get_from_cache(self, text: str, emotion: str) -> Optional[AudioOutput]:
        """Busca en cache por hash perceptual"""
        cache_key = self._compute_cache_key(text, emotion)
        return self.cache.get(cache_key)
    
    def _save_to_cache(self, text: str, emotion: str, audio: AudioOutput):
        """Guarda en cache con LRU eviction"""
        cache_key = self._compute_cache_key(text, emotion)
        
        # LRU eviction si cache > 100 entradas
        if len(self.cache) >= 100:
            # Eliminar entrada m√°s antigua
            oldest_key = min(self.cache.keys())
            del self.cache[oldest_key]
        
        self.cache[cache_key] = audio
    
    def _compute_cache_key(self, text: str, emotion: str) -> str:
        """Hash basado en texto + emoci√≥n"""
        import hashlib
        content = f"{text}|{emotion}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]


# Factory function
def create_tts_engine(config: Optional[TTSConfig] = None) -> TTSEngine:
    """
    Factory para crear TTSEngine con configuraci√≥n
    
    Usage:
        engine = create_tts_engine()
        audio = engine.synthesize("Hola", emotion="HAPPY")
    """
    return TTSEngine(config or TTSConfig())
```

---

### 2. `core/graph.py` - Nodo `generate_tts` (~50 LOC)

**Prop√≥sito**: Integrar TTS en el flujo LangGraph.

```python
# core/graph.py - Extensi√≥n del SARAiGraph

class SARAiGraph:
    # ... c√≥digo existente ...
    
    def _generate_tts(self, state: State) -> dict:
        """
        Genera audio de respuesta si input fue audio
        
        Flujo:
        1. Verificar si state["input_type"] == "audio"
        2. Si no ‚Üí retornar sin cambios
        3. Si s√≠ ‚Üí sintetizar state["response"] con TTS
        4. Aplicar prosody seg√∫n detected_emotion
        5. Guardar en state["audio_output"]
        """
        # Solo generar TTS si input fue audio
        if state.get("input_type") != "audio":
            logger.debug("Input was text, skipping TTS")
            return {}
        
        logger.info("üé§ Generating TTS response...")
        
        # Obtener emoci√≥n y prosody
        detected_emotion = state.get("detected_emotion", "neutral")
        
        # Cargar prosody params (generados en Fase 2)
        from agents.emotion_integration import apply_emotion_to_speech
        from agents.emotion_modulator import EmotionState
        
        emotion_state = EmotionState(
            primary_emotion=detected_emotion.upper(),
            intensity=0.7,
            secondary_emotion=None
        )
        
        prosody_params = apply_emotion_to_speech(
            emotion_state,
            state["response"]
        )
        
        # Sintetizar con TTS
        from agents.tts_engine import create_tts_engine
        
        tts_engine = create_tts_engine()
        audio_output = tts_engine.synthesize(
            text=state["response"],
            emotion=detected_emotion,
            prosody_params=prosody_params,
            lang=state.get("detected_lang", "es")
        )
        
        logger.info(f"‚úÖ TTS complete: {audio_output.engine_used}, "
                   f"{audio_output.duration_ms}ms")
        
        return {
            "audio_output": audio_output.audio_bytes,
            "voice_metadata": {
                "tts_engine": audio_output.engine_used,
                "tts_duration_ms": audio_output.duration_ms,
                "emotion_applied": audio_output.emotion_applied,
                "prosody_params": audio_output.prosody_params
            }
        }
    
    def compile(self) -> CompiledGraph:
        """Compila el grafo con TTS integrado"""
        workflow = StateGraph(State)
        
        # ... nodos existentes ...
        
        # NUEVO: Nodo TTS
        workflow.add_node("generate_tts", self._generate_tts)
        
        # Routing: despu√©s de feedback ‚Üí TTS ‚Üí END
        workflow.add_edge("feedback", "generate_tts")
        workflow.add_edge("generate_tts", END)
        
        return workflow.compile()
```

---

### 3. `agents/omni_pipeline.py` - M√©todo TTS (~50 LOC)

**Prop√≥sito**: Exponer m√©todo `generate_audio()` del modelo Omni.

```python
# agents/omni_pipeline.py - Extensi√≥n

def load_omni_model():
    """
    Lazy load de Qwen2.5-Omni-3B para TTS
    Reutiliza el modelo ya cargado si est√° disponible
    """
    global _omni_model_cache
    
    if _omni_model_cache is None:
        from transformers import AutoModelForCausalLM
        
        _omni_model_cache = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-Omni-3B",
            cache_dir="models/cache/omni",
            trust_remote_code=True
        )
        
        # Habilitar modo TTS
        _omni_model_cache.config.use_tts = True
    
    return _omni_model_cache


# Extension del modelo Omni
class OmniModelExtended:
    """
    Wrapper con m√©todos STT + TTS
    """
    
    def __init__(self, model):
        self.model = model
    
    def generate_audio(self,
                       prompt: str,
                       lang: str = "es",
                       pitch_shift: float = 0.0,
                       speed: float = 1.0,
                       energy: float = 1.0) -> np.ndarray:
        """
        Genera audio desde texto con prosody
        
        Args:
            prompt: Texto con tokens TTS (<|tts|>...)
            lang: Idioma ISO 639-1
            pitch_shift: Semitones (-12 a +12)
            speed: Factor de velocidad (0.5 a 2.0)
            energy: Factor de amplitud (0.5 a 1.5)
        
        Returns:
            numpy array de audio (16kHz, float32)
        """
        # Configurar generation parameters
        generation_config = {
            "max_new_tokens": 1000,
            "temperature": 0.7,
            "do_sample": True,
            # Prosody params (espec√≠ficos de Omni)
            "pitch_shift": pitch_shift,
            "speed_factor": speed,
            "energy_factor": energy
        }
        
        # Generar audio
        with torch.no_grad():
            audio_output = self.model.generate_audio(
                prompt,
                **generation_config
            )
        
        # Convertir tensor ‚Üí numpy
        audio_array = audio_output.cpu().numpy()
        
        return audio_array
```

---

## üß™ Testing Strategy

### Test Suite: `tests/test_tts_engine.py` (~250 LOC)

```python
# tests/test_tts_engine.py

import pytest
import numpy as np
from agents.tts_engine import TTSEngine, TTSConfig, AudioOutput
from agents.emotion_integration import apply_emotion_to_speech
from agents.emotion_modulator import EmotionState

class TestTTSEngine:
    """Test suite completa para TTSEngine"""
    
    @pytest.fixture
    def tts_engine(self):
        """Engine b√°sico para tests"""
        config = TTSConfig(
            engine="omni",
            cache_enabled=True
        )
        return TTSEngine(config)
    
    @pytest.fixture
    def tts_engine_fallback(self):
        """Engine con fallback habilitado"""
        config = TTSConfig(
            engine="pyttsx3",
            fallback_enabled=True
        )
        return TTSEngine(config)
    
    # ===== Tests Unitarios =====
    
    def test_tts_engine_initialization(self, tts_engine):
        """Verifica que TTSEngine se inicializa correctamente"""
        assert tts_engine.config.engine == "omni"
        assert tts_engine.config.sample_rate == 16000
        assert tts_engine._omni_model is None  # Lazy load
    
    def test_synthesize_basic_text(self, tts_engine):
        """S√≠ntesis b√°sica sin emoci√≥n"""
        audio = tts_engine.synthesize(
            text="Hola, ¬øc√≥mo est√°s?",
            emotion="neutral",
            lang="es"
        )
        
        assert isinstance(audio, AudioOutput)
        assert audio.engine_used in ["omni", "pyttsx3", "text_only"]
        assert audio.sample_rate > 0 or audio.engine_used == "text_only"
    
    def test_synthesize_with_emotion_happy(self, tts_engine):
        """S√≠ntesis con emoci√≥n HAPPY"""
        emotion_state = EmotionState(
            primary_emotion="HAPPY",
            intensity=0.8,
            secondary_emotion=None
        )
        
        prosody = apply_emotion_to_speech(emotion_state, "¬°Qu√© buena noticia!")
        
        audio = tts_engine.synthesize(
            text="¬°Qu√© buena noticia!",
            emotion="HAPPY",
            prosody_params=prosody,
            lang="es"
        )
        
        assert audio.emotion_applied == "HAPPY"
        assert audio.prosody_params["pitch_shift"] > 0  # Pitch alto
    
    def test_synthesize_with_emotion_sad(self, tts_engine):
        """S√≠ntesis con emoci√≥n SAD"""
        emotion_state = EmotionState(
            primary_emotion="SAD",
            intensity=0.7,
            secondary_emotion=None
        )
        
        prosody = apply_emotion_to_speech(emotion_state, "Lo siento mucho")
        
        audio = tts_engine.synthesize(
            text="Lo siento mucho",
            emotion="SAD",
            prosody_params=prosody,
            lang="es"
        )
        
        assert audio.emotion_applied == "SAD"
        assert audio.prosody_params["pitch_shift"] < 0  # Pitch bajo
        assert audio.prosody_params["speed_factor"] < 1.0  # M√°s lento
    
    def test_cache_functionality(self, tts_engine):
        """Verifica que el cache funciona correctamente"""
        text = "Hola mundo"
        
        # Primera llamada: genera audio
        audio1 = tts_engine.synthesize(text, emotion="neutral")
        
        # Segunda llamada: debe usar cache
        audio2 = tts_engine.synthesize(text, emotion="neutral")
        
        # Verificar que son el mismo objeto (cache hit)
        assert audio1.audio_bytes == audio2.audio_bytes
        assert len(tts_engine.cache) == 1
    
    def test_cache_different_emotions(self, tts_engine):
        """Cache debe diferenciar por emoci√≥n"""
        text = "Hola"
        
        audio_happy = tts_engine.synthesize(text, emotion="HAPPY")
        audio_sad = tts_engine.synthesize(text, emotion="SAD")
        
        # Deben ser diferentes (diferentes emociones)
        assert audio_happy.audio_bytes != audio_sad.audio_bytes
        assert len(tts_engine.cache) == 2
    
    def test_fallback_chain_omni_to_pyttsx3(self, mocker, tts_engine_fallback):
        """Verifica fallback cuando Omni falla"""
        # Mock Omni para que falle
        mocker.patch.object(
            tts_engine_fallback,
            '_synthesize_omni',
            side_effect=Exception("Omni not available")
        )
        
        audio = tts_engine_fallback.synthesize("Test fallback")
        
        # Debe usar pyttsx3
        assert audio.engine_used == "pyttsx3"
    
    def test_fallback_to_text_only(self, mocker, tts_engine):
        """Fallback final a texto cuando todo falla"""
        # Mock ambos engines para que fallen
        mocker.patch.object(
            tts_engine,
            '_synthesize_omni',
            side_effect=Exception("Omni failed")
        )
        mocker.patch.object(
            tts_engine,
            '_synthesize_pyttsx3',
            side_effect=Exception("pyttsx3 failed")
        )
        
        audio = tts_engine.synthesize("Ultimate fallback")
        
        # Debe retornar text_only
        assert audio.engine_used == "text_only"
        assert audio.audio_bytes == b""
    
    def test_audio_encoding_wav(self, tts_engine):
        """Verifica que el audio est√° en formato WAV"""
        audio = tts_engine.synthesize("Test WAV", emotion="neutral")
        
        if audio.engine_used != "text_only":
            # Verificar header WAV
            assert audio.audio_bytes[:4] == b'RIFF'
            assert audio.audio_bytes[8:12] == b'WAVE'
    
    def test_prosody_application(self, tts_engine):
        """Verifica que prosody params se aplican correctamente"""
        prosody = {
            "pitch_shift": 5.0,
            "speed_factor": 1.3,
            "energy_boost": 1.4
        }
        
        audio = tts_engine.synthesize(
            "Test prosody",
            emotion="EXCITED",
            prosody_params=prosody
        )
        
        assert audio.prosody_params == prosody
    
    # ===== Tests de Integraci√≥n =====
    
    @pytest.mark.skipif(
        not is_omni_model_available(),
        reason="Omni model not available"
    )
    def test_integration_omni_tts(self, tts_engine):
        """Test de integraci√≥n con modelo Omni real"""
        audio = tts_engine.synthesize(
            text="Esto es una prueba de integraci√≥n",
            emotion="HAPPY",
            lang="es"
        )
        
        assert audio.engine_used == "omni"
        assert audio.duration_ms > 0
        assert len(audio.audio_bytes) > 0
    
    # ===== Benchmarks =====
    
    def test_benchmark_tts_latency(self, tts_engine, benchmark):
        """Benchmark de latencia TTS"""
        result = benchmark(
            tts_engine.synthesize,
            "Hola, ¬øc√≥mo est√°s?",
            "neutral"
        )
        
        # Target: <500ms en CPU, <200ms en GPU
        # benchmark muestra stats autom√°ticamente
    
    def test_benchmark_cache_speedup(self, tts_engine):
        """Verifica que cache reduce latencia significativamente"""
        import time
        
        text = "Test cache speedup"
        
        # Primera llamada (sin cache)
        start1 = time.perf_counter()
        tts_engine.synthesize(text, "neutral")
        latency_no_cache = time.perf_counter() - start1
        
        # Segunda llamada (con cache)
        start2 = time.perf_counter()
        tts_engine.synthesize(text, "neutral")
        latency_cache = time.perf_counter() - start2
        
        # Cache debe ser >10x m√°s r√°pido
        assert latency_cache < latency_no_cache / 10
        print(f"\nüìä Cache speedup: {latency_no_cache/latency_cache:.1f}x")
```

---

## üìä KPIs de √âxito

| KPI | Target | M√©todo de Medici√≥n |
|-----|--------|-------------------|
| **Latencia TTS (Omni)** | <300ms P50, <500ms P99 | `test_benchmark_tts_latency` |
| **Latencia TTS (pyttsx3)** | <200ms | Benchmark fallback |
| **Cache Hit Rate** | >40% | Monitor en conversaciones reales |
| **Cache Speedup** | >10x | `test_benchmark_cache_speedup` |
| **Audio Quality (MOS)** | >4.0 | Panel de evaluadores (manual) |
| **Prosody Accuracy** | 100% params aplicados | `test_prosody_application` |
| **Fallback Rate** | <5% | Logs de producci√≥n |
| **Test Coverage** | >95% | pytest --cov |

---

## üîß Integration Checklist

### Pre-requisitos

- [x] M3.2 Fase 2 completada (emotion_integration.py disponible)
- [ ] Qwen2.5-Omni-3B instalado y funcional
- [ ] pyttsx3 instalado (`pip install pyttsx3`)
- [ ] espeak-ng instalado (Linux: `apt install espeak-ng`)
- [ ] Grafana dashboard preparado (Fase 4)

### Tareas de Implementaci√≥n

**D√≠a 1 - Morning (4h)**
- [ ] Implementar `TTSEngine` class (2h)
- [ ] Implementar m√©todos `_synthesize_omni` y `_synthesize_pyttsx3` (1.5h)
- [ ] Implementar cache con LRU eviction (30min)

**D√≠a 1 - Afternoon (4h)**
- [ ] Extender `omni_pipeline.py` con m√©todo `generate_audio()` (1h)
- [ ] A√±adir nodo `generate_tts` a `core/graph.py` (1h)
- [ ] Implementar tests unitarios (1-10) (1.5h)
- [ ] Implementar tests de integraci√≥n (11-13) (30min)

**D√≠a 1 - Evening (2h)**
- [ ] Benchmarks de latencia (30min)
- [ ] Testing manual con audio real (30min)
- [ ] Documentaci√≥n inline (30min)
- [ ] Commit + push (30min)

---

## üö® Riesgos y Mitigaciones

### Riesgo 1: Omni-3B no soporta TTS nativo

**Probabilidad**: Media  
**Impacto**: Alto  
**Mitigaci√≥n**:
- Verificar documentaci√≥n de Qwen2.5-Omni antes de implementar
- Si no soporta TTS: usar Coqui-TTS como alternativa
- Fallback a pyttsx3 siempre disponible

### Riesgo 2: Prosody params no se aplican correctamente

**Probabilidad**: Media  
**Impacto**: Medio  
**Mitigaci√≥n**:
- Testing exhaustivo con diferentes emociones
- Validaci√≥n manual con panel de evaluadores
- Logs detallados de params aplicados

### Riesgo 3: Latencia TTS >500ms degrada UX

**Probabilidad**: Alta (en CPU)  
**Impacto**: Medio  
**Mitigaci√≥n**:
- Cache agresivo (>40% hit rate esperado)
- Pre-generaci√≥n de respuestas comunes
- Streaming TTS en futuras versiones

---

## üìà M√©tricas de Progreso

### Commits Esperados

1. **feat(M3.2): Add TTSEngine with fallback chain** (~400 LOC)
2. **feat(M3.2): Integrate TTS in LangGraph** (~50 LOC)
3. **test(M3.2): Add TTS test suite** (~250 LOC)
4. **docs(M3.2): TTS integration guide** (~50 LOC)

**Total**: ~750 LOC, 4 commits

### Testing Progress

- [ ] 15 tests implementados
- [ ] 15/15 passing (100%)
- [ ] Coverage >95%
- [ ] Benchmarks ejecutados
- [ ] Integration test con modelo real

---

## üéì Lessons from Fase 2 (Applied)

### 1. Lazy Loading de Modelos Pesados
**Aprendizaje**: Wav2Vec2 (300MB) causaba lentitud en tests.  
**Aplicaci√≥n**: Omni-3B se carga solo cuando se necesita TTS.

### 2. Mocking Completo en Tests
**Aprendizaje**: Tests reales descargaban modelos (~2 min).  
**Aplicaci√≥n**: Todos los tests usan mocks excepto 1 integration test (skipif).

### 3. Cache con Perceptual Hashing
**Aprendizaje**: Cache hit rate +15% con TTL din√°mico.  
**Aplicaci√≥n**: Cache TTS por hash de texto+emoci√≥n, LRU eviction.

### 4. Fallback Chain Robusto
**Aprendizaje**: Sistema nunca debe fallar.  
**Aplicaci√≥n**: Omni ‚Üí pyttsx3 ‚Üí text_only (degradaci√≥n elegante).

---

## üèÜ Definition of Done

- [ ] `TTSEngine` class completa con 3 niveles de fallback
- [ ] Integraci√≥n en `core/graph.py` funcional
- [ ] 15 tests (13 unit + 2 integration) passing
- [ ] Coverage ‚â•95%
- [ ] Benchmarks documentados (latencia <500ms P99)
- [ ] Cache implementado con >40% hit rate
- [ ] Prosody params aplicados correctamente
- [ ] Audio encoding WAV 16kHz funcionando
- [ ] Commit message descriptivo + CHANGELOG actualizado
- [ ] Documentaci√≥n inline completa

---

## üìù Next Steps (Post-Fase 3)

### M3.2 Fase 4: Grafana Dashboards (1.5 d√≠as)
- Panel de m√©tricas TTS (latencia, cache hit, engine usado)
- Gr√°fico de emociones detectadas/aplicadas
- Audio quality dashboard

### M3.2 Fase 5: ONNX Q4 Optimization (2 d√≠as)
- Convertir Omni-3B a ONNX Q4
- Reducir de 2.1GB ‚Üí 1.5GB
- Latencia -30% esperada

### M3.2 Fase 6: E2E Testing (1 d√≠a)
- Tests end-to-end completos
- Voice ‚Üí LLM ‚Üí Voice pipeline
- Stress testing con 1000 requests

---

**Preparado por**: GitHub Copilot  
**Fecha**: Oct 28, 2025  
**Versi√≥n**: 1.0  
**Status**: Ready for Implementation üöÄ
