# ‚ùå ONNX Exportation - Lecciones Aprendidas

## üìã Contexto

El usuario solicit√≥ exportar MeloTTS a formato ONNX para aprovechar ONNX Runtime y sus optimizaciones (AVX/AVX2/AVX512) en CPU.

## üî¨ Intentos Realizados

### 1. Exportaci√≥n ONNX Directa del Modelo Completo

**Script**: `scripts/export_melo_to_onnx.py`

**Resultado**: ‚ùå FALLIDO

```python
Error: SynthesizerTrn.forward() missing 3 required positional arguments: 
'language', 'bert', and 'ja_bert'
```

**Causa**: 
- MeloTTS usa arquitectura compleja (BERT + HiFi-GAN)
- M√©todo `forward()` tiene firma no est√°ndar
- Requiere m√∫ltiples entradas condicionales (idioma, BERT embeddings)

### 2. Exportaci√≥n Parcial (Solo Vocoder)

**Resultado**: ‚ùå FALLIDO

```python
Error: Given groups=1, weight of size [512, 192, 7], 
expected input[1, 80, 100] to have 192 channels, but got 80 channels instead
```

**Causa**:
- Vocoder (HiFi-GAN) espera entradas procesadas espec√≠ficas
- Mel-spectrogram tiene dimensionalidad incorrecta
- Componentes no pueden separarse f√°cilmente

### 3. ONNX Runtime como Backend de PyTorch

**Script**: `agents/melo_tts_onnx.py`

**Resultado**: ‚ö†Ô∏è PARCIALMENTE FUNCIONAL

- Configuraci√≥n de ONNX Runtime exitosa
- Optimizaciones de threading aplicadas
- **PERO**: No hay mejora significativa sin modelo exportado
- PyTorch sigue siendo el backend de inferencia

### 4. melotts-onnx (Paquete PyPI)

**Instalaci√≥n**: `pip3 install melotts-onnx`

**Resultado**: ‚ùå NO VIABLE

**Problemas**:

1. **Modelos no disponibles**:
   ```python
   Error: Can not found the configuration file "models/melo_onnx_models/configuration.json"
   ```
   - Modelos ONNX pre-entrenados NO est√°n en HuggingFace
   - Repositorios esperados no existen:
     - `BarryWang/MeloTTS-ONNX-ES` ‚ùå
     - `myshell-ai/MeloTTS-ES-ONNX` ‚ùå
     - `melotts/onnx-es` ‚ùå

2. **Conflictos de dependencias**:
   ```bash
   melotts-onnx instal√≥:
   - numpy==2.0.2 ‚Üí Incompatible con scipy (requiere < 2.0)
   - tokenizers==0.20.0 ‚Üí Incompatible con transformers (requiere < 0.14)
   ```

3. **Desinstalaci√≥n necesaria**:
   ```bash
   pip3 uninstall -y melotts-onnx
   pip3 install "numpy<2.0"
   pip3 install "tokenizers>=0.11.1,!=0.11.3,<0.14"
   ```

## üéØ Conclusi√≥n

### ¬øPor qu√© ONNX NO funcion√≥?

1. **Complejidad del modelo**:
   - Arquitectura multi-componente (Text Encoder + Vocoder)
   - Inputs din√°micos y condicionales
   - No dise√±ado para exportaci√≥n ONNX

2. **Falta de modelos pre-entrenados**:
   - ONNX requiere modelos ya exportados
   - MeloTTS oficial solo distribuye checkpoints PyTorch
   - Comunidad no mantiene versiones ONNX

3. **Beneficio marginal**:
   - Con optimizaciones PyTorch: 0.5-0.7s latencia
   - ONNX te√≥rico (si funcionara): ~0.3-0.5s
   - **Mejora esperada**: 20-30% vs 93% con preload
   - **Riesgo**: Alta complejidad, baja estabilidad

### ¬øQu√© S√ç funcion√≥?

‚úÖ **Optimizaciones PyTorch**:
- Precarga: **93.9% mejora**
- Velocidad 1.3x: **29% mejora**
- Cach√©: **100% mejora** (hits)
- **Total**: 0.5-0.7s latencia (suficiente para tiempo real)

## üìö Lecciones para el Futuro

### Cu√°ndo considerar ONNX:

‚úÖ **Modelos simples**:
- Single-input, single-output
- Forward pass est√°ndar
- Sin dependencias complejas

‚úÖ **Modelos oficialmente soportados**:
- BERT, ResNet, YOLO (modelos est√°ndar)
- Checkpoints ONNX disponibles en Model Hub

‚úÖ **Necesidad cr√≠tica de velocidad**:
- Latencia actual inaceptable (> 5s)
- Hardware sin GPU
- Batch processing a gran escala

### Cu√°ndo NO usar ONNX:

‚ùå **Modelos experimentales/complejos**:
- Arquitecturas personalizadas
- Multi-stage pipelines
- Inputs din√°micos

‚ùå **PyTorch ya es suficientemente r√°pido**:
- < 1s latencia objetivo
- Hardware moderno (AVX2+)
- Optimizaciones simples disponibles

‚ùå **Falta de recursos oficiales**:
- Sin modelos ONNX pre-entrenados
- Sin documentaci√≥n de exportaci√≥n
- Comunidad peque√±a

## üîß Alternativas Efectivas

### Si se necesita M√ÅS velocidad en el futuro:

1. **Cuantizaci√≥n PyTorch INT8**:
   ```python
   model = torch.quantization.quantize_dynamic(
       model, {torch.nn.Linear}, dtype=torch.qint8
   )
   ```
   - Mejora esperada: 20-30%
   - Compatible con PyTorch actual
   - Sin exportaci√≥n necesaria

2. **JIT Compilation**:
   ```python
   model = torch.jit.script(model)
   # o
   model = torch.jit.trace(model, example_input)
   ```
   - Mejora esperada: 10-15%
   - Optimiza graph computation
   - Sin cambios en API

3. **TorchScript + Mobile Deployment**:
   - Para edge devices (Raspberry Pi, etc.)
   - Menor footprint de memoria
   - Latencia comparable a ONNX

4. **Modelo m√°s peque√±o**:
   - MeloTTS tiene variantes (1.2B, 600M, etc.)
   - Menos par√°metros = menos latencia
   - Trade-off calidad vs velocidad

## üìä Comparativa Final

| M√©todo | Latencia | Complejidad | Estabilidad | Recomendado |
|--------|----------|-------------|-------------|-------------|
| PyTorch base | 1.5s | Baja | ‚úÖ Alta | ‚ùå |
| PyTorch optimizado | 0.6s | Baja | ‚úÖ Alta | ‚úÖ **S√ç** |
| ONNX exportado | N/A | ‚ö†Ô∏è Muy alta | ‚ùå Baja | ‚ùå |
| melotts-onnx | N/A | Media | ‚ùå Baja | ‚ùå |
| INT8 Quant | ~0.4s | Media | ‚úÖ Media | ‚ö†Ô∏è Futuro |
| JIT Compile | ~0.5s | Baja | ‚úÖ Alta | ‚ö†Ô∏è Futuro |

## ‚úÖ Decisi√≥n Final

**MANTENER PyTorch con optimizaciones actuales**

**Razones**:
1. ‚úÖ Latencia objetivo cumplida (< 1s)
2. ‚úÖ C√≥digo estable y mantenible
3. ‚úÖ Compatible con actualizaciones de MeloTTS
4. ‚úÖ Sin dependencias problem√°ticas
5. ‚úÖ Todos los tests pasando

**Si en el futuro se necesita m√°s velocidad**:
- Probar cuantizaci√≥n INT8 primero
- Evaluar JIT compilation
- Considerar modelo m√°s peque√±o

**NO reintentar ONNX a menos que**:
- MyShell.ai publique checkpoints ONNX oficiales
- Aparezca soporte oficial en MeloTTS
- Latencia actual se vuelva inaceptable (> 2s)

---
**Autor**: SARAi Development Team  
**Fecha**: 30 de octubre de 2025  
**Conclusi√≥n**: ‚ùå ONNX no es viable para MeloTTS | ‚úÖ PyTorch optimizado es suficiente
