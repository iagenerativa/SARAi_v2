# SARAi v2.16.3 - Resumen Ejecutivo de Tests de Voz

**Fecha**: 30 de octubre de 2025  
**Estado**: Investigaci√≥n Completa ‚úÖ  
**Hardware**: CPU 4 cores (sin GPU)

---

## üéØ Hallazgos Clave

### ‚úÖ Lo que Funciona

1. **Componentes Individuales Disponibles**
   - ‚úÖ Audio Encoder INT8: 620MB (carga en ~6s)
   - ‚úÖ Projection ONNX: 2.4KB (carga en ~40ms)
   - ‚úÖ Talker ONNX: 384MB (carga en <10ms)
   - ‚úÖ Token2Wav INT8: 545MB (carga en ~2s)
   - ‚úÖ LFM2-1.2B: 697MB (disponible para razonamiento)

2. **Infraestructura de Testing**
   - ‚úÖ Script de benchmark interactivo (`tests/voice_benchmark.py`)
   - ‚úÖ Tests unitarios por componente
   - ‚úÖ Sistema de verificaci√≥n de prerequisitos
   - ‚úÖ Documentaci√≥n completa (`docs/VOICE_TEST_RESULTS.md`)

3. **Arquitectura Comprendida**
   ```
   Audio Input (16kHz)
       ‚Üì
   Audio Encoder (PyTorch INT8)
       ‚Üì [B, T', 512]
   Projection (ONNX)
       ‚Üì [B, T', 3584]
   [OPCIONAL: LFM2-1.2B para razonamiento]
       ‚Üì [B, T', 3584]
   Talker (ONNX)
       ‚Üì [B, T', 8448]
   Token2Wav (PyTorch INT8)
       ‚Üì
   Audio Output (24kHz)
   ```

### ‚ö†Ô∏è Bloqueadores Identificados

1. **AutoProcessor Dependency (CR√çTICO)**
   - El Audio Encoder requiere `AutoProcessor` de HuggingFace
   - Descarga ~500MB en primera ejecuci√≥n
   - Alternativa: Pre-procesar audio manualmente o usar librosa

2. **Tipo de Dato ONNX**
   - qwen25_audio_gpu_lite.onnx espera `float16`
   - Test genera `float32`
   - **Soluci√≥n**: Convertir tensores a FP16 antes de inference

3. **Pipeline Gaps**
   - Faltan conversiones entre componentes
   - Shapes no alineados autom√°ticamente
   - Requiere c√≥digo de orquestaci√≥n custom

---

## üìä M√©tricas de Referencia

### Proyecciones de Latencia (CPU)

| Etapa | Tiempo Estimado | Base |
|-------|----------------|------|
| Audio Encoder | 40-60ms | Benchmarks similares |
| Projection | 2-5ms | Tama√±o del modelo |
| Talker ONNX | ~110ms | Test previo exitoso |
| Token2Wav (3 steps) | ~50ms | Diffusion optimizado |
| **Total (sin LLM)** | **~200-230ms** | **‚úÖ Viable** |
| | | |
| + LFM2-1.2B | +1000-1500ms | Razonamiento |
| **Total (con LLM)** | **~1.2-1.7s** | **‚úÖ Aceptable** |

### RAM Utilizada

| Componente | RAM |
|------------|-----|
| Audio Encoder INT8 | ~700MB |
| Projection ONNX | ~5MB |
| Talker ONNX | ~450MB |
| Token2Wav INT8 | ~600MB |
| LFM2-1.2B (opcional) | ~900MB |
| **Total sin LLM** | **~1.8GB** |
| **Total con LLM** | **~2.7GB** |

---

## üõ†Ô∏è Plan de Acci√≥n Recomendado

### Fase 1: Fix T√©cnicos (2-4 horas)

#### 1.1 Resolver Float16 Issue
```python
# En test_voice_simple_onnx.py
dummy_input = np.random.randn(1, seq_len, 3584).astype(np.float16)  # ‚Üê float16
outputs = self.onnx_session.run(None, {input_name: dummy_input})
```

#### 1.2 Integrar AutoProcessor
**Opci√≥n A - HuggingFace (Recomendado)**:
```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    cache_dir="models/cache"
)
```

**Opci√≥n B - Manual (Lightweight)**:
```python
import librosa

def preprocess_audio_manual(audio_bytes, sr=16000):
    # Extraer mel-spectrogram
    waveform, _ = librosa.load(io.BytesIO(audio_bytes), sr=sr)
    mel = librosa.feature.melspectrogram(
        y=waveform,
        sr=sr,
        n_mels=128,
        n_fft=1024,
        hop_length=256
    )
    return mel  # [128, T]
```

#### 1.3 Crear Pipeline Orquestador
```python
# agents/voice_pipeline_modular.py

class VoicePipelineModular:
    def __init__(self):
        self.encoder = load_audio_encoder_int8()
        self.projection = load_projection_onnx()
        self.talker = load_talker_onnx()
        self.token2wav = load_token2wav_int8()
        self.processor = load_auto_processor()
    
    def process_audio(self, audio_bytes: bytes) -> bytes:
        # 1. Encoder
        inputs = self.processor(audio_bytes, sr=16000)
        features = self.encoder(inputs)  # [B, T', 512]
        
        # 2. Projection
        hidden = self.projection.run(None, {"x": features})[0]  # [B, T', 3584]
        
        # 3. Talker
        audio_embeds = self.talker.run(None, {"x": hidden.astype(np.float16)})[0]
        
        # 4. Token2Wav
        waveform = self.token2wav(audio_embeds, num_steps=3)
        
        return waveform.tobytes()
```

---

### Fase 2: Validaci√≥n (1-2 d√≠as)

#### 2.1 Test End-to-End Sin LLM
- Grabar audio de 5s
- Procesar pipeline completo
- Reproducir audio generado
- **KPI**: Latencia ‚â§ 250ms

#### 2.2 Test con LLM Integrado
- A√±adir LFM2-1.2B entre Projection y Talker
- Validar coherencia de respuestas
- **KPI**: Latencia ‚â§ 1.5s

#### 2.3 Test de Calidad (MOS)
- Generar 10 respuestas de voz
- Evaluaci√≥n humana (1-5 escala)
- **KPI**: MOS ‚â• 3.5 (aceptable)

---

### Fase 3: Optimizaci√≥n (1 semana)

#### 3.1 Diffusion Steps Tuning
```python
# Benchmark diferentes configuraciones
for num_steps in [1, 3, 5, 10]:
    latency = benchmark_token2wav(num_steps)
    quality = evaluate_mos(num_steps)
    print(f"Steps: {num_steps}, Latency: {latency}ms, MOS: {quality}")
```

#### 3.2 ONNX Conversion Completa
- Convertir Audio Encoder PyTorch ‚Üí ONNX
- Convertir Token2Wav PyTorch ‚Üí ONNX
- **Beneficio**: Latencia -30-40%

#### 3.3 Batching y Cach√©
- Procesar m√∫ltiples requests en paralelo
- Cache de respuestas frecuentes
- **Beneficio**: Throughput +200%

---

## üìã Checklist de Implementaci√≥n

### Corto Plazo (Hoy)
- [x] Documentar hallazgos (`docs/VOICE_TEST_RESULTS.md`)
- [x] Crear script de benchmark (`tests/voice_benchmark.py`)
- [ ] Fix float16 conversion en test ONNX
- [ ] Test Talker ONNX con datos reales (float16)

### Medio Plazo (Esta Semana)
- [ ] Descargar e integrar AutoProcessor
- [ ] Crear `VoicePipelineModular` class
- [ ] Test end-to-end sin LLM
- [ ] Medir latencias reales E2E
- [ ] Validar calidad de voz (espa√±ol)

### Largo Plazo (Este Mes)
- [ ] Integrar LFM2-1.2B en pipeline
- [ ] Optimizar diffusion steps (tuning)
- [ ] Convertir componentes PyTorch ‚Üí ONNX
- [ ] Tests de estr√©s (10+ turnos)
- [ ] Integraci√≥n con LangGraph

---

## üéì Lecciones Aprendidas

### 1. Modularidad es Compleja pero Poderosa
**Observaci√≥n**: Tener componentes separados (Encoder, Projection, Talker, Token2Wav) permite optimizar cada uno independientemente.

**Tradeoff**: Requiere m√°s c√≥digo de orquestaci√≥n y manejo de shapes entre componentes.

**Decisi√≥n**: Mantener modularidad, crear capa de abstracci√≥n (`VoicePipelineModular`).

---

### 2. ONNX Float16 vs Float32
**Observaci√≥n**: Modelos ONNX compilados para float16 (GPU) no aceptan float32.

**Soluci√≥n**: Convertir tensores a dtype correcto antes de inference:
```python
input_tensor = input_tensor.astype(np.float16)
```

**Alternativa**: Re-compilar ONNX con float32 para CPU puro.

---

### 3. AutoProcessor es Pesado pero Necesario
**Observaci√≥n**: HuggingFace AutoProcessor descarga ~500MB pero simplifica pre-procesamiento.

**Alternativa**: Implementar preprocessing manual con librosa (m√°s ligero).

**Recomendaci√≥n**: Usar AutoProcessor en primera versi√≥n, optimizar despu√©s si es necesario.

---

### 4. INT8 Quantization es Win-Win
**Observaci√≥n**: Modelos INT8 cargan 2x m√°s r√°pido y usan 50% menos RAM con p√©rdida m√≠nima de calidad.

**Evidencia**:
- Audio Encoder: 1.2GB (FP16) ‚Üí 620MB (INT8)
- Token2Wav: 858MB (FP16) ‚Üí 545MB (INT8)

**Decisi√≥n**: Usar INT8 por defecto en producci√≥n CPU.

---

## üîÆ Pr√≥ximos Pasos Inmediatos

### Opci√≥n 1: Quick Win (2 horas)
**Objetivo**: Test Talker ONNX con datos reales (float16)

1. Fix float16 conversion
2. Generar hidden_states sint√©ticos realistas
3. Validar output shape y calidad
4. **Output**: Baseline de latencia Talker real

### Opci√≥n 2: Full Pipeline (1 d√≠a)
**Objetivo**: Pipeline E2E sin LLM funcionando

1. Descargar AutoProcessor
2. Crear `VoicePipelineModular`
3. Integrar todos los componentes
4. Test end-to-end con audio real
5. **Output**: Latencia E2E medida, audio generado

### Opci√≥n 3: Production Ready (1 semana)
**Objetivo**: Pipeline con LLM listo para producci√≥n

1. Todo de Opci√≥n 2
2. Integrar LFM2-1.2B
3. Optimizar diffusion steps
4. Tests de calidad (MOS)
5. Integraci√≥n con LangGraph
6. **Output**: SARAi con voz natural en espa√±ol

---

## üìû Contacto y Recursos

- **Documentaci√≥n Completa**: `docs/VOICE_TEST_RESULTS.md`
- **Script de Benchmark**: `tests/voice_benchmark.py`
- **Tests Unitarios**: `tests/test_voice_*.py`
- **Pipeline Reference**: `models/onnx/pipeline_cpu_optimizado.py`

---

**Conclusi√≥n**: Tenemos todos los componentes y arquitectura comprendida. Los bloqueadores son t√©cnicos (float16, AutoProcessor) y solucionables en 1-2 d√≠as. El pipeline de voz de SARAi es **viable y prometedor** con latencias proyectadas de ~200ms sin LLM y ~1.5s con razonamiento completo. üöÄ

**Recomendaci√≥n**: Empezar con **Opci√≥n 1** (Quick Win) para validar Talker ONNX, luego escalar a **Opci√≥n 2** (Full Pipeline) esta misma semana.
