# M3.2 Voice Integration - Plan de Implementaci√≥n

**Milestone**: M3.2 Voice Integration  
**Dependencias**: M3.1 Omni-Sentinel (‚úÖ COMPLETADO)  
**Fecha Estimada**: 7-10 d√≠as  
**Objetivo**: Integrar pipeline de voz con LangGraph y LLMs existentes (SOLAR, LFM2)

---

## üéØ Objetivos del Milestone

1. **Integrar Omni Pipeline con LangGraph**: A√±adir nodo de voz al flujo principal
2. **Routing Audio/Text**: Sistema debe detectar tipo de input autom√°ticamente
3. **LLM Response Enhancement**: Respuestas SOLAR/LFM2 moduladas por tono de voz detectado
4. **Dashboard de Voz**: Grafana panel para m√©tricas de voz en tiempo real
5. **Optimizaci√≥n ONNX**: Reducir Qwen2.5-Omni-3B de FP16 a Q4 (2.1GB ‚Üí 1.5GB)

---

## üìä KPIs de √âxito

| KPI | Target | Medici√≥n |
|-----|--------|----------|
| **Latencia voz-a-voz** | <300ms P50, <500ms P99 | Benchmark con 1000 requests |
| **Precisi√≥n emocional** | >85% | Comparar emoci√≥n detectada vs ground truth |
| **MOS Score** | >4.3 | Panel de evaluadores humanos |
| **RAM P99** | ‚â§13GB | Monitor + stress test |
| **Test Coverage** | >90% | pytest --cov |
| **Integration Rate** | 100% | Todas las queries de voz procesadas sin fallback |

---

## üèóÔ∏è Arquitectura Propuesta

### Estado Extendido (State)

```python
# core/graph.py - Extensi√≥n del TypedDict State

class State(TypedDict):
    """Estado compartido v2.11 con soporte de voz"""
    # Campos existentes (v2.10)
    input: str                      # Texto del usuario (o transcripci√≥n si audio)
    hard: float
    soft: float
    web_query: float
    alpha: float
    beta: float
    agent_used: Literal["expert", "tiny", "multimodal", "rag", "voice"]  # Nuevo: "voice"
    response: str
    feedback: float
    rag_metadata: dict
    
    # NUEVOS CAMPOS v2.11 (Voice)
    input_type: Literal["text", "audio"]  # Tipo de input
    audio_input: Optional[bytes]          # Audio crudo si input_type="audio"
    audio_output: Optional[bytes]         # Audio de respuesta (TTS)
    detected_emotion: Optional[str]       # Emoci√≥n detectada por Omni
    detected_lang: Optional[str]          # Idioma detectado (ISO 639-1)
    voice_metadata: dict                  # Metadata de procesamiento de voz
```

### Nodos Nuevos en LangGraph

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SARAi v2.11 Voice Graph                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Input (raw) ‚îÄ‚îÄ‚Üí detect_input_type ‚îÄ‚îÄ‚î¨‚Üí [text] ‚Üí classify      ‚îÇ
‚îÇ                                       ‚îÇ                          ‚îÇ
‚îÇ                                       ‚îî‚Üí [audio] ‚Üí process_voice ‚îÇ
‚îÇ                                                         ‚Üì        ‚îÇ
‚îÇ  process_voice:                                                  ‚îÇ
‚îÇ    1. Audio Router (LID)                                        ‚îÇ
‚îÇ    2. Omni-3B (STT + Emotion + Context)                         ‚îÇ
‚îÇ    3. Actualizar State:                                         ‚îÇ
‚îÇ       - input = transcripci√≥n                                   ‚îÇ
‚îÇ       - detected_emotion = "emp√°tico"|"neutral"|"urgente"       ‚îÇ
‚îÇ       - detected_lang = "es"|"en"|"fr"...                       ‚îÇ
‚îÇ                                                         ‚Üì        ‚îÇ
‚îÇ                                                    classify      ‚îÇ
‚îÇ                                                         ‚Üì        ‚îÇ
‚îÇ                                                       mcp        ‚îÇ
‚îÇ                                                         ‚Üì        ‚îÇ
‚îÇ                                              route_to_agent     ‚îÇ
‚îÇ                                                         ‚Üì        ‚îÇ
‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ                      ‚Üì              ‚Üì                     ‚Üì      ‚îÇ
‚îÇ                  expert          tiny                   rag     ‚îÇ
‚îÇ                   (SOLAR)       (LFM2)                (Web)     ‚îÇ
‚îÇ                      ‚îÇ              ‚îÇ                     ‚îÇ      ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                     ‚Üì                            ‚îÇ
‚îÇ                         enhance_with_emotion                    ‚îÇ
‚îÇ                                     ‚Üì                            ‚îÇ
‚îÇ                  [Si audio_input] ‚Üí generate_tts                ‚îÇ
‚îÇ                                     ‚Üì                            ‚îÇ
‚îÇ                                 feedback                         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù Fases de Implementaci√≥n

### Fase 1: Extensi√≥n de State y Routing (2 d√≠as)

**Archivos a modificar**:
- `core/graph.py` (State, nodos nuevos)
- `agents/omni_pipeline.py` (adaptaci√≥n para LangGraph)

**Tasks**:

1. **Extender TypedDict State** (1h)
   ```python
   # core/graph.py
   class State(TypedDict):
       # ... campos existentes ...
       input_type: Literal["text", "audio"]
       audio_input: Optional[bytes]
       audio_output: Optional[bytes]
       detected_emotion: Optional[str]
       detected_lang: Optional[str]
       voice_metadata: dict
   ```

2. **Nodo detect_input_type** (2h)
   ```python
   def _detect_input_type(self, state: State) -> dict:
       """
       Detecta si el input es texto o audio
       - Si state["audio_input"] existe ‚Üí "audio"
       - Si state["input"] existe ‚Üí "text"
       """
       if state.get("audio_input"):
           return {"input_type": "audio"}
       return {"input_type": "text"}
   ```

3. **Nodo process_voice** (4h)
   ```python
   def _process_voice(self, state: State) -> dict:
       """
       Pipeline de voz completo:
       1. Audio Router (LID)
       2. Omni-3B (STT + Emotion)
       3. Actualizar state con transcripci√≥n y metadata
       """
       from agents.audio_router import route_audio
       from agents.omni_pipeline import process_audio_input
       
       audio_bytes = state["audio_input"]
       
       # Routing de audio
       engine, audio, lang = route_audio(audio_bytes)
       
       # Procesamiento seg√∫n motor
       if engine == "omni":
           result = process_audio_input(audio_bytes)
           transcription = result["text"]
           emotion = result.get("emotion", "neutral")
       elif engine == "nllb":
           # Pipeline traducci√≥n (implementado en M3.1)
           from agents.nllb_translator import NLLBPipeline
           pipeline = NLLBPipeline()
           result = pipeline.process(audio_bytes, src_lang=lang)
           transcription = result["text_es"]
           emotion = "neutral"  # NLLB no detecta emoci√≥n
       else:  # lfm2 fallback
           # Solo STT b√°sico
           transcription = whisper_transcribe(audio_bytes)
           emotion = "neutral"
       
       return {
           "input": transcription,  # Input de texto para clasificar
           "detected_emotion": emotion,
           "detected_lang": lang,
           "voice_metadata": {
               "engine_used": engine,
               "audio_length_ms": len(audio_bytes) / 16,  # Estimado
               "processing_time_ms": result.get("latency_ms", 0)
           }
       }
   ```

4. **Actualizar routing condicional** (1h)
   ```python
   # En _build_graph()
   workflow.add_node("detect_input_type", self._detect_input_type)
   workflow.add_node("process_voice", self._process_voice)
   
   workflow.set_entry_point("detect_input_type")
   
   workflow.add_conditional_edges(
       "detect_input_type",
       lambda state: state["input_type"],
       {
           "text": "classify",
           "audio": "process_voice"
       }
   )
   
   workflow.add_edge("process_voice", "classify")  # Luego sigue flujo normal
   ```

**Tests**:
- `tests/test_graph_voice_routing.py` (8 test cases)
  - `test_detect_text_input()`
  - `test_detect_audio_input()`
  - `test_process_voice_spanish()`
  - `test_process_voice_english()`
  - `test_process_voice_french_nllb()`
  - `test_voice_to_expert_flow()`
  - `test_voice_to_tiny_flow()`
  - `test_voice_safe_mode_blocks()`

**Validaci√≥n**:
```bash
pytest tests/test_graph_voice_routing.py -v
# Debe pasar 8/8 tests
```

---

### Fase 2: Modulaci√≥n de Respuesta por Emoci√≥n (2 d√≠as)

**Objetivo**: Usar la emoci√≥n detectada para modular el tono de la respuesta del LLM.

**Archivos a modificar**:
- `core/graph.py` (nodo enhance_with_emotion)
- `agents/tiny_agent.py` (soporte para emotion_context)

**Tasks**:

1. **Nodo enhance_with_emotion** (3h)
   ```python
   def _enhance_with_emotion(self, state: State) -> dict:
       """
       Modula la respuesta del LLM seg√∫n la emoci√≥n detectada
       
       Estrategia:
       - detected_emotion="emp√°tico" ‚Üí Respuesta c√°lida, pausas, confirmaci√≥n
       - detected_emotion="urgente" ‚Üí Respuesta directa, sin rodeos
       - detected_emotion="neutral" ‚Üí Respuesta informativa est√°ndar
       """
       response = state["response"]
       emotion = state.get("detected_emotion", "neutral")
       
       # Si no hay emoci√≥n detectada (input texto), skip
       if not emotion or state["input_type"] == "text":
           return {"response": response}
       
       # Prompt de modulaci√≥n seg√∫n emoci√≥n
       modulation_prompts = {
           "emp√°tico": (
               "Reformula la siguiente respuesta con un tono c√°lido y emp√°tico, "
               "usando frases como 'Entiendo c√≥mo te sientes', 'Estoy aqu√≠ para ayudarte'. "
               "Mant√©n todos los datos t√©cnicos pero a√±ade calidez humana.\n\n"
               f"Respuesta original:\n{response}\n\n"
               f"Respuesta modulada:"
           ),
           "urgente": (
               "Reformula la siguiente respuesta de forma directa y concisa, "
               "sin introducciones innecesarias. Ve directo al grano.\n\n"
               f"Respuesta original:\n{response}\n\n"
               f"Respuesta modulada:"
           ),
           "neutral": response  # No modular
       }
       
       prompt = modulation_prompts.get(emotion, response)
       
       # Si no es neutral, usar LFM2 para modular (r√°pido, 1-2s)
       if emotion != "neutral":
           modulated = self.tiny_agent.generate(
               prompt,
               soft_score=0.8,  # Modulaci√≥n = soft skill
               max_new_tokens=512
           )
           return {"response": modulated}
       
       return {"response": response}
   ```

2. **Integrar en grafo** (1h)
   ```python
   # En _build_graph()
   workflow.add_node("enhance_with_emotion", self._enhance_with_emotion)
   
   # Despu√©s de generar respuesta
   workflow.add_edge("generate_expert", "enhance_with_emotion")
   workflow.add_edge("generate_tiny", "enhance_with_emotion")
   workflow.add_edge("execute_rag", "enhance_with_emotion")
   
   workflow.add_edge("enhance_with_emotion", "generate_tts")  # Fase 3
   ```

3. **Cach√© de modulaciones** (2h)
   - Implementar cache LRU de modulaciones (evitar re-generar)
   - Key: `hash(response + emotion)`
   - TTL: 1h
   - Ahorro estimado: 60% de llamadas a LFM2

**Tests**:
- `tests/test_emotion_modulation.py` (10 test cases)
  - `test_modulate_empathetic_response()`
  - `test_modulate_urgent_response()`
  - `test_neutral_no_modulation()`
  - `test_text_input_skips_modulation()`
  - `test_modulation_cache_hit()`
  - `test_modulation_preserves_facts()`
  - `test_lfm2_modulation_latency()`
  - `test_safe_mode_blocks_modulation()`
  - `test_unknown_emotion_defaults_neutral()`
  - `test_modulation_with_rag_response()`

**Validaci√≥n**:
```bash
pytest tests/test_emotion_modulation.py -v --durations=10
# Latencia modulaci√≥n < 2s P50
```

---

### Fase 3: Text-to-Speech (TTS) Integration (1 d√≠a)

**Objetivo**: Si el input fue audio, generar respuesta de audio usando TTS.

**Archivos a modificar**:
- `core/graph.py` (nodo generate_tts)
- `agents/omni_pipeline.py` (TTS function)

**Tasks**:

1. **Nodo generate_tts** (2h)
   ```python
   def _generate_tts(self, state: State) -> dict:
       """
       Genera audio de respuesta si el input fue audio
       
       Skip si:
       - input_type = "text" (no se espera voz)
       - Safe Mode activo
       """
       # Solo generar TTS si input fue audio
       if state.get("input_type") != "audio":
           return {}
       
       # Safe Mode check
       if is_safe_mode():
           return {}
       
       from agents.omni_pipeline import generate_tts
       
       response_text = state["response"]
       lang = state.get("detected_lang", "es")
       emotion = state.get("detected_emotion", "neutral")
       
       # Generar audio con prosodia emocional
       audio_bytes = generate_tts(
           text=response_text,
           lang=lang,
           emotion=emotion  # "emp√°tico" ‚Üí pitch m√°s alto, pausas
       )
       
       return {"audio_output": audio_bytes}
   ```

2. **Implementar generate_tts en omni_pipeline.py** (3h)
   ```python
   # agents/omni_pipeline.py
   
   def generate_tts(
       text: str,
       lang: str = "es",
       emotion: str = "neutral"
   ) -> bytes:
       """
       TTS con prosodia emocional
       
       Usa Qwen2.5-Omni-3B en modo TTS-only
       (m√°s r√°pido que pipeline completo)
       """
       # Cargar modelo Omni si no est√° en memoria
       model = get_omni_model()
       
       # Prompt especial para TTS con emoci√≥n
       tts_prompt = f"<|tts|>{text}<|emotion:{emotion}|>"
       
       # Inferencia ONNX
       audio_array = model.generate_audio(tts_prompt, lang=lang)
       
       # Convertir a bytes (wav format)
       import io
       buffer = io.BytesIO()
       sf.write(buffer, audio_array, samplerate=16000, format='WAV')
       return buffer.getvalue()
   ```

3. **Integrar en grafo** (30 min)
   ```python
   # En _build_graph()
   workflow.add_node("generate_tts", self._generate_tts)
   
   workflow.add_edge("enhance_with_emotion", "generate_tts")
   workflow.add_edge("generate_tts", "feedback")
   ```

**Tests**:
- `tests/test_tts_generation.py` (6 test cases)
  - `test_tts_spanish_neutral()`
  - `test_tts_english_empathetic()`
  - `test_tts_french_urgent()`
  - `test_tts_skipped_for_text_input()`
  - `test_tts_latency_under_500ms()`
  - `test_safe_mode_blocks_tts()`

**Validaci√≥n**:
```bash
pytest tests/test_tts_generation.py -v
# Latencia TTS < 500ms P99
```

---

### Fase 4: Dashboard Grafana de Voz (1.5 d√≠as)

**Objetivo**: Panel visual para monitorear m√©tricas de voz en tiempo real.

**Archivos a crear**:
- `extras/grafana_voice_dashboard.json`
- `scripts/export_voice_metrics.py`

**Tasks**:

1. **Definir m√©tricas a exportar** (2h)
   ```python
   # scripts/export_voice_metrics.py
   
   VOICE_METRICS = {
       # Latencias
       "voice_latency_p50_ms": gauge,       # Latencia P50 voz-a-voz
       "voice_latency_p99_ms": gauge,       # Latencia P99
       "stt_latency_ms": histogram,         # STT solo
       "tts_latency_ms": histogram,         # TTS solo
       
       # Calidad
       "mos_score": gauge,                  # Mean Opinion Score
       "emotion_accuracy": gauge,           # % emociones correctamente detectadas
       "wer_score": gauge,                  # Word Error Rate (STT)
       
       # Throughput
       "voice_requests_total": counter,     # Total requests de voz
       "voice_errors_total": counter,       # Errores
       "voice_fallbacks_total": counter,    # Fallbacks a texto
       
       # Idiomas
       "voice_lang_distribution": histogram, # Distribuci√≥n de idiomas
       
       # Emociones
       "emotion_distribution": histogram,    # Distribuci√≥n de emociones detectadas
       
       # Engines
       "engine_usage": counter,              # Omni vs NLLB vs LFM2
   }
   ```

2. **Endpoint /metrics para Prometheus** (2h)
   ```python
   # sarai/health_dashboard.py - Extensi√≥n
   
   from prometheus_client import Counter, Gauge, Histogram, generate_latest
   
   # M√©tricas de voz
   voice_latency = Histogram(
       'sarai_voice_latency_seconds',
       'Latencia voz-a-voz',
       buckets=[0.1, 0.2, 0.3, 0.5, 1.0, 2.0]
   )
   
   mos_score = Gauge('sarai_mos_score', 'Mean Opinion Score')
   emotion_accuracy = Gauge('sarai_emotion_accuracy', 'Precisi√≥n detecci√≥n emoci√≥n')
   
   @app.get("/metrics")
   def metrics():
       """Endpoint Prometheus para m√©tricas de voz"""
       return generate_latest()
   ```

3. **Crear dashboard Grafana** (3h)
   - Panel 1: Latencias (P50, P99, STT, TTS) - Time series
   - Panel 2: MOS Score - Gauge
   - Panel 3: Distribuci√≥n de emociones - Pie chart
   - Panel 4: Distribuci√≥n de idiomas - Bar chart
   - Panel 5: Engine usage (Omni/NLLB/LFM2) - Stacked area
   - Panel 6: Error rate - Time series

4. **Script de publicaci√≥n** (1h)
   ```bash
   # Makefile target
   publish-voice-dashboard:
       @python scripts/publish_grafana.py \
           --dashboard extras/grafana_voice_dashboard.json \
           --name "SARAi Voice Metrics"
   ```

**Tests**:
- Validaci√≥n manual del dashboard
- Verificar que m√©tricas se actualizan en tiempo real
- Probar alertas (MOS < 4.0, latencia > 1s)

---

### Fase 5: Optimizaci√≥n ONNX Q4 (2 d√≠as)

**Objetivo**: Reducir RAM de Qwen2.5-Omni-3B de 2.1GB (FP16) a 1.5GB (Q4).

**Archivos a crear**:
- `scripts/quantize_omni_to_q4.py`

**Tasks**:

1. **Script de cuantizaci√≥n** (4h)
   ```python
   # scripts/quantize_omni_to_q4.py
   
   import onnx
   from onnxruntime.quantization import quantize_dynamic, QuantType
   
   def quantize_omni_model():
       """
       Cuantiza Qwen2.5-Omni-3B de FP16 a INT4
       
       Ahorro RAM: 2.1GB ‚Üí 1.5GB (28%)
       Degradaci√≥n WER: <0.5% (aceptable)
       """
       model_fp16 = "models/qwen2.5-omni-3B-es.onnx"
       model_q4 = "models/qwen2.5-omni-3B-es-q4.onnx"
       
       quantize_dynamic(
           model_input=model_fp16,
           model_output=model_q4,
           weight_type=QuantType.QUInt4,  # 4-bit quantization
           optimize_model=True,
           extra_options={
               'ActivationSymmetric': True,
               'WeightSymmetric': True
           }
       )
       
       print(f"‚úÖ Modelo cuantizado: {model_q4}")
   
   if __name__ == "__main__":
       quantize_omni_model()
   ```

2. **Benchmark de degradaci√≥n** (3h)
   - Comparar WER (Word Error Rate) FP16 vs Q4
   - Comparar MOS Score FP16 vs Q4
   - Comparar latencia FP16 vs Q4
   - Target: Degradaci√≥n WER <0.5%, MOS <0.1, latencia <10%

3. **Actualizar config** (1h)
   ```yaml
   # config/sarai.yaml
   
   voice:
     omni_model_path: "models/qwen2.5-omni-3B-es-q4.onnx"  # Q4 por defecto
     fallback_fp16: true  # Si Q4 falla, usar FP16
   ```

**Tests**:
- `tests/test_omni_q4_quality.py`
  - `test_q4_wer_degradation_under_05_percent()`
  - `test_q4_mos_degradation_under_01()`
  - `test_q4_latency_improvement()`
  - `test_q4_ram_usage_under_1_5gb()`

**Validaci√≥n**:
```bash
python scripts/quantize_omni_to_q4.py
pytest tests/test_omni_q4_quality.py -v
```

---

### Fase 6: Testing End-to-End (1 d√≠a)

**Objetivo**: Validar flujo completo voz ‚Üí LLM ‚Üí voz.

**Tasks**:

1. **Tests de integraci√≥n completa** (4h)
   ```python
   # tests/test_voice_e2e.py
   
   def test_voice_to_expert_to_voice():
       """
       Flujo: Audio espa√±ol ‚Üí Omni ‚Üí SOLAR ‚Üí Modulaci√≥n ‚Üí TTS
       
       Input: "¬øC√≥mo configurar SSH en Linux?" (audio)
       Expected:
       - Transcripci√≥n correcta
       - SOLAR responde con pasos t√©cnicos
       - Modulaci√≥n emp√°tica aplicada
       - TTS genera audio de respuesta
       - Latencia total < 2s
       """
       # Cargar audio de test
       audio_input = load_test_audio("ssh_question.wav")
       
       # Crear state inicial
       state = {
           "audio_input": audio_input,
           "input_type": "audio"
       }
       
       # Ejecutar grafo completo
       orchestrator = SARAiOrchestrator()
       result = orchestrator.app.invoke(state)
       
       # Validaciones
       assert result["input_type"] == "audio"
       assert "SSH" in result["input"]  # Transcripci√≥n correcta
       assert result["agent_used"] == "expert"  # Routing a SOLAR
       assert result["audio_output"] is not None  # TTS generado
       assert result["voice_metadata"]["latency_total_ms"] < 2000
   ```

2. **Benchmark de stress** (2h)
   ```python
   def test_voice_concurrent_100_requests():
       """
       100 requests de voz concurrentes
       
       Expected:
       - 0 crashes
       - RAM P99 < 13GB
       - Latencia P99 < 3s
       - 95% de requests exitosas
       """
   ```

3. **Golden queries de voz** (2h)
   - 20 queries de voz representativas
   - Espa√±ol, Ingl√©s, Franc√©s
   - Emociones: neutral, emp√°tico, urgente
   - Validar que todas pasan sin regresi√≥n

**Validaci√≥n**:
```bash
pytest tests/test_voice_e2e.py -v --durations=20
make validate-voice-pipeline
```

---

## üì¶ Entregables

### C√≥digo

| Archivo | Descripci√≥n | LOC Estimado |
|---------|-------------|--------------|
| `core/graph.py` | State extendido + nodos voz | +250 |
| `agents/omni_pipeline.py` | TTS generation | +150 |
| `tests/test_graph_voice_routing.py` | Tests routing | 300 |
| `tests/test_emotion_modulation.py` | Tests modulaci√≥n | 350 |
| `tests/test_tts_generation.py` | Tests TTS | 200 |
| `tests/test_voice_e2e.py` | Tests integraci√≥n | 400 |
| `scripts/quantize_omni_to_q4.py` | Cuantizaci√≥n ONNX | 150 |
| `scripts/export_voice_metrics.py` | M√©tricas Prometheus | 200 |
| `extras/grafana_voice_dashboard.json` | Dashboard Grafana | N/A (JSON) |

**Total**: ~2,000 LOC + dashboard

### Documentaci√≥n

- `docs/M3.2_COMPLETION_REPORT.md`: Informe final del milestone
- `README_v2.11.md`: Actualizaci√≥n con instrucciones de voz
- `ARCHITECTURE.md`: Diagrama de flujo voz-LLM-voz

### Tests

- 34 test cases nuevos (m√≠nimo)
- Cobertura >90% en c√≥digo nuevo
- 20 golden queries de voz

---

## üöÄ Cronograma Detallado

| Fase | D√≠as | Fecha Inicio | Fecha Fin | Dependencias |
|------|------|--------------|-----------|--------------|
| **Fase 1**: State + Routing | 2 | 2025-10-29 | 2025-10-30 | M3.1 ‚úÖ |
| **Fase 2**: Modulaci√≥n Emoci√≥n | 2 | 2025-10-31 | 2025-11-01 | Fase 1 |
| **Fase 3**: TTS Integration | 1 | 2025-11-02 | 2025-11-02 | Fase 2 |
| **Fase 4**: Dashboard Grafana | 1.5 | 2025-11-03 | 2025-11-04 | Fase 3 |
| **Fase 5**: Optimizaci√≥n ONNX | 2 | 2025-11-04 | 2025-11-05 | - (paralelo) |
| **Fase 6**: Testing E2E | 1 | 2025-11-06 | 2025-11-06 | Todas |
| **Buffer**: Debugging | 0.5 | 2025-11-07 | 2025-11-07 | - |

**Total**: 10 d√≠as (incluyendo buffer)

---

## üîç Riesgos y Mitigaciones

### Riesgo 1: Latencia >2s en flujo completo
**Probabilidad**: Media  
**Impacto**: Alto (degrada UX)  
**Mitigaci√≥n**:
- Prefetch de LLM mientras Omni procesa audio
- Cach√© de modulaciones emocionales
- Optimizaci√≥n ONNX Q4 (Fase 5)

### Riesgo 2: RAM P99 >13GB
**Probabilidad**: Media  
**Impacto**: Alto (viola budget)  
**Mitigaci√≥n**:
- Descarga de Omni-3B tras 5 min sin uso (TTL)
- Compartir ONNX runtime entre STT y TTS
- Cuantizaci√≥n Q4 reduce 600MB

### Riesgo 3: MOS Score <4.3 tras modulaci√≥n
**Probabilidad**: Baja  
**Impacto**: Medio (calidad percibida)  
**Mitigaci√≥n**:
- Benchmark con evaluadores humanos
- A/B testing modulaci√≥n vs sin modulaci√≥n
- Fallback: no modular si emoci√≥n d√©bil (<0.6 confidence)

### Riesgo 4: Integraci√≥n LangGraph rompe flujo RAG
**Probabilidad**: Baja  
**Impacto**: Alto (regresi√≥n)  
**Mitigaci√≥n**:
- Golden queries (M2.9) como regression tests
- Tests E2E con todas las rutas (expert, tiny, rag, voice)
- CI/CD valida todos los tests antes de merge

---

## ‚úÖ Criterios de Aceptaci√≥n

### Funcionales

- [ ] ‚úÖ Audio input correctamente procesado (espa√±ol, ingl√©s, franc√©s)
- [ ] ‚úÖ Transcripci√≥n con WER <2%
- [ ] ‚úÖ Emoci√≥n detectada con accuracy >85%
- [ ] ‚úÖ Routing correcto a SOLAR/LFM2/RAG
- [ ] ‚úÖ Modulaci√≥n emocional aplicada (emp√°tico, urgente, neutral)
- [ ] ‚úÖ TTS genera audio de respuesta
- [ ] ‚úÖ MOS Score >4.3
- [ ] ‚úÖ Dashboard Grafana funcional

### No Funcionales

- [ ] ‚úÖ Latencia voz-a-voz P50 <300ms
- [ ] ‚úÖ Latencia voz-a-voz P99 <500ms
- [ ] ‚úÖ RAM P99 ‚â§13GB
- [ ] ‚úÖ Test coverage >90%
- [ ] ‚úÖ 0 regresi√≥n en golden queries
- [ ] ‚úÖ Safe Mode respetado (no procesa voz si activo)
- [ ] ‚úÖ Logs HMAC auditados (voice_interactions_*.jsonl)

### Documentaci√≥n

- [ ] ‚úÖ M3.2 Completion Report creado
- [ ] ‚úÖ README_v2.11.md actualizado con ejemplos de voz
- [ ] ‚úÖ ARCHITECTURE.md con diagrama voz-LLM-voz

---

## üéØ Siguientes Pasos (M3.3)

**M3.3 Voice TRM Training** (despu√©s de M3.2):
- Dataset sint√©tico: 10,000 comandos de voz
- Entrenar cabeza `voice_query` en TRM-Router
- Distilaci√≥n desde Whisper
- Target: Clasificaci√≥n voice vs text con >95% accuracy

**M3.4 Omni Optimization** (opcional):
- Cuantizaci√≥n a INT8 (si Q4 no suficiente)
- Pruning de modelo (eliminar capas de baja importancia)
- Target: 1.2GB RAM, <0.7% degradaci√≥n WER

---

## üìä M√©tricas de Progreso

**Actualizaci√≥n diaria** (checklist):

### D√≠a 1-2 (Fase 1: State + Routing)
- [ ] State extendido con campos de voz
- [ ] Nodo detect_input_type implementado
- [ ] Nodo process_voice implementado
- [ ] Routing condicional audio/text
- [ ] 8/8 tests de routing pasando

### D√≠a 3-4 (Fase 2: Modulaci√≥n Emoci√≥n)
- [ ] Nodo enhance_with_emotion implementado
- [ ] Cach√© de modulaciones LRU
- [ ] 10/10 tests de modulaci√≥n pasando
- [ ] Latencia modulaci√≥n <2s P50

### D√≠a 5 (Fase 3: TTS Integration)
- [ ] Nodo generate_tts implementado
- [ ] TTS function en omni_pipeline.py
- [ ] 6/6 tests de TTS pasando
- [ ] Latencia TTS <500ms P99

### D√≠a 6-7 (Fase 4: Dashboard Grafana)
- [ ] M√©tricas Prometheus exportadas
- [ ] Dashboard JSON creado
- [ ] 6 paneles funcionales
- [ ] Script de publicaci√≥n automatizado

### D√≠a 8-9 (Fase 5: Optimizaci√≥n ONNX)
- [ ] Script de cuantizaci√≥n Q4
- [ ] Benchmark FP16 vs Q4 completado
- [ ] Degradaci√≥n WER <0.5%
- [ ] RAM reducida a 1.5GB

### D√≠a 10 (Fase 6: Testing E2E)
- [ ] 34+ tests E2E pasando
- [ ] 20 golden queries validadas
- [ ] Benchmark 100 requests concurrentes
- [ ] 0 regresiones detectadas

---

**Documento creado**: 28 octubre 2025  
**Autor**: GitHub Copilot + Noel  
**Versi√≥n**: 1.0  
**Estado**: Planificaci√≥n (M3.1 completado, M3.2 pendiente)
