# üîç Estrategia de Modelos Externos - An√°lisis Cr√≠tico

**Fecha**: 2025-11-01  
**Contexto**: Replanteamiento de modelos externos para tareas espec√≠ficas  
**Objetivo**: Validar si LLaMarketing-8B, Qwen2.5-Coder-7B, y SOLAR-10.7B son necesarios

---

## üéØ Modelos Propuestos (Evaluaci√≥n)

### 1. LLaMarketing-8B (Marketing Especializado)

| Aspecto | An√°lisis |
|---------|----------|
| **Prop√≥sito** | Generaci√≥n de contenido de marketing (copywriting, ads, campa√±as) |
| **Tama√±o** | 8B par√°metros (~5 GB GGUF Q4_K_M) |
| **Caso de uso** | Marketing copy, social media, SEO content |
| **Alternativa** | SOLAR-10.7B + skill_marketing (prompt especializado) |
| **Justificaci√≥n** | ‚ùì ¬øSARAi necesita ser experto en marketing? |

**üî¥ RECOMENDACI√ìN**: **NO incluir** (fuera del scope de SARAi v2.14)

**Razones**:
1. SARAi es un asistente **t√©cnico + emocional**, no de marketing
2. SOLAR-10.7B ya cubre generaci√≥n de texto profesional
3. 5 GB adicionales de RAM sin justificaci√≥n t√©cnica
4. **Filosof√≠a Phoenix v2.12**: Skills son prompts, no modelos separados

**Si realmente necesario** (futuro):
- Implementar `skill_marketing` con prompt especializado en SOLAR
- Temperatura alta (0.9) para creatividad
- System prompt: _"Eres un experto en marketing digital..."_

---

### 2. Qwen2.5-Coder-7B (Desarrollo/C√≥digo)

| Aspecto | An√°lisis |
|---------|----------|
| **Prop√≥sito** | Generaci√≥n de c√≥digo, debugging, code review |
| **Tama√±o** | 7B par√°metros (~4.5 GB GGUF Q4_K_M) |
| **Caso de uso** | Autocompletado, generaci√≥n de funciones, refactoring |
| **Alternativa** | SOLAR-10.7B + skill_programming (ya implementado) |
| **Justificaci√≥n** | ‚ö†Ô∏è Podr√≠a mejorar calidad de c√≥digo, pero a qu√© costo? |

**üü° RECOMENDACI√ìN**: **CONDICIONAL** (solo si benchmark valida mejora >15%)

**Razones PRO**:
- Especializaci√≥n real: Qwen2.5-Coder entrenado espec√≠ficamente en c√≥digo
- Podr√≠a superar a SOLAR en tasks de programaci√≥n
- 4.5 GB manejables con ModelPool

**Razones CONTRA**:
- SOLAR-10.7B ya es muy competente en c√≥digo (10.7B > 7B en general)
- **Filosof√≠a Phoenix v2.12**: `skill_programming` ya existe
- A√±ade complejidad: ¬øcu√°ndo usar Coder vs SOLAR?

**Criterio de decisi√≥n**:
```python
# Benchmark necesario ANTES de incluir
benchmark_solar_programming = benchmark_code_quality(
    model="solar_short",
    skill="programming",
    queries=PROGRAMMING_QUERIES
)

benchmark_coder = benchmark_code_quality(
    model="qwen25_coder",
    queries=PROGRAMMING_QUERIES
)

if benchmark_coder["accuracy"] > benchmark_solar_programming["accuracy"] * 1.15:
    # Solo si mejora >15%, justifica la inclusi√≥n
    add_model("qwen25_coder")
else:
    # SOLAR + skill_programming es suficiente
    pass
```

**Implementaci√≥n tentativa** (comentada):
```yaml
# qwen25_coder:
#   name: "Qwen2.5-Coder-7B-Instruct"
#   type: "text"
#   backend: "ollama"  # O "gguf" si local
#   
#   # Solo para skill_programming
#   skill_override: "programming"  # Reemplaza SOLAR en este skill
#   
#   api_url: "${OLLAMA_BASE_URL}"
#   model_name: "qwen2.5-coder:7b-instruct"
#   
#   n_ctx: 2048
#   temperature: 0.3  # Determinista para c√≥digo
#   max_tokens: 1024
#   
#   load_on_demand: true
#   priority: 7
#   max_memory_mb: 0  # Ollama remoto
```

---

### 3. SOLAR-10.7B (Razonamiento General)

| Aspecto | An√°lisis |
|---------|----------|
| **Prop√≥sito** | Modelo general hard-skills (ya implementado) |
| **Tama√±o** | 10.7B par√°metros (~6 GB GGUF) |
| **Caso de uso** | Razonamiento t√©cnico, explicaciones, an√°lisis |
| **Status** | ‚úÖ **YA IMPLEMENTADO** en v2.14 |
| **Backend** | Ollama (192.168.0.251:11434) |

**‚úÖ RECOMENDACI√ìN**: **MANTENER** (ya operacional)

**Razones**:
- Core del sistema de hard-skills
- 0 RAM local (servidor Ollama remoto)
- Altamente competente en razonamiento general
- 10.7B es sweet-spot (ni muy grande ni muy peque√±o)

**Configuraci√≥n actual** (validada):
```yaml
solar_short:
  backend: "ollama"
  api_url: "${OLLAMA_BASE_URL}"
  model_name: "${SOLAR_MODEL_NAME}"
  n_ctx: 512
  priority: 9

solar_long:
  backend: "ollama"
  api_url: "${OLLAMA_BASE_URL}"
  model_name: "${SOLAR_MODEL_NAME}"
  n_ctx: 2048
  priority: 8
```

**Variables .env**:
```bash
OLLAMA_BASE_URL=http://192.168.0.251:11434
SOLAR_MODEL_NAME=una-solar-10.7b-instruct-v1.0-q4
```

---

## üìä An√°lisis de RAM (Peor Caso)

### Escenario Actual v2.14 (SOLO modelos core)

| Modelo | Backend | RAM Local | Estado |
|--------|---------|-----------|--------|
| **LFM2-1.2B** | GGUF local | ~700 MB | Siempre en memoria |
| **SOLAR-10.7B** | Ollama remoto | 0 MB | On-demand |
| **Qwen3-VL-4B** | Multimodal local | ~3.5 GB | On-demand |
| **Total P99** | - | **~4.2 GB** | ‚úÖ |

**Margen disponible**: 16 GB - 4.2 GB = **11.8 GB libre** ‚úÖ

---

### Escenario Propuesto (con modelos externos)

| Modelo | Backend | RAM Local | Justificaci√≥n |
|--------|---------|-----------|---------------|
| LFM2-1.2B | GGUF local | ~700 MB | Core soft-skills |
| SOLAR-10.7B | Ollama remoto | 0 MB | Core hard-skills |
| Qwen3-VL-4B | Multimodal local | ~3.5 GB | Vision |
| **LLaMarketing-8B** | Ollama remoto | 0 MB | ‚ùå NO justificado |
| **Qwen2.5-Coder-7B** | Ollama remoto | 0 MB | ‚ö†Ô∏è Condicional |
| **Total P99** | - | **~4.2 GB** | ‚úÖ |

**An√°lisis**:
- Si todos los modelos externos usan **Ollama remoto** ‚Üí 0 impacto RAM
- PERO: A√±ade **complejidad de routing** sin beneficio claro
- **Filosof√≠a Phoenix violada**: Skills deben ser configs, no modelos

---

## üéØ Recomendaci√≥n Final

### ‚úÖ INCLUIR en .env (M√≠nimo Viable)

```bash
# ============================================
# MODELOS EXTERNOS - OLLAMA REMOTO
# ============================================

# Servidor Ollama (desarrollo/producci√≥n)
OLLAMA_BASE_URL=http://192.168.0.251:11434

# Modelo Expert (hard-skills)
SOLAR_MODEL_NAME=una-solar-10.7b-instruct-v1.0-q4

# NOTA: LFM2 es local GGUF, no usa Ollama
```

**Total modelos externos**: **1** (SOLAR-10.7B)

---

### üîµ OPCIONAL en .env (Futuro, si benchmarks lo justifican)

```bash
# ============================================
# MODELOS ESPECIALIZADOS (OPCIONALES)
# ============================================

# Qwen2.5-Coder (solo si benchmark valida >15% mejora)
# Descomentar cuando:
#   1. Benchmark programming accuracy (SOLAR vs Coder) ejecutado
#   2. Mejora >15% en code generation
#   3. Validaci√≥n manual de calidad de c√≥digo
# QWEN_CODER_MODEL_NAME=qwen2.5-coder:7b-instruct

# LLaMarketing (NO RECOMENDADO - fuera de scope)
# SARAi es asistente t√©cnico+emocional, no de marketing
# Si necesario, usar SOLAR + skill_marketing (prompt especializado)
# LLAMARKETING_MODEL_NAME=llamarketing-8b
```

---

### ‚ùå NO INCLUIR (Violaci√≥n de Filosof√≠a Phoenix)

- **LLaMarketing-8B**: Fuera del scope t√©cnico de SARAi
- **Modelos de marketing**: Implementar como skills (prompts), no modelos separados
- **Modelos gen√©ricos**: SOLAR-10.7B ya cubre razonamiento general

---

## üìù Criterios de Decisi√≥n para Futuros Modelos Externos

### ‚úÖ S√ç incluir cuando:

1. **Especializaci√≥n demostrable**: Benchmark valida >15% mejora vs SOLAR
2. **Caso de uso espec√≠fico**: No cubierto por skills existentes
3. **RAM-neutral**: Ollama remoto (0 RAM local) o justifica el costo
4. **Filosof√≠a preservada**: Skills siguen siendo prompts, modelo es engine

### ‚ùå NO incluir cuando:

1. **Skill puede hacerlo**: Prompt especializado en SOLAR suficiente
2. **Fuera de scope**: Marketing, finanzas, legal (SARAi = t√©cnico+emocional)
3. **Mejora marginal**: <15% de mejora vs SOLAR
4. **Complejidad injustificada**: Routing complejo sin beneficio claro

---

## üöÄ Plan de Acci√≥n Propuesto

### Fase 1: v2.14 (AHORA) - Minimalista

1. ‚úÖ Mantener SOLAR-10.7B como √∫nico modelo externo
2. ‚úÖ .env con solo `OLLAMA_BASE_URL` y `SOLAR_MODEL_NAME`
3. ‚úÖ Skills implementados como prompts (programming, diagnosis, etc.)

**Archivo .env propuesto**:
```bash
# SARAi v2.14 - Configuraci√≥n de Modelos Externos
OLLAMA_BASE_URL=http://192.168.0.251:11434
SOLAR_MODEL_NAME=una-solar-10.7b-instruct-v1.0-q4
```

**Total complejidad**: M√≠nima ‚úÖ  
**Total RAM local**: 4.2 GB ‚úÖ  
**Filosof√≠a Phoenix**: Preservada ‚úÖ

---

### Fase 2: v2.15 (FUTURO) - Validaci√≥n Condicional

Si y solo si benchmarks lo justifican:

1. ‚ö†Ô∏è Benchmark SOLAR vs Qwen2.5-Coder en programming tasks
2. ‚ö†Ô∏è Validar mejora >15% en code quality
3. ‚ö†Ô∏è Implementar routing condicional: `skill_programming` ‚Üí Coder
4. ‚ö†Ô∏è A√±adir a .env solo si validaci√≥n positiva

**Criterio de √©xito**:
```python
# Ejecutar ANTES de incluir Qwen2.5-Coder
make benchmark VERSION=v2.14_solar
make benchmark VERSION=v2.15_coder
make benchmark-compare OLD=v2.14_solar NEW=v2.15_coder

# SI programming_accuracy mejora >15% ‚Üí Incluir
# SI mejora <15% ‚Üí Mantener SOLAR + skill_programming
```

---

### Fase 3: v2.16+ (LARGO PLAZO) - Evaluaci√≥n Continua

- Revisar nuevos modelos especializados cada 6 meses
- Aplicar criterios de decisi√≥n estrictos
- Priorizar filosof√≠a Phoenix sobre especializaci√≥n

---

## üí° Filosof√≠a Final

**Mantra v2.14 Modelos Externos**:

> _"Un modelo especializado debe probar con benchmarks que es >15% mejor  
> que SOLAR + skill_config. Si no lo prueba, es solo ruido.  
> SARAi prefiere la simplicidad documentada sobre la complejidad especulativa."_

**Regla de oro**:
```
Nuevo modelo externo = Benchmark obligatorio ANTES de incluir
Sin benchmark = Sin inclusi√≥n
```

**Prioridades**:
1. ü•á Filosof√≠a Phoenix (skills = configs)
2. ü•à Simplicidad operacional
3. ü•â Especializaci√≥n validada (solo si benchmark lo prueba)

---

## üìä Resumen Ejecutivo

| Modelo | Incluir v2.14? | Raz√≥n | Alternativa |
|--------|----------------|-------|-------------|
| **SOLAR-10.7B** | ‚úÖ S√ç | Core hard-skills, ya operacional | N/A |
| **LLaMarketing-8B** | ‚ùå NO | Fuera de scope t√©cnico | SOLAR + skill_marketing |
| **Qwen2.5-Coder-7B** | üü° CONDICIONAL | Solo si benchmark >15% mejora | SOLAR + skill_programming |

**Decisi√≥n final v2.14**: 
- `.env` con **solo SOLAR-10.7B** (minimalista)
- Evaluar Qwen2.5-Coder en v2.15 **solo si benchmark lo justifica**
- NO incluir LLaMarketing (violaci√≥n de scope)

---

**Creado**: 2025-11-01  
**Versi√≥n**: v2.14  
**Status**: Propuesta para validaci√≥n  
**Next**: Crear .env minimalista con solo SOLAR-10.7B
