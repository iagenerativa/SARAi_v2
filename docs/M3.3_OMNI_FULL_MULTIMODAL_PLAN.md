# M3.3 Omni Full Multimodal - Master Plan

**Status**: üî¥ **PLANNED** (Revolutionary Extension)  
**Dependencies**: M3.2 Fase 2 (‚úÖ COMPLETE)  
**Estimated Duration**: 5-7 days  
**Target Date**: Nov 4-11, 2025  
**LOC Estimate**: ~2,500 l√≠neas (1,500 prod + 800 tests + 200 docs)

---

## üöÄ Vision Statement

> **"SARAi trasciende el chat de voz para convertirse en un asistente OMNI verdadero: audio-to-audio nativo, comprensi√≥n de im√°genes, an√°lisis de video, y s√≠ntesis de voz emocional. Todo en un modelo unificado end-to-end."**

---

## üéØ Capacidades Reales de Qwen3-VL-4B-Instruct/7B

### ‚úÖ Confirmadas (Seg√∫n Documentaci√≥n Oficial)

#### 1. **Audio-to-Audio (Native)**
```python
# Input: Audio bytes (voz usuario)
# Output: Audio bytes (voz SARAi) + texto
text_ids, audio = model.generate(audio_input, return_audio=True)
```
**Features**:
- ‚úÖ STT + LLM + TTS en **un solo forward pass**
- ‚úÖ Streaming (chunked input/output)
- ‚úÖ 2 voces disponibles: Chelsie (female), Ethan (male)
- ‚úÖ Latencia ultra-baja (<250ms end-to-end)
- ‚úÖ Multi-idioma nativo (es, en)

#### 2. **Vision (Image Understanding)**
```python
conversation = [{
    "role": "user",
    "content": [
        {"type": "image", "image": "path/to/image.jpg"},
        {"type": "text", "text": "¬øQu√© ves en esta imagen?"}
    ]
}]
```
**Features**:
- ‚úÖ Object detection
- ‚úÖ Scene understanding
- ‚úÖ OCR (text extraction)
- ‚úÖ Visual reasoning (MMMU benchmark)
- ‚úÖ Comparable a Qwen2.5-VL-7B

#### 3. **Video Understanding (with Audio)**
```python
conversation = [{
    "role": "user",
    "content": [
        {"type": "video", "video": "https://example.com/video.mp4"},
        {"type": "text", "text": "Resume este video"}
    ]
}]
```
**Features**:
- ‚úÖ Temporal reasoning (eventos a lo largo del tiempo)
- ‚úÖ Audio en video (detecta sonidos, m√∫sica, di√°logos)
- ‚úÖ TMRoPE (Time-aligned Multimodal RoPE) para sincronizaci√≥n
- ‚úÖ MVBench performance

#### 4. **Multimodal Fusion**
```python
# Combinar m√∫ltiples modalidades en UNA query
conversation = [{
    "role": "user",
    "content": [
        {"type": "audio", "audio": "question.wav"},      # Pregunta en audio
        {"type": "image", "image": "diagram.png"},       # Diagrama
        {"type": "text", "text": "Contexto adicional"}   # Texto
    ]
}]
```
**Features**:
- ‚úÖ OmniBench SOTA (state-of-the-art)
- ‚úÖ Cross-modal reasoning
- ‚úÖ Unified embedding space

---

## üèóÔ∏è Arquitectura Propuesta: SARAi Omni-Full

### Dise√±o de Alto Nivel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SARAi v2.12 Omni-Full                            ‚îÇ
‚îÇ              (Any-to-Any Multimodal Assistant)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  Input Detector        ‚îÇ
           ‚îÇ  (Modality Router)     ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                 ‚îÇ                 ‚îÇ                  ‚îÇ
     ‚ñº                 ‚ñº                 ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Audio   ‚îÇ      ‚îÇ Image   ‚îÇ      ‚îÇ Video   ‚îÇ      ‚îÇ Multimodal  ‚îÇ
‚îÇ Only    ‚îÇ      ‚îÇ Only    ‚îÇ      ‚îÇ (A+V)   ‚îÇ      ‚îÇ Fusion      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                 ‚îÇ                 ‚îÇ                  ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ Qwen3-VL-4B-Instruct      ‚îÇ
                   ‚îÇ (Unified Encoder)    ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ LangGraph Router     ‚îÇ
                   ‚îÇ (Skill Selection)    ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                     ‚îÇ                     ‚îÇ
        ‚ñº                     ‚ñº                     ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ SOLAR  ‚îÇ          ‚îÇ LFM2   ‚îÇ          ‚îÇ RAG        ‚îÇ
   ‚îÇ (Tech) ‚îÇ          ‚îÇ (Soft) ‚îÇ          ‚îÇ (Web)      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                     ‚îÇ                     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ Emotion Modulator    ‚îÇ
                   ‚îÇ (Prosody Application)‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ Qwen3-VL-4B-Instruct      ‚îÇ
                   ‚îÇ (Audio Synthesis)    ‚îÇ
                   ‚îÇ Chelsie/Ethan Voice  ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                      Audio + Text Output
```

---

## üì¶ M√≥dulos a Implementar

### 1. `agents/omni_multimodal.py` (~600 LOC)

**Prop√≥sito**: Wrapper unificado para todas las capacidades Omni.

```python
# agents/omni_multimodal.py

from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Literal, Union
import logging
import numpy as np
import torch
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

logger = logging.getLogger(__name__)

@dataclass
class MultimodalInput:
    """
    Input multimodal unificado
    
    Soporta combinaciones:
    - Solo texto
    - Solo audio
    - Solo imagen
    - Solo video
    - Audio + Imagen
    - Audio + Video
    - Texto + Imagen + Audio (fusion)
    """
    text: Optional[str] = None
    audio: Optional[bytes] = None
    image: Optional[Union[str, bytes]] = None  # Path, URL o bytes
    video: Optional[Union[str, bytes]] = None
    use_audio_in_video: bool = True
    
    def to_conversation(self) -> List[Dict]:
        """Convierte a formato de conversaci√≥n Qwen"""
        content = []
        
        if self.text:
            content.append({"type": "text", "text": self.text})
        
        if self.audio:
            content.append({"type": "audio", "audio": self.audio})
        
        if self.image:
            content.append({"type": "image", "image": self.image})
        
        if self.video:
            content.append({"type": "video", "video": self.video})
        
        return [{
            "role": "user",
            "content": content
        }]

@dataclass
class MultimodalOutput:
    """
    Output multimodal unificado
    """
    text: str                           # Respuesta textual
    audio: Optional[np.ndarray] = None  # Audio s√≠ntesis (24kHz)
    detected_modalities: List[str] = None  # ["audio", "image", "video"]
    metadata: Dict[str, Any] = None
    
    # Vision outputs (si aplicable)
    detected_objects: Optional[List[str]] = None
    scene_description: Optional[str] = None
    ocr_text: Optional[str] = None
    
    # Audio outputs (si aplicable)
    detected_emotion: Optional[str] = None
    detected_language: Optional[str] = None
    speech_confidence: Optional[float] = None
    
    # Video outputs (si aplicable)
    video_summary: Optional[str] = None
    key_events: Optional[List[str]] = None
    audio_description: Optional[str] = None

class OmniMultimodal:
    """
    Wrapper completo de Qwen2.5-Omni con TODAS las capacidades
    
    Features:
    - Audio-to-Audio nativo (STT + LLM + TTS en 1 pass)
    - Vision (image understanding + OCR)
    - Video (temporal reasoning + audio-in-video)
    - Multimodal fusion (audio + image + video simult√°neos)
    - Voice selection (Chelsie/Ethan)
    - Emotion-aware prosody
    """
    
    SYSTEM_PROMPT_OMNI = (
        "You are Qwen, a virtual human developed by the Qwen Team, "
        "Alibaba Group, capable of perceiving auditory and visual inputs, "
        "as well as generating text and speech."
    )
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen3-VL-4B-Instruct",
                 device: str = "auto",
                 enable_audio_output: bool = True,
                 default_speaker: Literal["Chelsie", "Ethan"] = "Chelsie"):
        """
        Initialize Omni model
        
        Args:
            model_name: HuggingFace model ID
            device: "cpu", "cuda", or "auto"
            enable_audio_output: Si False, ahorra ~2GB RAM
            default_speaker: Voz por defecto (Chelsie=female, Ethan=male)
        """
        self.model_name = model_name
        self.device = device
        self.enable_audio_output = enable_audio_output
        self.default_speaker = default_speaker
        
        self._model = None  # Lazy load
        self._processor = None
        
        logger.info(f"OmniMultimodal initialized: {model_name}")
    
    def load_model(self):
        """Lazy loading del modelo Omni"""
        if self._model is not None:
            return
        
        logger.info(f"Loading {self.model_name}...")
        
        self._model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Deshabilitar TTS si no se necesita (ahorra ~2GB)
        if not self.enable_audio_output:
            self._model.disable_talker()
            logger.info("‚úÖ Talker disabled (text-only mode)")
        
        self._processor = Qwen2_5OmniProcessor.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        logger.info(f"‚úÖ Omni model loaded on {self.device}")
    
    def process(self, 
                multimodal_input: MultimodalInput,
                return_audio: bool = True,
                speaker: Optional[str] = None,
                emotion: Optional[str] = None) -> MultimodalOutput:
        """
        Procesa input multimodal y genera output
        
        Args:
            multimodal_input: Input con modalidades combinadas
            return_audio: Si generar audio de respuesta
            speaker: "Chelsie" o "Ethan" (override default)
            emotion: Emoci√≥n para modular prosody (de emotion_modulator)
        
        Returns:
            MultimodalOutput con texto + audio + metadata
        """
        self.load_model()
        
        # Construir conversaci√≥n
        conversation = [
            {
                "role": "system",
                "content": [{"type": "text", "text": self.SYSTEM_PROMPT_OMNI}]
            }
        ] + multimodal_input.to_conversation()
        
        # Detectar modalidades presentes
        detected_modalities = self._detect_modalities(multimodal_input)
        
        # Preparar input
        text = self._processor.apply_chat_template(
            conversation,
            add_generation_prompt=True,
            tokenize=False
        )
        
        # Procesar multimedia
        audios, images, videos = process_mm_info(
            conversation,
            use_audio_in_video=multimodal_input.use_audio_in_video
        )
        
        inputs = self._processor(
            text=text,
            audio=audios,
            images=images,
            videos=videos,
            return_tensors="pt",
            padding=True,
            use_audio_in_video=multimodal_input.use_audio_in_video
        )
        
        inputs = inputs.to(self._model.device)
        
        # Generar respuesta
        generate_kwargs = {
            "use_audio_in_video": multimodal_input.use_audio_in_video,
            "return_audio": return_audio and self.enable_audio_output,
            "max_new_tokens": 1000,
            "temperature": 0.7,
            "do_sample": True
        }
        
        # Seleccionar voz
        if return_audio and self.enable_audio_output:
            generate_kwargs["speaker"] = speaker or self.default_speaker
        
        # Generar
        if return_audio and self.enable_audio_output:
            text_ids, audio_output = self._model.generate(**inputs, **generate_kwargs)
        else:
            text_ids = self._model.generate(**inputs, **generate_kwargs)
            audio_output = None
        
        # Decodificar texto
        response_text = self._processor.batch_decode(
            text_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # Extraer metadata seg√∫n modalidad
        metadata = self._extract_metadata(
            response_text,
            detected_modalities,
            multimodal_input
        )
        
        return MultimodalOutput(
            text=response_text,
            audio=audio_output.reshape(-1).detach().cpu().numpy() if audio_output is not None else None,
            detected_modalities=detected_modalities,
            metadata=metadata,
            **self._parse_specialized_outputs(response_text, detected_modalities)
        )
    
    def _detect_modalities(self, input: MultimodalInput) -> List[str]:
        """Detecta qu√© modalidades est√°n presentes"""
        modalities = []
        if input.audio:
            modalities.append("audio")
        if input.image:
            modalities.append("image")
        if input.video:
            modalities.append("video")
        if input.text:
            modalities.append("text")
        return modalities
    
    def _extract_metadata(self, 
                         response: str,
                         modalities: List[str],
                         input: MultimodalInput) -> Dict[str, Any]:
        """Extrae metadata seg√∫n modalidades procesadas"""
        metadata = {
            "model": self.model_name,
            "modalities_used": modalities,
            "audio_enabled": self.enable_audio_output,
            "speaker": self.default_speaker if "audio" in modalities else None
        }
        
        # Metadata espec√≠fica por modalidad
        if "audio" in modalities:
            metadata["audio_input_size"] = len(input.audio) if input.audio else 0
        
        if "image" in modalities:
            metadata["image_type"] = type(input.image).__name__
        
        if "video" in modalities:
            metadata["video_type"] = type(input.video).__name__
            metadata["use_audio_in_video"] = input.use_audio_in_video
        
        return metadata
    
    def _parse_specialized_outputs(self, 
                                   response: str,
                                   modalities: List[str]) -> Dict[str, Any]:
        """
        Parse outputs especializados seg√∫n modalidad
        
        Por ejemplo, si hay imagen:
        - detected_objects: Lista de objetos detectados
        - scene_description: Descripci√≥n de la escena
        - ocr_text: Texto extra√≠do
        """
        outputs = {}
        
        # Vision outputs (b√°sico, puede mejorarse con prompts espec√≠ficos)
        if "image" in modalities:
            outputs["scene_description"] = response  # El modelo ya describe
            # TODO: Parsing m√°s sofisticado
        
        # Video outputs
        if "video" in modalities:
            outputs["video_summary"] = response
            # TODO: Extraer key events con prompts estructurados
        
        return outputs
    
    # ===== High-Level Methods =====
    
    def audio_to_audio(self, 
                      audio_bytes: bytes,
                      speaker: str = None,
                      emotion: str = None) -> MultimodalOutput:
        """
        Modo audio-to-audio puro (nativo)
        
        Ideal para: Voice chat, asistente de voz
        """
        input_data = MultimodalInput(audio=audio_bytes)
        return self.process(input_data, return_audio=True, speaker=speaker, emotion=emotion)
    
    def describe_image(self, 
                      image: Union[str, bytes],
                      question: Optional[str] = None) -> MultimodalOutput:
        """
        An√°lisis de imagen
        
        Args:
            image: Path, URL o bytes de imagen
            question: Pregunta espec√≠fica (opcional)
        
        Returns:
            Descripci√≥n de la imagen
        """
        text = question or "Describe esta imagen en detalle."
        input_data = MultimodalInput(image=image, text=text)
        return self.process(input_data, return_audio=False)
    
    def analyze_video(self,
                     video: Union[str, bytes],
                     question: Optional[str] = None,
                     use_audio: bool = True) -> MultimodalOutput:
        """
        An√°lisis de video (con o sin audio)
        
        Args:
            video: Path o URL del video
            question: Pregunta espec√≠fica
            use_audio: Si procesar audio del video
        
        Returns:
            Resumen del video + eventos clave
        """
        text = question or "Resume este video y describe los eventos importantes."
        input_data = MultimodalInput(
            video=video,
            text=text,
            use_audio_in_video=use_audio
        )
        return self.process(input_data, return_audio=False)
    
    def multimodal_query(self,
                        text: str,
                        audio: Optional[bytes] = None,
                        image: Optional[Union[str, bytes]] = None,
                        video: Optional[Union[str, bytes]] = None,
                        return_audio: bool = True) -> MultimodalOutput:
        """
        Query multimodal completo (fusion)
        
        Ejemplo:
            - Audio: Usuario pregunta en voz
            - Imagen: Diagrama t√©cnico
            - Texto: Contexto adicional
        
        Returns:
            Respuesta que integra TODAS las modalidades
        """
        input_data = MultimodalInput(
            text=text,
            audio=audio,
            image=image,
            video=video
        )
        return self.process(input_data, return_audio=return_audio)


# ===== Factory Functions =====

def create_omni_full(enable_audio: bool = True) -> OmniMultimodal:
    """
    Factory para crear OmniMultimodal
    
    Args:
        enable_audio: Si habilitar s√≠ntesis de audio (~2GB extra)
    
    Returns:
        OmniMultimodal instance
    """
    return OmniMultimodal(
        model_name="Qwen/Qwen3-VL-4B-Instruct",
        device="auto",
        enable_audio_output=enable_audio,
        default_speaker="Chelsie"
    )
```

---

### 2. `core/graph.py` - Extensi√≥n Multimodal (~200 LOC)

**Prop√≥sito**: Integrar capacidades Omni en LangGraph.

```python
# core/graph.py - Extensi√≥n Multimodal

class SARAiGraph:
    # ... c√≥digo existente ...
    
    def _process_multimodal_input(self, state: State) -> dict:
        """
        NUEVO NODO: Procesa input multimodal
        
        Detecta y procesa:
        - Im√°genes (OCR, object detection, scene understanding)
        - Videos (resumen, eventos, audio-in-video)
        - Combinaciones (audio + imagen, etc.)
        
        Updates state con:
        - Transcripci√≥n (si hay audio)
        - Descripci√≥n visual (si hay imagen/video)
        - Metadata multimodal
        """
        from agents.omni_multimodal import create_omni_full, MultimodalInput
        
        # Detectar modalidades presentes
        has_image = state.get("image_input") is not None
        has_video = state.get("video_input") is not None
        has_audio = state.get("audio_input") is not None
        
        if not (has_image or has_video):
            # Solo audio/texto ‚Üí usar pipeline normal
            return {}
        
        logger.info(f"üé® Processing multimodal input: "
                   f"image={has_image}, video={has_video}, audio={has_audio}")
        
        # Crear input multimodal
        mm_input = MultimodalInput(
            text=state.get("input", ""),
            audio=state.get("audio_input"),
            image=state.get("image_input"),
            video=state.get("video_input"),
            use_audio_in_video=True
        )
        
        # Procesar con Omni
        omni = create_omni_full(enable_audio=False)  # Solo an√°lisis
        result = omni.process(mm_input, return_audio=False)
        
        # Actualizar state con resultados
        updates = {
            "multimodal_analysis": result.text,
            "detected_modalities": result.detected_modalities,
            "multimodal_metadata": result.metadata
        }
        
        # Si hay imagen: a√±adir descripci√≥n visual al contexto
        if has_image:
            updates["visual_context"] = result.scene_description
            updates["ocr_text"] = result.ocr_text
        
        # Si hay video: a√±adir resumen temporal
        if has_video:
            updates["video_summary"] = result.video_summary
            updates["key_events"] = result.key_events
        
        # CR√çTICO: Actualizar input para LLM con contexto enriquecido
        enriched_input = f"{state['input']}\n\n"
        
        if has_image:
            enriched_input += f"[Contexto Visual]: {result.scene_description}\n"
            if result.ocr_text:
                enriched_input += f"[Texto en Imagen]: {result.ocr_text}\n"
        
        if has_video:
            enriched_input += f"[Resumen Video]: {result.video_summary}\n"
        
        updates["input"] = enriched_input
        
        logger.info(f"‚úÖ Multimodal analysis complete: {len(enriched_input)} chars")
        
        return updates
    
    def compile(self) -> CompiledGraph:
        """Compila grafo con soporte multimodal"""
        workflow = StateGraph(State)
        
        # ... nodos existentes ...
        
        # NUEVO: Nodo multimodal ANTES de classify
        workflow.add_node("process_multimodal", self._process_multimodal_input)
        
        # Routing modificado:
        # Input ‚Üí detect_input_type ‚Üí [multimodal] ‚Üí process_multimodal ‚Üí classify
        #                           ‚îî‚Üí [text/audio] ‚Üí classify
        
        workflow.set_entry_point("detect_input_type")
        
        workflow.add_conditional_edges(
            "detect_input_type",
            self._route_by_modality,
            {
                "multimodal": "process_multimodal",
                "simple": "classify"
            }
        )
        
        workflow.add_edge("process_multimodal", "classify")
        
        # ... resto del grafo ...
        
        return workflow.compile()
    
    def _route_by_modality(self, state: State) -> str:
        """Decide si usar procesamiento multimodal"""
        if state.get("image_input") or state.get("video_input"):
            return "multimodal"
        return "simple"
```

---

### 3. `agents/vision_skills.py` (~400 LOC)

**Prop√≥sito**: Skills especializados para visi√≥n.

```python
# agents/vision_skills.py

from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class ObjectDetection:
    """Resultado de detecci√≥n de objetos"""
    label: str
    confidence: float
    bbox: List[int]  # [x, y, width, height]

class VisionSkills:
    """
    Skills especializados para procesamiento de visi√≥n
    
    Construidos sobre Qwen2.5-Omni pero con prompts optimizados
    """
    
    def __init__(self, omni_model):
        self.omni = omni_model
    
    def detect_objects(self, image: bytes) -> List[ObjectDetection]:
        """
        Detecci√≥n de objetos en imagen
        
        Prompt optimizado para listar objetos detectables
        """
        prompt = (
            "List all objects you can see in this image. "
            "Format: 'object1, object2, object3, ...'"
        )
        
        result = self.omni.describe_image(image, question=prompt)
        
        # Parse response
        objects_text = result.text
        objects = [obj.strip() for obj in objects_text.split(',')]
        
        # TODO: Convertir a ObjectDetection con bboxes (requiere prompts m√°s avanzados)
        return [ObjectDetection(label=obj, confidence=0.9, bbox=[0,0,0,0]) 
                for obj in objects]
    
    def extract_text_ocr(self, image: bytes) -> str:
        """
        OCR: Extrae todo el texto visible en la imagen
        """
        prompt = (
            "Extract ALL text visible in this image. "
            "Include labels, captions, watermarks, etc. "
            "If no text is visible, say 'No text detected'."
        )
        
        result = self.omni.describe_image(image, question=prompt)
        return result.text
    
    def describe_scene(self, image: bytes, detail_level: str = "high") -> str:
        """
        Descripci√≥n detallada de la escena
        
        Args:
            detail_level: "low", "medium", "high"
        """
        prompts = {
            "low": "Describe this image briefly in 1-2 sentences.",
            "medium": "Describe this image in detail (3-5 sentences).",
            "high": "Provide a comprehensive description of this image, including objects, people, actions, colors, lighting, and mood."
        }
        
        result = self.omni.describe_image(image, question=prompts[detail_level])
        return result.text
    
    def answer_visual_question(self, image: bytes, question: str) -> str:
        """
        VQA (Visual Question Answering)
        
        Examples:
        - "¬øCu√°ntas personas hay en esta imagen?"
        - "¬øQu√© color tiene el coche?"
        - "¬øEst√° lloviendo en la imagen?"
        """
        result = self.omni.describe_image(image, question=question)
        return result.text
    
    def compare_images(self, image1: bytes, image2: bytes) -> str:
        """
        Comparaci√≥n de 2 im√°genes
        
        TODO: Requiere procesamiento secuencial o multi-turn conversation
        """
        # Analizar primera imagen
        desc1 = self.describe_scene(image1, detail_level="medium")
        
        # Analizar segunda imagen
        desc2 = self.describe_scene(image2, detail_level="medium")
        
        # Generar comparaci√≥n (usando LLM backend)
        comparison_prompt = f"""
        Image 1: {desc1}
        Image 2: {desc2}
        
        Compare these two images. What are the similarities and differences?
        """
        
        # TODO: Usar SOLAR/LFM2 para s√≠ntesis de comparaci√≥n
        return f"Comparison pending implementation"
```

---

### 4. `agents/video_skills.py` (~300 LOC)

**Prop√≥sito**: Skills especializados para video.

```python
# agents/video_skills.py

from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class VideoEvent:
    """Evento detectado en video"""
    timestamp: float  # Segundos desde inicio
    description: str
    confidence: float

class VideoSkills:
    """
    Skills especializados para procesamiento de video
    """
    
    def __init__(self, omni_model):
        self.omni = omni_model
    
    def summarize_video(self, video_path: str, max_length: int = 200) -> str:
        """
        Resumen de video
        
        Args:
            video_path: Path o URL del video
            max_length: Palabras m√°ximas del resumen
        """
        prompt = f"Summarize this video in maximum {max_length} words."
        
        result = self.omni.analyze_video(video_path, question=prompt)
        return result.text
    
    def detect_key_events(self, video_path: str) -> List[VideoEvent]:
        """
        Detecta eventos importantes en el video
        
        Prompt optimizado para extraer timeline
        """
        prompt = (
            "List all key events in this video in chronological order. "
            "Format: 'At [time], [event description]'"
        )
        
        result = self.omni.analyze_video(video_path, question=prompt)
        
        # Parse events
        # TODO: Parsing m√°s robusto con regex
        events = []
        for line in result.text.split('\n'):
            if 'At' in line:
                # Placeholder parsing
                events.append(VideoEvent(
                    timestamp=0.0,
                    description=line,
                    confidence=0.8
                ))
        
        return events
    
    def transcribe_with_context(self, video_path: str) -> Dict[str, Any]:
        """
        Transcripci√≥n de audio con contexto visual
        
        Returns:
            {
                "transcript": "...",
                "visual_context": "...",
                "speaker_changes": [...]
            }
        """
        prompt = (
            "Transcribe all spoken dialogue in this video. "
            "Also describe what is happening visually during each dialogue."
        )
        
        result = self.omni.analyze_video(video_path, question=prompt, use_audio=True)
        
        return {
            "transcript": result.text,
            "visual_context": result.video_summary,
            "audio_description": result.audio_description
        }
    
    def answer_video_question(self, video_path: str, question: str) -> str:
        """
        Responde preguntas sobre el video
        
        Examples:
        - "¬øQu√© hace la persona al inicio del video?"
        - "¬øCu√°ntas veces aparece un gato?"
        - "¬øQu√© m√∫sica se escucha?"
        """
        result = self.omni.analyze_video(video_path, question=question)
        return result.text
```

---

## üß™ Testing Strategy

### Test Suite Completa (~800 LOC)

```python
# tests/test_omni_multimodal.py

import pytest
import numpy as np
from agents.omni_multimodal import (
    OmniMultimodal,
    MultimodalInput,
    MultimodalOutput
)

class TestOmniMultimodal:
    """Test suite para capacidades multimodales"""
    
    @pytest.fixture
    def omni_engine(self):
        """Engine con audio habilitado"""
        return OmniMultimodal(enable_audio_output=True)
    
    @pytest.fixture
    def omni_text_only(self):
        """Engine sin audio (ahorra RAM)"""
        return OmniMultimodal(enable_audio_output=False)
    
    # ===== Audio-to-Audio Tests =====
    
    def test_audio_to_audio_basic(self, omni_engine):
        """Test audio-to-audio nativo"""
        # Generar audio de test
        audio_input = generate_test_audio("Hola, ¬øc√≥mo est√°s?")
        
        result = omni_engine.audio_to_audio(audio_input)
        
        assert isinstance(result, MultimodalOutput)
        assert result.text != ""
        assert result.audio is not None
        assert isinstance(result.audio, np.ndarray)
        assert "audio" in result.detected_modalities
    
    def test_audio_speaker_selection(self, omni_engine):
        """Verifica que se puede cambiar la voz"""
        audio_input = generate_test_audio("Test voice")
        
        result_chelsie = omni_engine.audio_to_audio(audio_input, speaker="Chelsie")
        result_ethan = omni_engine.audio_to_audio(audio_input, speaker="Ethan")
        
        # Los audios deben ser diferentes (diferentes voces)
        assert not np.array_equal(result_chelsie.audio, result_ethan.audio)
    
    # ===== Vision Tests =====
    
    def test_describe_image(self, omni_text_only):
        """Test an√°lisis de imagen"""
        image_path = "tests/fixtures/test_image.jpg"
        
        result = omni_text_only.describe_image(image_path)
        
        assert result.text != ""
        assert "image" in result.detected_modalities
        assert result.scene_description is not None
    
    def test_image_ocr(self, omni_text_only):
        """Test OCR en imagen con texto"""
        image_with_text = "tests/fixtures/text_image.jpg"
        
        from agents.vision_skills import VisionSkills
        skills = VisionSkills(omni_text_only)
        
        ocr_text = skills.extract_text_ocr(image_with_text)
        
        assert len(ocr_text) > 0
        assert "No text detected" not in ocr_text
    
    def test_visual_question_answering(self, omni_text_only):
        """Test VQA"""
        image_path = "tests/fixtures/people_image.jpg"
        
        from agents.vision_skills import VisionSkills
        skills = VisionSkills(omni_text_only)
        
        answer = skills.answer_visual_question(
            image_path,
            "¬øCu√°ntas personas hay en la imagen?"
        )
        
        assert answer != ""
        # TODO: Validar que la respuesta es un n√∫mero
    
    # ===== Video Tests =====
    
    def test_video_summary(self, omni_text_only):
        """Test resumen de video"""
        video_url = "https://example.com/test_video.mp4"
        
        result = omni_text_only.analyze_video(video_url)
        
        assert result.text != ""
        assert "video" in result.detected_modalities
        assert result.video_summary is not None
    
    def test_video_with_audio(self, omni_text_only):
        """Test video con procesamiento de audio"""
        video_path = "tests/fixtures/video_with_speech.mp4"
        
        result = omni_text_only.analyze_video(video_path, use_audio=True)
        
        assert result.audio_description is not None
    
    # ===== Multimodal Fusion Tests =====
    
    def test_multimodal_fusion_audio_image(self, omni_engine):
        """Test fusi√≥n audio + imagen"""
        audio_input = generate_test_audio("¬øQu√© ves en esta imagen?")
        image_input = "tests/fixtures/test_image.jpg"
        
        result = omni_engine.multimodal_query(
            text="",
            audio=audio_input,
            image=image_input,
            return_audio=True
        )
        
        assert "audio" in result.detected_modalities
        assert "image" in result.detected_modalities
        assert result.audio is not None  # Respuesta de voz
    
    def test_multimodal_complex(self, omni_engine):
        """Test fusi√≥n compleja: texto + audio + imagen"""
        mm_input = MultimodalInput(
            text="Contexto: an√°lisis t√©cnico requerido",
            audio=generate_test_audio("Explica este diagrama"),
            image="tests/fixtures/diagram.png"
        )
        
        result = omni_engine.process(mm_input, return_audio=True)
        
        assert len(result.detected_modalities) >= 3
        assert result.text != ""
    
    # ===== Performance Benchmarks =====
    
    def test_benchmark_audio_to_audio_latency(self, omni_engine, benchmark):
        """Benchmark latencia audio-to-audio"""
        audio = generate_test_audio("Test benchmark", duration=3.0)
        
        result = benchmark(omni_engine.audio_to_audio, audio)
        
        # Target: <500ms en GPU, <2s en CPU
        # pytest-benchmark mostrar√° stats
    
    def test_benchmark_vision_latency(self, omni_text_only, benchmark):
        """Benchmark an√°lisis de imagen"""
        image = "tests/fixtures/test_image.jpg"
        
        result = benchmark(omni_text_only.describe_image, image)
    
    # ===== Integration Tests =====
    
    @pytest.mark.skipif(
        not is_gpu_available(),
        reason="GPU required for full multimodal"
    )
    def test_integration_full_pipeline(self, omni_engine):
        """Test integraci√≥n completa con LangGraph"""
        from core.graph import SARAiGraph
        
        graph = SARAiGraph()
        compiled = graph.compile()
        
        # Input multimodal
        state = {
            "input": "Analiza esta imagen y responde en voz",
            "image_input": "tests/fixtures/test_image.jpg",
            "input_type": "multimodal"
        }
        
        # Ejecutar grafo
        result = compiled.invoke(state)
        
        assert result["response"] != ""
        assert result.get("audio_output") is not None
```

---

## üìä KPIs de √âxito

| KPI | Target | M√©todo |
|-----|--------|--------|
| **Audio-to-Audio Latency** | <500ms P50, <1s P99 | Benchmark end-to-end |
| **Vision Accuracy** | >80% en VQA | Dataset MMMU subset |
| **Video Understanding** | >75% en MVBench | Benchmark oficial |
| **OCR Accuracy** | >90% en texto claro | Dataset interno |
| **Multimodal Fusion** | >85% en OmniBench | Benchmark oficial |
| **RAM Usage** | ‚â§15GB con audio, ‚â§13GB sin audio | Monitor runtime |
| **Test Coverage** | >90% | pytest --cov |

---

## üöÄ Implementation Roadmap

### Fase 3.1: Audio-to-Audio Nativo (1 d√≠a)
- [ ] Implementar `OmniMultimodal` con audio_to_audio()
- [ ] Voice selection (Chelsie/Ethan)
- [ ] Tests audio-to-audio (10 tests)
- [ ] Benchmark latencia <500ms

### Fase 3.2: Vision Skills (1.5 d√≠as)
- [ ] Implementar `VisionSkills`
- [ ] OCR, object detection, scene description
- [ ] VQA (Visual Question Answering)
- [ ] Tests visi√≥n (15 tests)

### Fase 3.3: Video Skills (1.5 d√≠as)
- [ ] Implementar `VideoSkills`
- [ ] Resumen de video, detecci√≥n de eventos
- [ ] Transcripci√≥n con contexto visual
- [ ] Tests video (12 tests)

### Fase 3.4: Multimodal Fusion (1 d√≠a)
- [ ] Integraci√≥n LangGraph
- [ ] Nodo `process_multimodal`
- [ ] Routing por modalidad
- [ ] Tests integraci√≥n (10 tests)

### Fase 3.5: Optimizaci√≥n y Docs (1 d√≠a)
- [ ] Optimizar uso de memoria
- [ ] Documentaci√≥n completa
- [ ] Ejemplos de uso
- [ ] Completion report

**Total**: 6 d√≠as  
**LOC Estimado**: ~2,500

---

## üéØ Use Cases Revolucionarios

### 1. Asistente M√©dico Visual
```python
# Usuario env√≠a foto de una lesi√≥n
image = "patient_skin_lesion.jpg"
audio = record_audio("Doctor, ¬øqu√© opinas de esto?")

result = omni.multimodal_query(
    audio=audio,
    image=image,
    return_audio=True
)

# SARAi responde en voz con an√°lisis visual
# "Veo una lesi√≥n de aproximadamente 2cm de di√°metro..."
```

### 2. Tutor Educativo Interactivo
```python
# Estudiante muestra diagrama y pregunta en voz
video = "student_explaining_diagram.mp4"  # Con audio

result = omni.analyze_video(video, use_audio=True)

# SARAi detecta errores en la explicaci√≥n Y en el diagrama
```

### 3. An√°lisis de Seguridad en Tiempo Real
```python
# Video de c√°mara de seguridad
video_stream = "security_cam_feed.mp4"

events = video_skills.detect_key_events(video_stream)
# [(0.5s, "Persona entra por puerta principal"),
#  (3.2s, "Movimiento sospechoso detectado"),
#  (5.1s, "Objeto dejado en esquina")]
```

### 4. Accesibilidad para Personas con Discapacidad Visual
```python
# Usuario ciego toma foto con m√≥vil
image = camera.capture()
audio = record_audio("¬øQu√© hay frente a m√≠?")

result = omni.multimodal_query(audio=audio, image=image, return_audio=True)

# SARAi describe el entorno en voz:
# "Frente a ti hay una puerta de madera cerrada, 
#  a la izquierda un sof√° gris, y a la derecha una ventana..."
```

---

## üèÜ Ventaja Competitiva

### vs. Otros Asistentes de Voz

| Feature | SARAi Omni-Full | GPT-4o | Gemini 1.5 Pro | Alexa/Siri |
|---------|-----------------|--------|----------------|------------|
| Audio-to-Audio Nativo | ‚úÖ End-to-end | ‚ùå Pipeline | ‚ùå Pipeline | ‚ö†Ô∏è Limitado |
| Vision + Audio Simult√°neo | ‚úÖ Fusion | ‚úÖ | ‚úÖ | ‚ùå |
| Video Understanding | ‚úÖ Con audio-in-video | ‚úÖ | ‚úÖ | ‚ùå |
| Local/On-Premise | ‚úÖ 100% | ‚ùå Cloud only | ‚ùå Cloud only | ‚ùå Cloud only |
| Emotional Prosody | ‚úÖ (Fase 2) | ‚ö†Ô∏è B√°sico | ‚ö†Ô∏è B√°sico | ‚ö†Ô∏è B√°sico |
| Skills Modulares | ‚úÖ (v2.12) | ‚ùå | ‚ùå | ‚ö†Ô∏è Limited |
| **Costo** | **$0 (local)** | **$$$** | **$$$** | **Gratis (limitado)** |

---

## üö® Riesgos y Mitigaciones

### Riesgo 1: RAM Overflow con Multimodal
**Probabilidad**: Alta  
**Impacto**: Cr√≠tico  
**Mitigaci√≥n**:
- Usar `disable_talker()` cuando no se necesita audio (-2GB)
- Lazy loading de Omni model
- Procesar imagen/video en chunks si es muy grande
- Monitoreo de RAM con l√≠mite estricto 15GB

### Riesgo 2: Latencia alta en CPU
**Probabilidad**: Alta  
**Impacto**: Alto  
**Mitigaci√≥n**:
- Cache agresivo de an√°lisis de im√°genes
- Downscale de im√°genes antes de procesar (resize)
- Usar Omni-3B en vez de Omni-7B (m√°s r√°pido)
- ONNX Q4 optimization en Fase 5

### Riesgo 3: Precisi√≥n baja en OCR/Objetos
**Probabilidad**: Media  
**Impacto**: Medio  
**Mitigaci√≥n**:
- Prompts optimizados con ejemplos (few-shot)
- Post-procesamiento con regex
- Fallback a bibliotecas especializadas (Tesseract para OCR)

---

## üéì Conclusi√≥n

**M3.3 Omni Full Multimodal** transforma SARAi en un **asistente AGI verdadero**:

‚úÖ **Any-to-Any**: Texto, audio, imagen, video ‚Üí Texto + audio  
‚úÖ **End-to-End**: Sin pipelines fragmentados, todo en un modelo  
‚úÖ **Emotional**: Prosody emocional integrada (Fase 2)  
‚úÖ **Modular**: Skills especializados (vision, video, fusion)  
‚úÖ **Local**: 100% on-premise, privacidad garantizada  
‚úÖ **Escalable**: Fundaci√≥n para v2.12 MoE skills  

**Next Step**: Aprobar plan ‚Üí Implementar Fase 3.1 (Audio-to-Audio) ma√±ana.

---

**Preparado por**: SARAi  
**Fecha**: Oct 28, 2025  
**Versi√≥n**: 1.0 - Master Plan  
**Status**: Ready for Revolutionary Implementation üöÄüåü
