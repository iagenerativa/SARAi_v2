# M3.3 Omni Full Multimodal - Master Plan

**Status**: ðŸ”´ **PLANNED** (Revolutionary Extension)  
**Dependencies**: M3.2 Fase 2 (âœ… COMPLETE)  
**Estimated Duration**: 5-7 days  
**Target Date**: Nov 4-11, 2025  
**LOC Estimate**: ~2,500 lÃ­neas (1,500 prod + 800 tests + 200 docs)

---

## ðŸš€ Vision Statement

> **"SARAi trasciende el chat de voz para convertirse en un asistente OMNI verdadero: audio-to-audio nativo, comprensiÃ³n de imÃ¡genes, anÃ¡lisis de video, y sÃ­ntesis de voz emocional. Todo en un modelo unificado end-to-end."**

---

## ðŸŽ¯ Capacidades Reales de Qwen3-VL-4B-Instruct/7B

### âœ… Confirmadas (SegÃºn DocumentaciÃ³n Oficial)

#### 1. **Audio-to-Audio (Native)**
```python
# Input: Audio bytes (voz usuario)
# Output: Audio bytes (voz SARAi) + texto
text_ids, audio = model.generate(audio_input, return_audio=True)
```
**Features**:
- âœ… STT + LLM + TTS en **un solo forward pass**
- âœ… Streaming (chunked input/output)
- âœ… 2 voces disponibles: Chelsie (female), Ethan (male)
- âœ… Latencia ultra-baja (<250ms end-to-end)
- âœ… Multi-idioma nativo (es, en)

#### 2. **Vision (Image Understanding)**
```python
conversation = [{
    "role": "user",
    "content": [
        {"type": "image", "image": "path/to/image.jpg"},
        {"type": "text", "text": "Â¿QuÃ© ves en esta imagen?"}
    ]
}]
```
**Features**:
- âœ… Object detection
- âœ… Scene understanding
- âœ… OCR (text extraction)
- âœ… Visual reasoning (MMMU benchmark)
- âœ… Comparable a Qwen2.5-VL-7B

#### 3. **Video Understanding (with Audio)**
```python
conversation = [{
    "role": "user",
    "content": [
        {"type": "video", "video": "https://example.com/video.mp4"},
        {"type": "text", "text": "Resume este video"}
    ]
}]
```
**Features**:
- âœ… Temporal reasoning (eventos a lo largo del tiempo)
- âœ… Audio en video (detecta sonidos, mÃºsica, diÃ¡logos)
- âœ… TMRoPE (Time-aligned Multimodal RoPE) para sincronizaciÃ³n
- âœ… MVBench performance

#### 4. **Multimodal Fusion**
```python
# Combinar mÃºltiples modalidades en UNA query
conversation = [{
    "role": "user",
    "content": [
        {"type": "audio", "audio": "question.wav"},      # Pregunta en audio
        {"type": "image", "image": "diagram.png"},       # Diagrama
        {"type": "text", "text": "Contexto adicional"}   # Texto
    ]
}]
```
**Features**:
- âœ… OmniBench SOTA (state-of-the-art)
- âœ… Cross-modal reasoning
- âœ… Unified embedding space

---

## ðŸ—ï¸ Arquitectura Propuesta: SARAi Omni-Full

### DiseÃ±o de Alto Nivel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SARAi v2.12 Omni-Full                            â”‚
â”‚              (Any-to-Any Multimodal Assistant)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Input Detector        â”‚
           â”‚  (Modality Router)     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                 â”‚                 â”‚                  â”‚
     â–¼                 â–¼                 â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio   â”‚      â”‚ Image   â”‚      â”‚ Video   â”‚      â”‚ Multimodal  â”‚
â”‚ Only    â”‚      â”‚ Only    â”‚      â”‚ (A+V)   â”‚      â”‚ Fusion      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Qwen3-VL-4B-Instruct      â”‚
                   â”‚ (Unified Encoder)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ LangGraph Router     â”‚
                   â”‚ (Skill Selection)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ SOLAR  â”‚          â”‚ LFM2   â”‚          â”‚ RAG        â”‚
   â”‚ (Tech) â”‚          â”‚ (Soft) â”‚          â”‚ (Web)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Emotion Modulator    â”‚
                   â”‚ (Prosody Application)â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Qwen3-VL-4B-Instruct      â”‚
                   â”‚ (Audio Synthesis)    â”‚
                   â”‚ Chelsie/Ethan Voice  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                      Audio + Text Output
```

---

## ðŸ“¦ MÃ³dulos a Implementar

### 1. `agents/omni_multimodal.py` (~600 LOC)

**PropÃ³sito**: Wrapper unificado para todas las capacidades Omni.

```python
# agents/omni_multimodal.py

from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Literal, Union
import logging
import numpy as np
import torch
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

logger = logging.getLogger(__name__)

@dataclass
class MultimodalInput:
    """
    Input multimodal unificado
    
    Soporta combinaciones:
    - Solo texto
    - Solo audio
    - Solo imagen
    - Solo video
    - Audio + Imagen
    - Audio + Video
    - Texto + Imagen + Audio (fusion)
    """
    text: Optional[str] = None
    audio: Optional[bytes] = None
    image: Optional[Union[str, bytes]] = None  # Path, URL o bytes
    video: Optional[Union[str, bytes]] = None
    use_audio_in_video: bool = True
    
    def to_conversation(self) -> List[Dict]:
        """Convierte a formato de conversaciÃ³n Qwen"""
        content = []
        
        if self.text:
            content.append({"type": "text", "text": self.text})
        
        if self.audio:
            content.append({"type": "audio", "audio": self.audio})
        
        if self.image:
            content.append({"type": "image", "image": self.image})
        
        if self.video:
            content.append({"type": "video", "video": self.video})
        
        return [{
            "role": "user",
            "content": content
        }]

@dataclass
class MultimodalOutput:
    """
    Output multimodal unificado
    """
    text: str                           # Respuesta textual
    audio: Optional[np.ndarray] = None  # Audio sÃ­ntesis (24kHz)
    detected_modalities: List[str] = None  # ["audio", "image", "video"]
    metadata: Dict[str, Any] = None
    
    # Vision outputs (si aplicable)
    detected_objects: Optional[List[str]] = None
    scene_description: Optional[str] = None
    ocr_text: Optional[str] = None
    
    # Audio outputs (si aplicable)
    detected_emotion: Optional[str] = None
    detected_language: Optional[str] = None
    speech_confidence: Optional[float] = None
    
    # Video outputs (si aplicable)
    video_summary: Optional[str] = None
    key_events: Optional[List[str]] = None
    audio_description: Optional[str] = None

class OmniMultimodal:
    """
    Wrapper completo de Qwen2.5-Omni con TODAS las capacidades
    
    Features:
    - Audio-to-Audio nativo (STT + LLM + TTS en 1 pass)
    - Vision (image understanding + OCR)
    - Video (temporal reasoning + audio-in-video)
    - Multimodal fusion (audio + image + video simultÃ¡neos)
    - Voice selection (Chelsie/Ethan)
    - Emotion-aware prosody
    """
    
    SYSTEM_PROMPT_OMNI = (
        "You are Qwen, a virtual human developed by the Qwen Team, "
        "Alibaba Group, capable of perceiving auditory and visual inputs, "
        "as well as generating text and speech."
    )
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen3-VL-4B-Instruct",
                 device: str = "auto",
                 enable_audio_output: bool = True,
                 default_speaker: Literal["Chelsie", "Ethan"] = "Chelsie"):
        """
        Initialize Omni model
        
        Args:
            model_name: HuggingFace model ID
            device: "cpu", "cuda", or "auto"
            enable_audio_output: Si False, ahorra ~2GB RAM
            default_speaker: Voz por defecto (Chelsie=female, Ethan=male)
        """
        self.model_name = model_name
        self.device = device
        self.enable_audio_output = enable_audio_output
        self.default_speaker = default_speaker
        
        self._model = None  # Lazy load
        self._processor = None
        
        logger.info(f"OmniMultimodal initialized: {model_name}")
    
    def load_model(self):
        """Lazy loading del modelo Omni"""
        if self._model is not None:
            return
        
        logger.info(f"Loading {self.model_name}...")
        
        self._model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Deshabilitar TTS si no se necesita (ahorra ~2GB)
        if not self.enable_audio_output:
            self._model.disable_talker()
            logger.info("âœ… Talker disabled (text-only mode)")
        
        self._processor = Qwen2_5OmniProcessor.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        logger.info(f"âœ… Omni model loaded on {self.device}")
    
    def process(self, 
                multimodal_input: MultimodalInput,
                return_audio: bool = True,
                speaker: Optional[str] = None,
                emotion: Optional[str] = None) -> MultimodalOutput:
        """
        Procesa input multimodal y genera output
        
        Args:
            multimodal_input: Input con modalidades combinadas
            return_audio: Si generar audio de respuesta
            speaker: "Chelsie" o "Ethan" (override default)
            emotion: EmociÃ³n para modular prosody (de emotion_modulator)
        
        Returns:
            MultimodalOutput con texto + audio + metadata
        """
        self.load_model()
        
        # Construir conversaciÃ³n
        conversation = [
            {
                "role": "system",
                "content": [{"type": "text", "text": self.SYSTEM_PROMPT_OMNI}]
            }
        ] + multimodal_input.to_conversation()
        
        # Detectar modalidades presentes
        detected_modalities = self._detect_modalities(multimodal_input)
        
        # Preparar input
        text = self._processor.apply_chat_template(
            conversation,
            add_generation_prompt=True,
            tokenize=False
        )
        
        # Procesar multimedia
        audios, images, videos = process_mm_info(
            conversation,
            use_audio_in_video=multimodal_input.use_audio_in_video
        )
        
        inputs = self._processor(
            text=text,
            audio=audios,
            images=images,
            videos=videos,
            return_tensors="pt",
            padding=True,
            use_audio_in_video=multimodal_input.use_audio_in_video
        )
        
        inputs = inputs.to(self._model.device)
        
        # Generar respuesta
        generate_kwargs = {
            "use_audio_in_video": multimodal_input.use_audio_in_video,
            "return_audio": return_audio and self.enable_audio_output,
            "max_new_tokens": 1000,
            "temperature": 0.7,
            "do_sample": True
        }
        
        # Seleccionar voz
        if return_audio and self.enable_audio_output:
            generate_kwargs["speaker"] = speaker or self.default_speaker
        
        # Generar
        if return_audio and self.enable_audio_output:
            text_ids, audio_output = self._model.generate(**inputs, **generate_kwargs)
        else:
            text_ids = self._model.generate(**inputs, **generate_kwargs)
            audio_output = None
        
        # Decodificar texto
        response_text = self._processor.batch_decode(
            text_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # Extraer metadata segÃºn modalidad
        metadata = self._extract_metadata(
            response_text,
            detected_modalities,
            multimodal_input
        )
        
        return MultimodalOutput(
            text=response_text,
            audio=audio_output.reshape(-1).detach().cpu().numpy() if audio_output is not None else None,
            detected_modalities=detected_modalities,
            metadata=metadata,
            **self._parse_specialized_outputs(response_text, detected_modalities)
        )
    
    def _detect_modalities(self, input: MultimodalInput) -> List[str]:
        """Detecta quÃ© modalidades estÃ¡n presentes"""
        modalities = []
        if input.audio:
            modalities.append("audio")
        if input.image:
            modalities.append("image")
        if input.video:
            modalities.append("video")
        if input.text:
            modalities.append("text")
        return modalities
    
    def _extract_metadata(self, 
                         response: str,
                         modalities: List[str],
                         input: MultimodalInput) -> Dict[str, Any]:
        """Extrae metadata segÃºn modalidades procesadas"""
        metadata = {
            "model": self.model_name,
            "modalities_used": modalities,
            "audio_enabled": self.enable_audio_output,
            "speaker": self.default_speaker if "audio" in modalities else None
        }
        
        # Metadata especÃ­fica por modalidad
        if "audio" in modalities:
            metadata["audio_input_size"] = len(input.audio) if input.audio else 0
        
        if "image" in modalities:
            metadata["image_type"] = type(input.image).__name__
        
        if "video" in modalities:
            metadata["video_type"] = type(input.video).__name__
            metadata["use_audio_in_video"] = input.use_audio_in_video
        
        return metadata
    
    def _parse_specialized_outputs(self, 
                                   response: str,
                                   modalities: List[str]) -> Dict[str, Any]:
        """
        Parse outputs especializados segÃºn modalidad
        
        Por ejemplo, si hay imagen:
        - detected_objects: Lista de objetos detectados
        - scene_description: DescripciÃ³n de la escena
        - ocr_text: Texto extraÃ­do
        """
        outputs = {}
        
        # Vision outputs (bÃ¡sico, puede mejorarse con prompts especÃ­ficos)
        if "image" in modalities:
            outputs["scene_description"] = response  # El modelo ya describe
            # TODO: Parsing mÃ¡s sofisticado
        
        # Video outputs
        if "video" in modalities:
            outputs["video_summary"] = response
            # TODO: Extraer key events con prompts estructurados
        
        return outputs
    
    # ===== High-Level Methods =====
    
    def audio_to_audio(self, 
                      audio_bytes: bytes,
                      speaker: str = None,
                      emotion: str = None) -> MultimodalOutput:
        """
        Modo audio-to-audio puro (nativo)
        
        Ideal para: Voice chat, asistente de voz
        """
        input_data = MultimodalInput(audio=audio_bytes)
        return self.process(input_data, return_audio=True, speaker=speaker, emotion=emotion)
    
    def describe_image(self, 
                      image: Union[str, bytes],
                      question: Optional[str] = None) -> MultimodalOutput:
        """
        AnÃ¡lisis de imagen
        
        Args:
            image: Path, URL o bytes de imagen
            question: Pregunta especÃ­fica (opcional)
        
        Returns:
            DescripciÃ³n de la imagen
        """
        text = question or "Describe esta imagen en detalle."
        input_data = MultimodalInput(image=image, text=text)
        return self.process(input_data, return_audio=False)
    
    def analyze_video(self,
                     video: Union[str, bytes],
                     question: Optional[str] = None,
                     use_audio: bool = True) -> MultimodalOutput:
        """
        AnÃ¡lisis de video (con o sin audio)
        
        Args:
            video: Path o URL del video
            question: Pregunta especÃ­fica
            use_audio: Si procesar audio del video
        
        Returns:
            Resumen del video + eventos clave
        """
        text = question or "Resume este video y describe los eventos importantes."
        input_data = MultimodalInput(
            video=video,
            text=text,
            use_audio_in_video=use_audio
        )
        return self.process(input_data, return_audio=False)
    
    def multimodal_query(self,
                        text: str,
                        audio: Optional[bytes] = None,
                        image: Optional[Union[str, bytes]] = None,
                        video: Optional[Union[str, bytes]] = None,
                        return_audio: bool = True) -> MultimodalOutput:
        """
        Query multimodal completo (fusion)
        
        Ejemplo:
            - Audio: Usuario pregunta en voz
            - Imagen: Diagrama tÃ©cnico
            - Texto: Contexto adicional
        
        Returns:
            Respuesta que integra TODAS las modalidades
        """
        input_data = MultimodalInput(
            text=text,
            audio=audio,
            image=image,
            video=video
        )
        return self.process(input_data, return_audio=return_audio)


# ===== Factory Functions =====

def create_omni_full(enable_audio: bool = True) -> OmniMultimodal:
    """
    Factory para crear OmniMultimodal
    
    Args:
        enable_audio: Si habilitar sÃ­ntesis de audio (~2GB extra)
    
    Returns:
        OmniMultimodal instance
    """
    return OmniMultimodal(
        model_name="Qwen/Qwen3-VL-4B-Instruct",
        device="auto",
        enable_audio_output=enable_audio,
        default_speaker="Chelsie"
    )
```

---

### 2. `core/graph.py` - ExtensiÃ³n Multimodal (~200 LOC)

**PropÃ³sito**: Integrar capacidades Omni en LangGraph.

```python
# core/graph.py - ExtensiÃ³n Multimodal

class SARAiGraph:
    # ... cÃ³digo existente ...
    
    def _process_multimodal_input(self, state: State) -> dict:
        """
        NUEVO NODO: Procesa input multimodal
        
        Detecta y procesa:
        - ImÃ¡genes (OCR, object detection, scene understanding)
        - Videos (resumen, eventos, audio-in-video)
        - Combinaciones (audio + imagen, etc.)
        
        Updates state con:
        - TranscripciÃ³n (si hay audio)
        - DescripciÃ³n visual (si hay imagen/video)
        - Metadata multimodal
        """
        from agents.omni_multimodal import create_omni_full, MultimodalInput
        
        # Detectar modalidades presentes
        has_image = state.get("image_input") is not None
        has_video = state.get("video_input") is not None
        has_audio = state.get("audio_input") is not None
        
        if not (has_image or has_video):
            # Solo audio/texto â†’ usar pipeline normal
            return {}
        
        logger.info(f"ðŸŽ¨ Processing multimodal input: "
                   f"image={has_image}, video={has_video}, audio={has_audio}")
        
        # Crear input multimodal
        mm_input = MultimodalInput(
            text=state.get("input", ""),
            audio=state.get("audio_input"),
            image=state.get("image_input"),
            video=state.get("video_input"),
            use_audio_in_video=True
        )
        
        # Procesar con Omni
        omni = create_omni_full(enable_audio=False)  # Solo anÃ¡lisis
        result = omni.process(mm_input, return_audio=False)
        
        # Actualizar state con resultados
        updates = {
            "multimodal_analysis": result.text,
            "detected_modalities": result.detected_modalities,
            "multimodal_metadata": result.metadata
        }
        
        # Si hay imagen: aÃ±adir descripciÃ³n visual al contexto
        if has_image:
            updates["visual_context"] = result.scene_description
            updates["ocr_text"] = result.ocr_text
        
        # Si hay video: aÃ±adir resumen temporal
        if has_video:
            updates["video_summary"] = result.video_summary
            updates["key_events"] = result.key_events
        
        # CRÃTICO: Actualizar input para LLM con contexto enriquecido
        enriched_input = f"{state['input']}\n\n"
        
        if has_image:
            enriched_input += f"[Contexto Visual]: {result.scene_description}\n"
            if result.ocr_text:
                enriched_input += f"[Texto en Imagen]: {result.ocr_text}\n"
        
        if has_video:
            enriched_input += f"[Resumen Video]: {result.video_summary}\n"
        
        updates["input"] = enriched_input
        
        logger.info(f"âœ… Multimodal analysis complete: {len(enriched_input)} chars")
        
        return updates
    
    def compile(self) -> CompiledGraph:
        """Compila grafo con soporte multimodal"""
        workflow = StateGraph(State)
        
        # ... nodos existentes ...
        
        # NUEVO: Nodo multimodal ANTES de classify
        workflow.add_node("process_multimodal", self._process_multimodal_input)
        
        # Routing modificado:
        # Input â†’ detect_input_type â†’ [multimodal] â†’ process_multimodal â†’ classify
        #                           â””â†’ [text/audio] â†’ classify
        
        workflow.set_entry_point("detect_input_type")
        
        workflow.add_conditional_edges(
            "detect_input_type",
            self._route_by_modality,
            {
                "multimodal": "process_multimodal",
                "simple": "classify"
            }
        )
        
        workflow.add_edge("process_multimodal", "classify")
        
        # ... resto del grafo ...
        
        return workflow.compile()
    
    def _route_by_modality(self, state: State) -> str:
        """Decide si usar procesamiento multimodal"""
        if state.get("image_input") or state.get("video_input"):
            return "multimodal"
        return "simple"
```

---

### 3. `agents/vision_skills.py` (~400 LOC)

**PropÃ³sito**: Skills especializados para visiÃ³n.

```python
# agents/vision_skills.py

from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class ObjectDetection:
    """Resultado de detecciÃ³n de objetos"""
    label: str
    confidence: float
    bbox: List[int]  # [x, y, width, height]

class VisionSkills:
    """
    Skills especializados para procesamiento de visiÃ³n
    
    Construidos sobre Qwen2.5-Omni pero con prompts optimizados
    """
    
    def __init__(self, omni_model):
        self.omni = omni_model
    
    def detect_objects(self, image: bytes) -> List[ObjectDetection]:
        """
        DetecciÃ³n de objetos en imagen
        
        Prompt optimizado para listar objetos detectables
        """
        prompt = (
            "List all objects you can see in this image. "
            "Format: 'object1, object2, object3, ...'"
        )
        
        result = self.omni.describe_image(image, question=prompt)
        
        # Parse response
        objects_text = result.text
        objects = [obj.strip() for obj in objects_text.split(',')]
        
        # TODO: Convertir a ObjectDetection con bboxes (requiere prompts mÃ¡s avanzados)
        return [ObjectDetection(label=obj, confidence=0.9, bbox=[0,0,0,0]) 
                for obj in objects]
    
    def extract_text_ocr(self, image: bytes) -> str:
        """
        OCR: Extrae todo el texto visible en la imagen
        """
        prompt = (
            "Extract ALL text visible in this image. "
            "Include labels, captions, watermarks, etc. "
            "If no text is visible, say 'No text detected'."
        )
        
        result = self.omni.describe_image(image, question=prompt)
        return result.text
    
    def describe_scene(self, image: bytes, detail_level: str = "high") -> str:
        """
        DescripciÃ³n detallada de la escena
        
        Args:
            detail_level: "low", "medium", "high"
        """
        prompts = {
            "low": "Describe this image briefly in 1-2 sentences.",
            "medium": "Describe this image in detail (3-5 sentences).",
            "high": "Provide a comprehensive description of this image, including objects, people, actions, colors, lighting, and mood."
        }
        
        result = self.omni.describe_image(image, question=prompts[detail_level])
        return result.text
    
    def answer_visual_question(self, image: bytes, question: str) -> str:
        """
        VQA (Visual Question Answering)
        
        Examples:
        - "Â¿CuÃ¡ntas personas hay en esta imagen?"
        - "Â¿QuÃ© color tiene el coche?"
        - "Â¿EstÃ¡ lloviendo en la imagen?"
        """
        result = self.omni.describe_image(image, question=question)
        return result.text
    
    def compare_images(self, image1: bytes, image2: bytes) -> str:
        """
        ComparaciÃ³n de 2 imÃ¡genes
        
        TODO: Requiere procesamiento secuencial o multi-turn conversation
        """
        # Analizar primera imagen
        desc1 = self.describe_scene(image1, detail_level="medium")
        
        # Analizar segunda imagen
        desc2 = self.describe_scene(image2, detail_level="medium")
        
        # Generar comparaciÃ³n (usando LLM backend)
        comparison_prompt = f"""
        Image 1: {desc1}
        Image 2: {desc2}
        
        Compare these two images. What are the similarities and differences?
        """
        
        # TODO: Usar SOLAR/LFM2 para sÃ­ntesis de comparaciÃ³n
        return f"Comparison pending implementation"
```

---

### 4. `agents/video_skills.py` (~300 LOC)

**PropÃ³sito**: Skills especializados para video.

```python
# agents/video_skills.py

from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class VideoEvent:
    """Evento detectado en video"""
    timestamp: float  # Segundos desde inicio
    description: str
    confidence: float

class VideoSkills:
    """
    Skills especializados para procesamiento de video
    """
    
    def __init__(self, omni_model):
        self.omni = omni_model
    
    def summarize_video(self, video_path: str, max_length: int = 200) -> str:
        """
        Resumen de video
        
        Args:
            video_path: Path o URL del video
            max_length: Palabras mÃ¡ximas del resumen
        """
        prompt = f"Summarize this video in maximum {max_length} words."
        
        result = self.omni.analyze_video(video_path, question=prompt)
        return result.text
    
    def detect_key_events(self, video_path: str) -> List[VideoEvent]:
        """
        Detecta eventos importantes en el video
        
        Prompt optimizado para extraer timeline
        """
        prompt = (
            "List all key events in this video in chronological order. "
            "Format: 'At [time], [event description]'"
        )
        
        result = self.omni.analyze_video(video_path, question=prompt)
        
        # Parse events
        # TODO: Parsing mÃ¡s robusto con regex
        events = []
        for line in result.text.split('\n'):
            if 'At' in line:
                # Placeholder parsing
                events.append(VideoEvent(
                    timestamp=0.0,
                    description=line,
                    confidence=0.8
                ))
        
        return events
    
    def transcribe_with_context(self, video_path: str) -> Dict[str, Any]:
        """
        TranscripciÃ³n de audio con contexto visual
        
        Returns:
            {
                "transcript": "...",
                "visual_context": "...",
                "speaker_changes": [...]
            }
        """
        prompt = (
            "Transcribe all spoken dialogue in this video. "
            "Also describe what is happening visually during each dialogue."
        )
        
        result = self.omni.analyze_video(video_path, question=prompt, use_audio=True)
        
        return {
            "transcript": result.text,
            "visual_context": result.video_summary,
            "audio_description": result.audio_description
        }
    
    def answer_video_question(self, video_path: str, question: str) -> str:
        """
        Responde preguntas sobre el video
        
        Examples:
        - "Â¿QuÃ© hace la persona al inicio del video?"
        - "Â¿CuÃ¡ntas veces aparece un gato?"
        - "Â¿QuÃ© mÃºsica se escucha?"
        """
        result = self.omni.analyze_video(video_path, question=question)
        return result.text
```

---

## ðŸ§ª Testing Strategy

### Test Suite Completa (~800 LOC)

```python
# tests/test_omni_multimodal.py

import pytest
import numpy as np
from agents.omni_multimodal import (
    OmniMultimodal,
    MultimodalInput,
    MultimodalOutput
)

class TestOmniMultimodal:
    """Test suite para capacidades multimodales"""
    
    @pytest.fixture
    def omni_engine(self):
        """Engine con audio habilitado"""
        return OmniMultimodal(enable_audio_output=True)
    
    @pytest.fixture
    def omni_text_only(self):
        """Engine sin audio (ahorra RAM)"""
        return OmniMultimodal(enable_audio_output=False)
    
    # ===== Audio-to-Audio Tests =====
    
    def test_audio_to_audio_basic(self, omni_engine):
        """Test audio-to-audio nativo"""
        # Generar audio de test
        audio_input = generate_test_audio("Hola, Â¿cÃ³mo estÃ¡s?")
        
        result = omni_engine.audio_to_audio(audio_input)
        
        assert isinstance(result, MultimodalOutput)
        assert result.text != ""
        assert result.audio is not None
        assert isinstance(result.audio, np.ndarray)
        assert "audio" in result.detected_modalities
    
    def test_audio_speaker_selection(self, omni_engine):
        """Verifica que se puede cambiar la voz"""
        audio_input = generate_test_audio("Test voice")
        
        result_chelsie = omni_engine.audio_to_audio(audio_input, speaker="Chelsie")
        result_ethan = omni_engine.audio_to_audio(audio_input, speaker="Ethan")
        
        # Los audios deben ser diferentes (diferentes voces)
        assert not np.array_equal(result_chelsie.audio, result_ethan.audio)
    
    # ===== Vision Tests =====
    
    def test_describe_image(self, omni_text_only):
        """Test anÃ¡lisis de imagen"""
        image_path = "tests/fixtures/test_image.jpg"
        
        result = omni_text_only.describe_image(image_path)
        
        assert result.text != ""
        assert "image" in result.detected_modalities
        assert result.scene_description is not None
    
    def test_image_ocr(self, omni_text_only):
        """Test OCR en imagen con texto"""
        image_with_text = "tests/fixtures/text_image.jpg"
        
        from agents.vision_skills import VisionSkills
        skills = VisionSkills(omni_text_only)
        
        ocr_text = skills.extract_text_ocr(image_with_text)
        
        assert len(ocr_text) > 0
        assert "No text detected" not in ocr_text
    
    def test_visual_question_answering(self, omni_text_only):
        """Test VQA"""
        image_path = "tests/fixtures/people_image.jpg"
        
        from agents.vision_skills import VisionSkills
        skills = VisionSkills(omni_text_only)
        
        answer = skills.answer_visual_question(
            image_path,
            "Â¿CuÃ¡ntas personas hay en la imagen?"
        )
        
        assert answer != ""
        # TODO: Validar que la respuesta es un nÃºmero
    
    # ===== Video Tests =====
    
    def test_video_summary(self, omni_text_only):
        """Test resumen de video"""
        video_url = "https://example.com/test_video.mp4"
        
        result = omni_text_only.analyze_video(video_url)
        
        assert result.text != ""
        assert "video" in result.detected_modalities
        assert result.video_summary is not None
    
    def test_video_with_audio(self, omni_text_only):
        """Test video con procesamiento de audio"""
        video_path = "tests/fixtures/video_with_speech.mp4"
        
        result = omni_text_only.analyze_video(video_path, use_audio=True)
        
        assert result.audio_description is not None
    
    # ===== Multimodal Fusion Tests =====
    
    def test_multimodal_fusion_audio_image(self, omni_engine):
        """Test fusiÃ³n audio + imagen"""
        audio_input = generate_test_audio("Â¿QuÃ© ves en esta imagen?")
        image_input = "tests/fixtures/test_image.jpg"
        
        result = omni_engine.multimodal_query(
            text="",
            audio=audio_input,
            image=image_input,
            return_audio=True
        )
        
        assert "audio" in result.detected_modalities
        assert "image" in result.detected_modalities
        assert result.audio is not None  # Respuesta de voz
    
    def test_multimodal_complex(self, omni_engine):
        """Test fusiÃ³n compleja: texto + audio + imagen"""
        mm_input = MultimodalInput(
            text="Contexto: anÃ¡lisis tÃ©cnico requerido",
            audio=generate_test_audio("Explica este diagrama"),
            image="tests/fixtures/diagram.png"
        )
        
        result = omni_engine.process(mm_input, return_audio=True)
        
        assert len(result.detected_modalities) >= 3
        assert result.text != ""
    
    # ===== Performance Benchmarks =====
    
    def test_benchmark_audio_to_audio_latency(self, omni_engine, benchmark):
        """Benchmark latencia audio-to-audio"""
        audio = generate_test_audio("Test benchmark", duration=3.0)
        
        result = benchmark(omni_engine.audio_to_audio, audio)
        
        # Target: <500ms en GPU, <2s en CPU
        # pytest-benchmark mostrarÃ¡ stats
    
    def test_benchmark_vision_latency(self, omni_text_only, benchmark):
        """Benchmark anÃ¡lisis de imagen"""
        image = "tests/fixtures/test_image.jpg"
        
        result = benchmark(omni_text_only.describe_image, image)
    
    # ===== Integration Tests =====
    
    @pytest.mark.skipif(
        not is_gpu_available(),
        reason="GPU required for full multimodal"
    )
    def test_integration_full_pipeline(self, omni_engine):
        """Test integraciÃ³n completa con LangGraph"""
        from core.graph import SARAiGraph
        
        graph = SARAiGraph()
        compiled = graph.compile()
        
        # Input multimodal
        state = {
            "input": "Analiza esta imagen y responde en voz",
            "image_input": "tests/fixtures/test_image.jpg",
            "input_type": "multimodal"
        }
        
        # Ejecutar grafo
        result = compiled.invoke(state)
        
        assert result["response"] != ""
        assert result.get("audio_output") is not None
```

---

## ðŸ“Š KPIs de Ã‰xito

| KPI | Target | MÃ©todo |
|-----|--------|--------|
| **Audio-to-Audio Latency** | <500ms P50, <1s P99 | Benchmark end-to-end |
| **Vision Accuracy** | >80% en VQA | Dataset MMMU subset |
| **Video Understanding** | >75% en MVBench | Benchmark oficial |
| **OCR Accuracy** | >90% en texto claro | Dataset interno |
| **Multimodal Fusion** | >85% en OmniBench | Benchmark oficial |
| **RAM Usage** | â‰¤15GB con audio, â‰¤13GB sin audio | Monitor runtime |
| **Test Coverage** | >90% | pytest --cov |

---

## ðŸš€ Implementation Roadmap

### Fase 3.1: Audio-to-Audio Nativo (1 dÃ­a)
- [ ] Implementar `OmniMultimodal` con audio_to_audio()
- [ ] Voice selection (Chelsie/Ethan)
- [ ] Tests audio-to-audio (10 tests)
- [ ] Benchmark latencia <500ms

### Fase 3.2: Vision Skills (1.5 dÃ­as)
- [ ] Implementar `VisionSkills`
- [ ] OCR, object detection, scene description
- [ ] VQA (Visual Question Answering)
- [ ] Tests visiÃ³n (15 tests)

### Fase 3.3: Video Skills (1.5 dÃ­as)
- [ ] Implementar `VideoSkills`
- [ ] Resumen de video, detecciÃ³n de eventos
- [ ] TranscripciÃ³n con contexto visual
- [ ] Tests video (12 tests)

### Fase 3.4: Multimodal Fusion (1 dÃ­a)
- [ ] IntegraciÃ³n LangGraph
- [ ] Nodo `process_multimodal`
- [ ] Routing por modalidad
- [ ] Tests integraciÃ³n (10 tests)

### Fase 3.5: OptimizaciÃ³n y Docs (1 dÃ­a)
- [ ] Optimizar uso de memoria
- [ ] DocumentaciÃ³n completa
- [ ] Ejemplos de uso
- [ ] Completion report

**Total**: 6 dÃ­as  
**LOC Estimado**: ~2,500

---

## ðŸŽ¯ Use Cases Revolucionarios

### 1. Asistente MÃ©dico Visual
```python
# Usuario envÃ­a foto de una lesiÃ³n
image = "patient_skin_lesion.jpg"
audio = record_audio("Doctor, Â¿quÃ© opinas de esto?")

result = omni.multimodal_query(
    audio=audio,
    image=image,
    return_audio=True
)

# SARAi responde en voz con anÃ¡lisis visual
# "Veo una lesiÃ³n de aproximadamente 2cm de diÃ¡metro..."
```

### 2. Tutor Educativo Interactivo
```python
# Estudiante muestra diagrama y pregunta en voz
video = "student_explaining_diagram.mp4"  # Con audio

result = omni.analyze_video(video, use_audio=True)

# SARAi detecta errores en la explicaciÃ³n Y en el diagrama
```

### 3. AnÃ¡lisis de Seguridad en Tiempo Real
```python
# Video de cÃ¡mara de seguridad
video_stream = "security_cam_feed.mp4"

events = video_skills.detect_key_events(video_stream)
# [(0.5s, "Persona entra por puerta principal"),
#  (3.2s, "Movimiento sospechoso detectado"),
#  (5.1s, "Objeto dejado en esquina")]
```

### 4. Accesibilidad para Personas con Discapacidad Visual
```python
# Usuario ciego toma foto con mÃ³vil
image = camera.capture()
audio = record_audio("Â¿QuÃ© hay frente a mÃ­?")

result = omni.multimodal_query(audio=audio, image=image, return_audio=True)

# SARAi describe el entorno en voz:
# "Frente a ti hay una puerta de madera cerrada, 
#  a la izquierda un sofÃ¡ gris, y a la derecha una ventana..."
```

---

## ðŸ† Ventaja Competitiva

### vs. Otros Asistentes de Voz

| Feature | SARAi Omni-Full | GPT-4o | Gemini 1.5 Pro | Alexa/Siri |
|---------|-----------------|--------|----------------|------------|
| Audio-to-Audio Nativo | âœ… End-to-end | âŒ Pipeline | âŒ Pipeline | âš ï¸ Limitado |
| Vision + Audio SimultÃ¡neo | âœ… Fusion | âœ… | âœ… | âŒ |
| Video Understanding | âœ… Con audio-in-video | âœ… | âœ… | âŒ |
| Local/On-Premise | âœ… 100% | âŒ Cloud only | âŒ Cloud only | âŒ Cloud only |
| Emotional Prosody | âœ… (Fase 2) | âš ï¸ BÃ¡sico | âš ï¸ BÃ¡sico | âš ï¸ BÃ¡sico |
| Skills Modulares | âœ… (v2.12) | âŒ | âŒ | âš ï¸ Limited |
| **Costo** | **$0 (local)** | **$$$** | **$$$** | **Gratis (limitado)** |

---

## ðŸš¨ Riesgos y Mitigaciones

### Riesgo 1: RAM Overflow con Multimodal
**Probabilidad**: Alta  
**Impacto**: CrÃ­tico  
**MitigaciÃ³n**:
- Usar `disable_talker()` cuando no se necesita audio (-2GB)
- Lazy loading de Omni model
- Procesar imagen/video en chunks si es muy grande
- Monitoreo de RAM con lÃ­mite estricto 15GB

### Riesgo 2: Latencia alta en CPU
**Probabilidad**: Alta  
**Impacto**: Alto  
**MitigaciÃ³n**:
- Cache agresivo de anÃ¡lisis de imÃ¡genes
- Downscale de imÃ¡genes antes de procesar (resize)
- Usar Omni-3B en vez de Omni-7B (mÃ¡s rÃ¡pido)
- ONNX Q4 optimization en Fase 5

### Riesgo 3: PrecisiÃ³n baja en OCR/Objetos
**Probabilidad**: Media  
**Impacto**: Medio  
**MitigaciÃ³n**:
- Prompts optimizados con ejemplos (few-shot)
- Post-procesamiento con regex
- Fallback a bibliotecas especializadas (Tesseract para OCR)

---

## ðŸŽ“ ConclusiÃ³n

**M3.3 Omni Full Multimodal** transforma SARAi en un **asistente AGI verdadero**:

âœ… **Any-to-Any**: Texto, audio, imagen, video â†’ Texto + audio  
âœ… **End-to-End**: Sin pipelines fragmentados, todo en un modelo  
âœ… **Emotional**: Prosody emocional integrada (Fase 2)  
âœ… **Modular**: Skills especializados (vision, video, fusion)  
âœ… **Local**: 100% on-premise, privacidad garantizada  
âœ… **Escalable**: FundaciÃ³n para v2.12 MoE skills  

**Next Step**: Aprobar plan â†’ Implementar Fase 3.1 (Audio-to-Audio) maÃ±ana.

---

**Preparado por**: SARAi  
**Fecha**: Oct 28, 2025  
**VersiÃ³n**: 1.0 - Master Plan  
**Status**: Ready for Revolutionary Implementation ðŸš€ðŸŒŸ
