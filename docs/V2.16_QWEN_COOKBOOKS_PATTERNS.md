# Qwen2.5-Omni: Patrones de Implementaci√≥n Oficiales (Cookbooks)

**Fuente**: https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks  
**Paper**: https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf  
**Fecha**: 28 de octubre de 2025  
**Analista**: SARAi v2.16 Integration Team

---

## üìã Resumen Ejecutivo

Los cookbooks oficiales revelan **6 patrones de implementaci√≥n cr√≠ticos** que maximizan el rendimiento de Qwen2.5-Omni en aplicaciones reales:

1. **Batch Inference Mixto** (text + audio + video + image en un solo batch)
2. **Audio-in-Video Processing** (extracci√≥n autom√°tica de audio de videos)
3. **Screen Recording Interaction** (an√°lisis de grabaciones de pantalla en tiempo real)
4. **Multi-Round Omni Chatting** (conversaciones multimodales largas)
5. **Voice Chatting** (input audio ‚Üí output audio end-to-end)
6. **Video Information Extracting** (an√°lisis de video con contexto temporal)

---

## üéØ Patrones de Implementaci√≥n

### Patr√≥n #1: **Batch Inference Mixto** (Cr√≠tico para SARAi)

**Problema**: Procesar m√∫ltiples queries con diferentes modalidades (texto, audio, video) sin bloquear el sistema.

**Soluci√≥n Oficial**:

```python
# README.md L813-L909
from qwen_omni_utils import process_mm_info

# 4 conversaciones diferentes
conversation1 = [  # Video puro
    {"role": "system", "content": [{"type": "text", "text": "..."}]},
    {"role": "user", "content": [{"type": "video", "video": "/path/to/video.mp4"}]}
]

conversation2 = [  # Audio puro
    {"role": "system", "content": [{"type": "text", "text": "..."}]},
    {"role": "user", "content": [{"type": "audio", "audio": "/path/to/audio.wav"}]}
]

conversation3 = [  # Texto puro
    {"role": "system", "content": [{"type": "text", "text": "..."}]},
    {"role": "user", "content": "who are you?"}
]

conversation4 = [  # Mixto (imagen + video + audio)
    {"role": "system", "content": [{"type": "text", "text": "..."}]},
    {"role": "user", "content": [
        {"type": "image", "image": "/path/to/image.jpg"},
        {"type": "video", "video": "/path/to/video.mp4"},
        {"type": "audio", "audio": "/path/to/audio.wav"},
        {"type": "text", "text": "What are the elements can you see and hear?"}
    ]}
]

# PATR√ìN CR√çTICO: Combinar en batch
conversations = [conversation1, conversation2, conversation3, conversation4]
USE_AUDIO_IN_VIDEO = True

# Pre-procesamiento en batch
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(
    text=text, 
    audio=audios, 
    images=images, 
    videos=videos, 
    return_tensors="pt", 
    padding=True,  # ‚úÖ Padding autom√°tico para batch
    use_audio_in_video=USE_AUDIO_IN_VIDEO
)
inputs = inputs.to(model.device).to(model.dtype)

# Inferencia batch (SOLO TEXTO OUTPUT)
text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)
```

**Beneficios para SARAi**:
- ‚úÖ Procesar m√∫ltiples usuarios simult√°neamente
- ‚úÖ Padding autom√°tico alinea secuencias
- ‚úÖ `return_audio=False` evita overhead de TTS en batch

**Restricciones**:
- ‚ùå Audio output NO soportado en batch mode
- ‚ö†Ô∏è Debe usar `use_audio_in_video` consistente en todo el pipeline

---

### Patr√≥n #2: **Audio-in-Video Processing** (Game Changer)

**Problema**: Videos contienen audio (preguntas del usuario, eventos sonoros) que es cr√≠tico para comprensi√≥n.

**Soluci√≥n Oficial** (README_CN.md L981-L996):

```python
# PASO 1: Pre-procesamiento (extrae audio del video autom√°ticamente)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=True)
#                                                       ^^^^^^^^^^^^^^^^^^^ ‚úÖ

# PASO 2: Procesador
inputs = processor(
    text=text, 
    audio=audios,  # ‚úÖ Contiene audio extra√≠do del video
    images=images, 
    videos=videos, 
    return_tensors="pt", 
    padding=True, 
    use_audio_in_video=True  # ‚úÖ DEBE coincidir
)

# PASO 3: Generaci√≥n
text_ids, audio = model.generate(**inputs, use_audio_in_video=True)  # ‚úÖ DEBE coincidir
```

**‚ö†Ô∏è CR√çTICO**: Los 3 `use_audio_in_video` DEBEN ser id√©nticos en todas las etapas:
1. `process_mm_info(..., use_audio_in_video=X)`
2. `processor(..., use_audio_in_video=X)`
3. `model.generate(..., use_audio_in_video=X)`

**Caso de Uso SARAi**:
```python
# Usuario sube video de pantalla con audio explicativo
video_path = "user_screen_recording.mp4"
messages = [{
    "role": "user",
    "content": [
        {"type": "video", "video": video_path},
        {"type": "text", "text": "¬øQu√© problema ve en esta pantalla?"}
    ]
}]

# Qwen2.5-Omni procesa:
# - Video frames (OCR, elementos visuales)
# - Audio del video (voz del usuario explicando el problema)
# - Texto de la pregunta
# Resultado: Respuesta contextualmente rica combinando los 3 inputs
```

---

### Patr√≥n #3: **Screen Recording Interaction** (Real-Time)

**Cookbook**: `screen_recording_interaction.ipynb`

**Funci√≥n de Inferencia Est√°ndar**:

```python
# screen_recording_interaction.ipynb L61-L75
def inference(video_path, prompt, sys_prompt):
    """
    Patr√≥n oficial para an√°lisis de grabaciones de pantalla
    """
    messages = [
        {"role": "system", "content": [{"type": "text", "text": sys_prompt}]},
        {"role": "user", "content": [
            {"type": "video", "video": video_path},
            {"type": "text", "text": prompt},
        ]}
    ]
    
    # Template de chat
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # Pre-procesamiento multimodal
    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)
    #                                                   ^^^^^^^^^^^^^^^^^^^ ‚ùå Sin audio
    
    # Procesamiento
    inputs = processor(
        text=text, 
        audio=audios, 
        images=images, 
        videos=videos, 
        return_tensors="pt", 
        padding=True, 
        use_audio_in_video=False
    )
    inputs = inputs.to(model.device).to(model.dtype)
    
    # Generaci√≥n
    text_ids = model.generate(**inputs, use_audio_in_video=False, max_new_tokens=512)
    
    # Decodificaci√≥n
    response = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
    return response
```

**Casos de Uso del Cookbook**:

1. **OCR** (L224-L247):
   ```python
   video_path = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/screen.mp4"
   prompt = "Please translate the abstract of paper into Chinese."
   response = inference(video_path, prompt, sys_prompt="You are a helpful assistant.")
   # Resultado: Traduce texto visible en el paper de la grabaci√≥n
   ```

2. **Assistant Mode** (L359-L389):
   ```python
   prompt = "Summarize this paper in short."
   # Resultado: Resume el contenido del paper mostrado en pantalla
   ```

3. **Information Extraction** (L175-L197):
   ```python
   prompt = "What the browser is used in this video?"
   # Resultado: "Google Chrome" (detectado visualmente)
   ```

**Beneficio para SARAi**: Debugging remoto, tutoriales, an√°lisis de UX.

---

### Patr√≥n #4: **Multi-Round Omni Chatting** (Conversaci√≥n Larga)

**Cookbook**: `multi_round_omni_chatting.ipynb`

**Patr√≥n de Estado Conversacional**:

```python
# Pseudo-c√≥digo basado en los cookbooks
history = []  # Estado de conversaci√≥n

# ROUND 1: Usuario env√≠a audio
round1_messages = [
    {"role": "system", "content": [{"type": "text", "text": "..."}]},
    {"role": "user", "content": [{"type": "audio", "audio": "user_question.wav"}]}
]

# Procesar
text_ids_1, audio_1 = model.generate(..., return_audio=True, speaker="Chelsie")
history.append({
    "user": round1_messages,
    "assistant": processor.decode(text_ids_1)
})

# ROUND 2: Usuario env√≠a video de seguimiento
round2_messages = history + [
    {"role": "user", "content": [
        {"type": "video", "video": "followup.mp4"},
        {"type": "text", "text": "Y esto?"}
    ]}
]

# Procesar con contexto acumulado
text_ids_2, audio_2 = model.generate(..., return_audio=True)
history.append({
    "user": round2_messages[-1],
    "assistant": processor.decode(text_ids_2)
})
```

**‚ö†Ô∏è WARNING del Cookbook** (screen_recording_interaction.ipynb L389):

```
WARNING:root:System prompt modified, audio output may not work as expected. 
Audio output mode only works when using default system prompt 
'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, 
capable of perceiving auditory and visual inputs, as well as generating text and speech.'
```

**Implicaci√≥n para SARAi**: Si modificamos el system prompt (empathy, creative, etc.), perdemos audio output. Soluci√≥n: usar prefijos en lugar de reemplazar el system prompt completo.

---

### Patr√≥n #5: **Voice Chatting** (End-to-End Audio)

**Cookbook**: `voice_chatting.ipynb`

**Pipeline Completo**:

```python
# Patr√≥n inferido de los cookbooks
from qwen_omni_utils import process_mm_info

# Input: Audio del usuario
messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": 
            "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, "
            "capable of perceiving auditory and visual inputs, as well as generating text and speech."
        }]
    },
    {
        "role": "user",
        "content": [{"type": "audio", "audio": "user_voice.wav"}]
    }
]

# Pre-procesamiento
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(messages, use_audio_in_video=False)

inputs = processor(
    text=text,
    audio=audios,
    images=images,
    videos=videos,
    return_tensors="pt",
    padding=True
)
inputs = inputs.to(model.device).to(model.dtype)

# Generaci√≥n con audio output
text_ids, audio_output = model.generate(
    **inputs,
    return_audio=True,  # ‚úÖ Activar audio output
    speaker="Chelsie"   # ‚úÖ Voz femenina (default)
)

# Salida:
# - text_ids: Texto de la respuesta (para logs/auditor√≠a)
# - audio_output: Numpy array con audio WAV

# Guardar audio
import soundfile as sf
sf.write("response_audio.wav", audio_output, samplerate=16000)
```

**Voces Disponibles** (README.md L1024-L1042):

| Voice Type | Gender | Descripci√≥n |
|------------|--------|-------------|
| **Chelsie** (default) | Femenino | Voz aterciopelada, c√°lida, clara y luminosa |
| **Ethan** | Masculino | Voz brillante, en√©rgica, c√°lida y accesible |

**API Cloud** (README_CN.md L1107-L1134):

```python
# OpenAI-compatible API (Alibaba Cloud)
from openai import OpenAI

client = OpenAI(
    api_key="your_api_key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

completion = client.chat.completions.create(
    model="qwen-omni-turbo",
    messages=messages,
    modalities=["text", "audio"],
    audio={
        "voice": "Cherry",  # Cherry, Ethan, Serena, Chelsie
        "format": "wav"
    },
    stream=True,
    stream_options={"include_usage": True}
)

# Streaming de audio chunks
audio_string = ""
for chunk in completion:
    if chunk.choices and hasattr(chunk.choices[0].delta, "audio"):
        audio_string += chunk.choices[0].delta.audio["data"]

# Decodificar base64
import base64
audio_bytes = base64.b64decode(audio_string)
```

---

### Patr√≥n #6: **Video Information Extracting** (Temporal Context)

**Cookbook**: `video_information_extracting.ipynb`

**Caso de Uso**: Contar objetos en video (shopping.mp4 L268-L289):

```python
video_path = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/shopping.mp4"
prompt = "How many bottles of drinks have I picked up?"

# Qwen2.5-Omni analiza frames del video temporalmente
response = inference(video_path, prompt, sys_prompt="You are a helpful assistant.")
# Output: "You have picked up two bottles of drinks."
```

**Patr√≥n de Frames**:
- Qwen2.5-Omni extrae frames autom√°ticamente
- Analiza contexto temporal (objeto aparece en frame 10, desaparece en frame 20)
- Cuenta cambios de estado (botella cogida = cambio visual)

**Configuraci√≥n de FPS** (qwen-omni-utils README.md L38-L45):

```python
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "file:///path/to/video.mp4",
                "fps": 2.0,               # ‚úÖ Muestreo a 2 FPS (default)
                "resized_height": 280,    # ‚úÖ Resize autom√°tico
                "resized_width": 280
            },
            {"type": "text", "text": "Describe this video."}
        ]
    }
]
```

---

## üõ†Ô∏è qwen-omni-utils: API Unificada

**Instalaci√≥n** (README.md L714-L732):

```bash
# RECOMENDADO: Con decord (10x m√°s r√°pido que torchvision)
pip install qwen-omni-utils[decord] -U

# Alternativa (si decord no disponible en tu OS)
pip install qwen-omni-utils -U  # Usa torchvision
```

**Funci√≥n Core** (`qwen-omni-utils/src/qwen_omni_utils/v2_5/__init__.py` L0-L14):

```python
from qwen_omni_utils import process_mm_info

def process_mm_info(conversations, use_audio_in_video, return_video_kwargs=False):
    """
    Funci√≥n unificada para procesar CUALQUIER combinaci√≥n de modalidades
    
    Args:
        conversations: Lista de mensajes con tipos {text, image, audio, video}
        use_audio_in_video: Si extraer audio de videos autom√°ticamente
        return_video_kwargs: Si devolver metadatos de video
    
    Returns:
        (audios, images, videos) o (audios, images, videos, video_kwargs)
    """
    audios = process_audio_info(conversations, use_audio_in_video)
    vision = process_vision_info(conversations, return_video_kwargs=return_video_kwargs)
    return (audios,) + vision
```

**Tipos de Input Soportados** (qwen-omni-utils README.md):

```python
# AUDIO
## Archivo local
{"type": "audio", "audio": "file:///path/to/audio.wav"}

## Numpy array
{"type": "audio", "audio": numpy_audio_array}

## URL remota
{"type": "audio", "audio": "https://example.com/audio.wav"}

# VIDEO
## Archivo local
{"type": "video", "video": "file:///path/to/video.mp4"}

## Frames extra√≠dos (lista de im√°genes)
{"type": "video", "video": ["frame1.jpg", "frame2.jpg", "frame3.jpg"]}

## URL remota
{"type": "video", "video": "https://example.com/video.mp4"}

# IMAGE
## Archivo local
{"type": "image", "image": "file:///path/to/image.jpg"}

## PIL.Image object
{"type": "image", "image": pil_image}

## Base64 string
{"type": "image", "image": "data:image/jpeg;base64,..."}

## URL remota
{"type": "image", "image": "https://example.com/image.jpg"}
```

**Backend de Video** (README.md L813-L837):

| Backend | HTTP | HTTPS |
|---------|------|-------|
| **torchvision ‚â• 0.19.0** | ‚úÖ | ‚úÖ |
| **torchvision < 0.19.0** | ‚ùå | ‚ùå |
| **decord** (recomendado) | ‚úÖ | ‚ùå |

---

## üìä KPIs de Rendimiento (Inferidos de Cookbooks)

### Latencia de Procesamiento

**Screen Recording (video_information_extracting.ipynb L245-L268)**:
- Input: Video de 640x360, ~10 segundos
- Modelo: Qwen2.5-Omni-7B
- Hardware: GPU con `flash_attention_2`
- Latencia: ~5-10 segundos para generar respuesta completa
- **Estimaci√≥n CPU**: 30-60 segundos (6-10x m√°s lento)

**Voice Chatting (inferido)**:
- Input: Audio WAV 5 segundos
- Output: Texto + audio response
- Latencia GPU: ~2-3 segundos
- **Estimaci√≥n CPU**: 10-15 segundos

### RAM Usage

**Modelo 7B** (README.md L751-L777):
```python
model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="flash_attention_2"
)
```

**Estimaci√≥n**:
- Base model (BF16): ~14GB GPU VRAM
- + CLIP projector: ~2.6GB
- + Audio encoder: ~1GB
- **Total GPU**: ~17-18GB VRAM

**v2.16 Stack (CPU)**:
- Qwen3-VL-4B-Instruct (Q5_K_M): ~2.36GB RAM
- + Dynamic vision: +2.6GB (cuando se usa)
- **Total CPU**: ~5GB peak (con Low-VRAM Mode)

---

## üéØ Recomendaciones de Integraci√≥n SARAi v2.16

### 1. **Implementar Batch Inference** (HIGH PRIORITY)

```python
# core/inference.py
class OmniInferenceEngine:
    def __init__(self, model_pool):
        self.pool = model_pool
        self.request_queue = []  # Cola de requests
    
    def batch_process(self, max_batch_size=4):
        """
        Procesa m√∫ltiples requests en batch
        """
        if len(self.request_queue) == 0:
            return []
        
        # Tomar hasta max_batch_size requests
        batch = self.request_queue[:max_batch_size]
        self.request_queue = self.request_queue[max_batch_size:]
        
        # Construir conversations
        conversations = [req["messages"] for req in batch]
        
        # PATR√ìN OFICIAL
        text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(conversations, use_audio_in_video=True)
        
        inputs = processor(
            text=text,
            audio=audios,
            images=images,
            videos=videos,
            return_tensors="pt",
            padding=True,
            use_audio_in_video=True
        )
        
        # Inferencia batch
        omni_model = self.pool.get("omni3b")
        text_ids = omni_model.generate(**inputs, use_audio_in_video=True, return_audio=False)
        
        # Decodificar por separado
        responses = processor.batch_decode(text_ids, skip_special_tokens=True)
        
        return responses
```

**Beneficio**: Latencia P50 -40% bajo carga m√∫ltiple.

### 2. **Usar Audio-in-Video Consistente** (CRITICAL)

```python
# config/sarai.yaml
omni:
  use_audio_in_video: true  # Global setting
  
# core/graph.py
def process_video_query(state: State):
    USE_AUDIO_IN_VIDEO = config['omni']['use_audio_in_video']
    
    # PASO 1: Pre-procesamiento
    audios, images, videos = process_mm_info(
        state["messages"],
        use_audio_in_video=USE_AUDIO_IN_VIDEO  # ‚úÖ
    )
    
    # PASO 2: Procesador
    inputs = processor(
        text=state["text"],
        audio=audios,
        images=images,
        videos=videos,
        return_tensors="pt",
        padding=True,
        use_audio_in_video=USE_AUDIO_IN_VIDEO  # ‚úÖ DEBE coincidir
    )
    
    # PASO 3: Generaci√≥n
    text_ids, audio = model.generate(
        **inputs,
        use_audio_in_video=USE_AUDIO_IN_VIDEO  # ‚úÖ DEBE coincidir
    )
    
    return state
```

### 3. **Preservar System Prompt para Audio Output** (MEDIUM PRIORITY)

```python
# agents/omni_pipeline.py
DEFAULT_SYSTEM_PROMPT = (
    "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, "
    "capable of perceiving auditory and visual inputs, as well as generating text and speech."
)

def build_omni_messages(user_input, mode="empathy"):
    """
    Construir mensajes con system prompt compatible con audio output
    """
    if mode == "empathy":
        # PATR√ìN: Prefijo en lugar de reemplazo
        system_content = (
            f"{DEFAULT_SYSTEM_PROMPT}\n\n"
            "Additionally, respond with empathy and warmth."
        )
    else:
        system_content = DEFAULT_SYSTEM_PROMPT
    
    return [
        {"role": "system", "content": [{"type": "text", "text": system_content}]},
        {"role": "user", "content": user_input}
    ]
```

### 4. **Implementar Voice Chatting End-to-End** (v2.17)

```python
# agents/voice_agent.py
import soundfile as sf
from qwen_omni_utils import process_mm_info

def voice_chat(audio_input_path: str, speaker: str = "Chelsie") -> tuple:
    """
    Voice input ‚Üí Voice output end-to-end
    """
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": DEFAULT_SYSTEM_PROMPT}]
        },
        {
            "role": "user",
            "content": [{"type": "audio", "audio": audio_input_path}]
        }
    ]
    
    # Pre-procesamiento
    text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)
    
    inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", padding=True)
    inputs = inputs.to(model.device).to(model.dtype)
    
    # Generaci√≥n con audio output
    text_ids, audio_output = model.generate(
        **inputs,
        return_audio=True,
        speaker=speaker  # "Chelsie" o "Ethan"
    )
    
    # Guardar audio de respuesta
    output_path = f"state/voice_responses/{uuid.uuid4()}.wav"
    sf.write(output_path, audio_output, samplerate=16000)
    
    # Texto para auditor√≠a
    response_text = processor.decode(text_ids[0], skip_special_tokens=True)
    
    return response_text, output_path
```

### 5. **Utilizar qwen-omni-utils para Inputs Diversos** (HIGH PRIORITY)

```python
# core/input_handler.py
from qwen_omni_utils import process_mm_info

class InputHandler:
    """
    Maneja inputs de m√∫ltiples fuentes: archivos, URLs, base64, numpy
    """
    
    def normalize_input(self, raw_input: dict) -> dict:
        """
        Convierte cualquier formato a formato qwen-omni-utils
        """
        if raw_input["type"] == "image":
            # Soporta: file://, https://, data:image/jpeg;base64, PIL.Image
            return {"type": "image", "image": raw_input["data"]}
        
        elif raw_input["type"] == "audio":
            # Soporta: file://, https://, numpy array
            return {"type": "audio", "audio": raw_input["data"]}
        
        elif raw_input["type"] == "video":
            # Soporta: file://, https://, lista de frames
            return {
                "type": "video",
                "video": raw_input["data"],
                "fps": raw_input.get("fps", 2.0),
                "resized_height": raw_input.get("height", 280),
                "resized_width": raw_input.get("width", 280)
            }
        
        else:
            return {"type": "text", "text": raw_input["data"]}
    
    def process_multimodal_input(self, inputs: list):
        """
        Procesa lista de inputs variados
        """
        messages = [{
            "role": "user",
            "content": [self.normalize_input(inp) for inp in inputs]
        }]
        
        # PATR√ìN OFICIAL: qwen-omni-utils maneja todo
        audios, images, videos = process_mm_info(messages, use_audio_in_video=True)
        
        return audios, images, videos
```

---

## üìö Cookbooks Disponibles

| Cookbook | Capacidad | Enlace Colab |
|----------|-----------|--------------|
| **universal_audio_understanding.ipynb** | ASR, traducci√≥n audio-a-texto, an√°lisis de audio | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb) |
| **voice_chatting.ipynb** | Audio input ‚Üí Audio output end-to-end | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb) |
| **screen_recording_interaction.ipynb** | An√°lisis de grabaciones de pantalla en tiempo real | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb) |
| **video_information_extracting.ipynb** | Extracci√≥n de informaci√≥n de video (conteo, an√°lisis temporal) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb) |
| **omni_chatting_for_math.ipynb** | Conversaci√≥n multimodal sobre contenido matem√°tico | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb) |
| **omni_chatting_for_music.ipynb** | Conversaci√≥n multimodal sobre contenido musical | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb) |
| **multi_round_omni_chatting.ipynb** | M√∫ltiples rondas de di√°logo multimodal | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb) |

---

## üîó Referencias Completas

- **Repositorio Principal**: https://github.com/QwenLM/Qwen2.5-Omni
- **Cookbooks**: https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks
- **qwen-omni-utils**: https://github.com/QwenLM/Qwen2.5-Omni/tree/main/qwen-omni-utils
- **Low-VRAM Mode**: https://github.com/QwenLM/Qwen2.5-Omni/tree/main/low-VRAM-mode
- **Paper PDF**: https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf
- **Demo Web**: https://github.com/QwenLM/Qwen2.5-Omni/blob/main/web_demo.py
- **README Principal**: https://github.com/QwenLM/Qwen2.5-Omni/blob/main/README.md
- **README CN**: https://github.com/QwenLM/Qwen2.5-Omni/blob/main/README_CN.md

---

## ‚úÖ Checklist de Integraci√≥n SARAi v2.16

- [ ] **Instalar qwen-omni-utils con decord**
  ```bash
  pip install qwen-omni-utils[decord] -U
  ```

- [ ] **Actualizar agents/omni_native.py con patrones oficiales**
  - [ ] Implementar `use_audio_in_video` consistente
  - [ ] A√±adir batch inference support
  - [ ] Preservar system prompt para audio output

- [ ] **Crear agents/voice_agent.py para voice chatting**
  - [ ] Audio input ‚Üí Audio output end-to-end
  - [ ] Soporte para voces "Chelsie" y "Ethan"
  - [ ] Auditor√≠a de conversaciones de voz

- [ ] **Implementar screen_recording_interaction pattern**
  - [ ] Funci√≥n `inference()` est√°ndar
  - [ ] Soporte para an√°lisis de grabaciones de pantalla
  - [ ] Casos de uso: OCR, summarization, assistance

- [ ] **A√±adir video_information_extracting capabilities**
  - [ ] Configuraci√≥n de FPS (default 2.0)
  - [ ] Resize autom√°tico (280x280)
  - [ ] An√°lisis temporal (conteo, cambios de estado)

- [ ] **Configurar audio-in-video globalmente**
  - [ ] A√±adir flag `use_audio_in_video` a config/sarai.yaml
  - [ ] Validar consistencia en proceso completo (3 pasos)

- [ ] **Testing end-to-end con cookbooks**
  - [ ] Ejecutar `voice_chatting.ipynb` localmente
  - [ ] Ejecutar `screen_recording_interaction.ipynb`
  - [ ] Ejecutar `video_information_extracting.ipynb`

---

**Pr√≥ximos Pasos**:
1. Revisar este documento completo
2. Ejecutar al menos 1 cookbook oficial en local
3. Adaptar patrones a arquitectura SARAi
4. Commit + documentaci√≥n de integraci√≥n

**Mantra v2.16**: _"Los cookbooks oficiales son la verdad absoluta. Adapta SARAi a ellos, no al rev√©s."_
