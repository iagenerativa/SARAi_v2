# ðŸ“Š SARAi Benchmark System - Quick Reference

## ðŸŽ¯ v2.14 Baseline (2025-11-01)

### Resultados Reales

| MÃ©trica | v2.13 | v2.14 | Mejora | Estado |
|---------|-------|-------|--------|--------|
| **LOC graph.py** | 720 | **354** | **-50.8%** | ðŸŽ‰ EXCELENTE |
| **Nesting Max** | 6 | **5** | **-16.7%** | âœ… BUENO |
| **RAM Baseline** | ~3.0 GB | **2.1 GB** | **-30%** | âœ… SUPERADO |
| **Try/Except** | 8 | 5 | -37.5% | âœ… BUENO |

**Veredicto**: v2.14 **CUMPLE** todos los objetivos del Anti-Spaghetti Pattern âœ…

---

## ðŸš€ Comandos RÃ¡pidos

```bash
# Ejecutar benchmark de una versiÃ³n
make benchmark VERSION=v2.15

# Comparar dos versiones
make benchmark-compare OLD=v2.14 NEW=v2.15

# Ver historial de benchmarks
make benchmark-history

# Benchmark rÃ¡pido (solo latencia + RAM)
make benchmark-quick VERSION=v2.15
```

---

## ðŸ“‚ Archivos Clave

- **Sistema**: `tests/benchmark_suite.py` (490 LOC)
- **GuÃ­a completa**: `docs/BENCHMARKING_GUIDE.md`
- **Resultados v2.14**: `docs/BENCHMARK_v2.14_RESULTS.md`
- **Baseline JSON**: `benchmarks/results/benchmark_v2.14_*_real.json`

---

## ðŸ“ˆ KPIs Medidos

### 1. Latencia (3 categorÃ­as)
- `text_short`: 5 queries cortos (P50, P95, P99)
- `text_long`: 2 queries largos
- `rag`: 3 bÃºsquedas web

### 2. Memoria (4 mÃ©tricas)
- `base_gb`: RAM sin modelos
- `text_gb`: RAM con LFM2
- `vision_gb`: RAM con Qwen3-VL
- `p99_gb`: Pico de RAM (objetivo: â‰¤12 GB)

### 3. PrecisiÃ³n (3 tipos)
- `hard_precision`: ClasificaciÃ³n tÃ©cnica (â‰¥0.85)
- `soft_precision`: ClasificaciÃ³n emocional (â‰¥0.75)
- `skills_precision`: DetecciÃ³n de skills (â‰¥0.90)

### 4. Complejidad de CÃ³digo (3 mÃ©tricas)
- `graph_loc`: LÃ­neas de cÃ³digo (menor es mejor)
- `nesting_max`: IndentaciÃ³n mÃ¡xima (objetivo: â‰¤5)
- `try_except_count`: Bloques de manejo (menor es mejor)

### 5. Cold-Start (2 mÃ©tricas)
- `lfm2_load_time`: Tiempo de carga LFM2 (â‰¤2s)
- `solar_load_time`: Tiempo de carga SOLAR (â‰¤2s)

---

## ðŸŽ¯ Workflow por Fase

### Al Final de Cada Fase

1. **Ejecutar benchmark**:
   ```bash
   make benchmark VERSION=v2.XX
   ```

2. **Comparar con versiÃ³n anterior**:
   ```bash
   make benchmark-compare OLD=v2.XX-1 NEW=v2.XX
   ```

3. **Validar resultados**:
   - âœ… Al menos 1 mejora >5%
   - âš ï¸ Ninguna regresiÃ³n >20%

4. **Commit a git**:
   ```bash
   git add benchmarks/results/
   git commit -m "benchmark: v2.XX results"
   ```

---

## ðŸ“Š Ejemplo de Output

```
================================================================================
                         ðŸŽ‰ COMPARACIÃ“N: v2.14 â†’ v2.15                          
================================================================================

âœ… MEJORAS

  â€¢ latency_text_short_p50: 2.8s â†’ 2.3s (-17.9%)
  â€¢ memory_p99_gb: 12.1 GB â†’ 10.8 GB (-10.7%)
  â€¢ graph_loc: 720 â†’ 354 (-50.8%)

âŒ REGRESIONES

  (ninguna)

ðŸ“ˆ RESUMEN

  â€¢ Total mejoras: 3
  â€¢ Total regresiones: 0

ðŸ’š VEREDICTO: v2.15 MEJORÃ“ vs v2.14
```

---

## ðŸ” Troubleshooting

### "No such file or directory: benchmark_suite.py"

**SoluciÃ³n**: El sistema de benchmarking estÃ¡ en `tests/benchmark_suite.py`. AsegÃºrate de ejecutar desde la raÃ­z del proyecto.

### "ModuleNotFoundError: No module named 'psutil'"

**SoluciÃ³n**: Instala dependencias con `pip install psutil`.

### "Benchmark muy lento (>10 min)"

**SoluciÃ³n**: Usa `make benchmark-quick` para un benchmark rÃ¡pido (solo latencia + RAM).

---

## ðŸ“ FilosofÃ­a

> **"No optimizamos lo que no medimos.  
> Cada fase debe probar con nÃºmeros que es mejor que la anterior."**

**Principios**:
1. Benchmark al **final de cada fase**
2. Comparar **solo versiones estables** (no WIP)
3. Guardar **resultados en git** (trazabilidad)
4. Validar **antes de merge** a master

---

## ðŸŽ¯ Objetivos por VersiÃ³n

| Version | KPI Principal | Objetivo |
|---------|--------------|----------|
| v2.14 | Anti-Spaghetti | -50.8% LOC âœ… |
| v2.15 | (TBD) | (TBD) |
| v2.16 | (TBD) | (TBD) |

---

**Ãšltima actualizaciÃ³n**: 2025-11-01  
**VersiÃ³n sistema**: v2.14  
**Status**: âœ… Operacional
