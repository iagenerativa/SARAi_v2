#!/usr/bin/env python3
"""
Benchmark Comparativo: Omni-3B vs Omni-7B

Mide throughput REAL de ambos modelos con queries id√©nticas:
- Omni-3B (Fast): Target 9-12 tok/s
- Omni-7B (Quality): Baseline 4 tok/s

Valida la hip√≥tesis: 3B es 2.3x m√°s r√°pido que 7B

Uso:
    python3 tests/benchmark_omni_3b_vs_7b.py
"""

import sys
import os
import time
from statistics import mean, median
from typing import List, Dict

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from agents.omni_fast import get_omni_fast_agent
from agents.omni_native import get_omni_agent


def estimate_tokens(text: str) -> int:
    """Estimaci√≥n r√°pida de tokens (1 token ‚âà 4 chars)"""
    return len(text) // 4


def benchmark_model(agent, model_name: str, queries: List[str], iterations: int = 3) -> List[Dict]:
    """
    Benchmarking de un modelo espec√≠fico
    
    Args:
        agent: Agente (OmniFastAgent o OmniNativeAgent)
        model_name: Nombre del modelo ("Omni-3B" o "Omni-7B")
        queries: Lista de queries
        iterations: Iteraciones por query
    
    Returns:
        Lista de resultados por query
    """
    print(f"\n{'='*80}")
    print(f"üöÄ BENCHMARKING: {model_name}")
    print(f"{'='*80}")
    
    results = []
    
    for idx, query in enumerate(queries):
        print(f"\nüìù Query {idx + 1}/{len(queries)}: \"{query[:60]}...\"")
        
        query_results = []
        
        for iteration in range(iterations):
            start = time.perf_counter()
            response = agent.invoke(query, max_tokens=100)  # Fijo 100 tokens para comparabilidad
            end = time.perf_counter()
            
            latency = end - start
            tokens = estimate_tokens(response)
            tok_per_sec = tokens / latency if latency > 0 else 0
            
            query_results.append({
                "latency": latency,
                "tokens": tokens,
                "tok_per_sec": tok_per_sec
            })
            
            print(f"   Iter {iteration + 1}: {latency:.2f}s, {tokens} tokens, {tok_per_sec:.1f} tok/s")
        
        # Consolidar resultados
        avg_tok_per_sec = mean([r["tok_per_sec"] for r in query_results])
        avg_latency = mean([r["latency"] for r in query_results])
        avg_tokens = int(mean([r["tokens"] for r in query_results]))
        
        results.append({
            "query": query,
            "model": model_name,
            "avg_latency": avg_latency,
            "avg_tokens": avg_tokens,
            "avg_tok_per_sec": avg_tok_per_sec
        })
    
    return results


def print_comparison_table(results_3b: List[Dict], results_7b: List[Dict]):
    """Imprime tabla comparativa"""
    print("\n" + "="*100)
    print("üìä COMPARACI√ìN OMNI-3B vs OMNI-7B")
    print("="*100)
    
    print(f"\n{'Query':<40} {'Modelo':<12} {'Lat (s)':<10} {'Tokens':<8} {'Tok/s':<10} {'vs Baseline':<12}")
    print("-"*100)
    
    for r3b, r7b in zip(results_3b, results_7b):
        query_preview = r3b['query'][:37] + "..." if len(r3b['query']) > 37 else r3b['query']
        
        # Omni-7B (baseline)
        print(f"{query_preview:<40} "
              f"{'7B (base)':<12} "
              f"{r7b['avg_latency']:<10.2f} "
              f"{r7b['avg_tokens']:<8} "
              f"{r7b['avg_tok_per_sec']:<10.1f} "
              f"{'---':<12}")
        
        # Omni-3B (fast)
        speedup = (r7b['avg_latency'] / r3b['avg_latency']) if r3b['avg_latency'] > 0 else 0
        improvement = ((r3b['avg_tok_per_sec'] - r7b['avg_tok_per_sec']) / r7b['avg_tok_per_sec'] * 100) if r7b['avg_tok_per_sec'] > 0 else 0
        
        print(f"{'':<40} "
              f"{'3B (fast)':<12} "
              f"{r3b['avg_latency']:<10.2f} "
              f"{r3b['avg_tokens']:<8} "
              f"{r3b['avg_tok_per_sec']:<10.1f} "
              f"{improvement:+.0f}% ({speedup:.1f}x)")
        
        print()
    
    # M√©tricas globales
    avg_3b = mean([r["avg_tok_per_sec"] for r in results_3b])
    avg_7b = mean([r["avg_tok_per_sec"] for r in results_7b])
    
    global_speedup = avg_3b / avg_7b if avg_7b > 0 else 0
    global_improvement = ((avg_3b - avg_7b) / avg_7b * 100) if avg_7b > 0 else 0
    
    print("="*100)
    print("üéØ M√âTRICAS GLOBALES")
    print("="*100)
    
    print(f"\nOmni-7B (Baseline):")
    print(f"  ‚Ä¢ Throughput promedio: {avg_7b:.1f} tok/s")
    print(f"  ‚Ä¢ Latencia promedio:   {mean([r['avg_latency'] for r in results_7b]):.1f} s")
    
    print(f"\nOmni-3B (Fast):")
    print(f"  ‚Ä¢ Throughput promedio: {avg_3b:.1f} tok/s")
    print(f"  ‚Ä¢ Latencia promedio:   {mean([r['avg_latency'] for r in results_3b]):.1f} s")
    
    print(f"\nüöÄ MEJORA GLOBAL:")
    print(f"  ‚Ä¢ Speedup:     {global_speedup:.2f}x m√°s r√°pido")
    print(f"  ‚Ä¢ Improvement: {global_improvement:+.1f}% throughput")
    
    # Validaci√≥n KPI 3B
    print(f"\nüéØ VALIDACI√ìN KPI OMNI-3B:")
    target_min = 7  # M√≠nimo del rango 7-15 tok/s
    target_max = 15
    
    if avg_3b >= target_min and avg_3b <= target_max:
        status = "‚úÖ CUMPLIDO"
    elif avg_3b > target_max:
        status = f"‚≠ê SUPERADO ({avg_3b - target_max:.1f} tok/s extra)"
    else:
        status = f"‚ö†Ô∏è  BAJO TARGET ({target_min - avg_3b:.1f} tok/s faltantes)"
    
    print(f"  ‚Ä¢ Target:      {target_min}-{target_max} tok/s")
    print(f"  ‚Ä¢ Real:        {avg_3b:.1f} tok/s")
    print(f"  ‚Ä¢ {status}")
    
    # Validaci√≥n hip√≥tesis
    expected_speedup = 2.3
    speedup_diff = abs(global_speedup - expected_speedup)
    
    print(f"\nüìê VALIDACI√ìN HIP√ìTESIS (3B = 2.3x m√°s r√°pido que 7B):")
    print(f"  ‚Ä¢ Esperado:    {expected_speedup:.1f}x")
    print(f"  ‚Ä¢ Real:        {global_speedup:.2f}x")
    
    if speedup_diff < 0.5:
        print(f"  ‚Ä¢ ‚úÖ HIP√ìTESIS CONFIRMADA (diferencia {speedup_diff:.2f}x)")
    else:
        print(f"  ‚Ä¢ ‚ö†Ô∏è  DESVIACI√ìN SIGNIFICATIVA (diferencia {speedup_diff:.2f}x)")


def main():
    """Ejecuta benchmark comparativo"""
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           BENCHMARK COMPARATIVO: OMNI-3B vs OMNI-7B v2.16.1            ‚ïë
‚ïë                                                                          ‚ïë
‚ïë  Hip√≥tesis a validar:                                                    ‚ïë
‚ïë  ‚Ä¢ Omni-3B: 9-12 tok/s (2.3x m√°s r√°pido que 7B)                         ‚ïë
‚ïë  ‚Ä¢ Omni-7B: 4 tok/s (baseline)                                          ‚ïë
‚ïë                                                                          ‚ïë
‚ïë  Metodolog√≠a:                                                            ‚ïë
‚ïë  ‚Ä¢ Queries id√©nticas para ambos modelos                                 ‚ïë
‚ïë  ‚Ä¢ max_tokens=100 fijo (comparabilidad)                                 ‚ïë
‚ïë  ‚Ä¢ 3 iteraciones por query (promedio)                                   ‚ïë
‚ïë                                                                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Queries de test (mix conversacional)
    test_queries = [
        "Hola, ¬øc√≥mo est√°s hoy?",
        "Cu√©ntame un chiste corto",
        "¬øQu√© opinas del clima?",
        "Dame un consejo para estudiar mejor",
        "Expl√≠came qu√© es la inteligencia artificial"
    ]
    
    print("\nüìã Configuraci√≥n:")
    print(f"   Queries: {len(test_queries)}")
    print(f"   Iteraciones por query: 3")
    print(f"   Max tokens: 100 (fijo)")
    
    # Cargar agentes
    print("\nüöÄ Cargando agentes...")
    print("\n[1/2] Cargando Omni-3B (Fast)...")
    agent_3b = get_omni_fast_agent()
    
    print("\n[2/2] Cargando Omni-7B (Quality)...")
    agent_7b = get_omni_agent()
    
    print("\n‚úÖ Ambos agentes listos")
    
    # Benchmark Omni-3B
    results_3b = benchmark_model(agent_3b, "Omni-3B", test_queries, iterations=3)
    
    # Benchmark Omni-7B
    results_7b = benchmark_model(agent_7b, "Omni-7B", test_queries, iterations=3)
    
    # Comparaci√≥n
    print_comparison_table(results_3b, results_7b)
    
    print("\n" + "="*100)
    print("‚úÖ BENCHMARK COMPLETADO")
    print("="*100)


if __name__ == "__main__":
    main()
