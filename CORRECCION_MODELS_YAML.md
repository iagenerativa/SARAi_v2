# ğŸ”§ CorrecciÃ³n config/models.yaml - ConfiguraciÃ³n Real

**Fecha**: 1 Noviembre 2025  
**Cambios**: Ajuste a configuraciÃ³n real del sistema

---

## âŒ Problemas Detectados

### 1. Modelo SOLAR Incorrecto

**ANTES** (models.yaml v2.14 inicial):
```yaml
solar_short:
  backend: "gguf"
  model_path: "models/cache/solar/solar-10.7b-instruct-v1.0.Q4_K_M.gguf"
```

**PROBLEMA**: Este archivo NO existe en local. SOLAR se sirve desde **Ollama remoto**.

---

### 2. Prioridades Incorrectas

**ANTES**:
- SOLAR: `priority: 10` (siempre en memoria)
- LFM2: `priority: 8` (carga bajo demanda)

**PROBLEMA**: LFM2 es el modelo BASE para todo lo que no sea visiÃ³n. Debe estar SIEMPRE en memoria.

---

## âœ… Soluciones Implementadas

### 1. SOLAR â†’ Ollama Backend

**AHORA**:
```yaml
solar_short:
  name: "UNA-SOLAR-10.7B-Instruct (Short Context)"
  backend: "ollama"
  
  # Modelo servido por Ollama (definir en OLLAMA_BASE_URL)
  api_url: "${OLLAMA_BASE_URL}"  # Ejemplo: http://localhost:11434
  model_name: "${SOLAR_MODEL_NAME}"  # hf.co/fblgit/UNA-SOLAR-10.7B-Instruct-v1.0:Q5_K_M
  
  n_ctx: 512
  temperature: 0.7
  max_tokens: 512
  
  load_on_demand: true  # Servidor externo
  priority: 9
  max_memory_mb: 0  # Sin RAM local
```

**Variables de entorno** (.env):
```bash
OLLAMA_BASE_URL=http://<OLLAMA_HOST>:11434
SOLAR_MODEL_NAME=hf.co/fblgit/UNA-SOLAR-10.7B-Instruct-v1.0:Q5_K_M
```

**Beneficios**:
- âœ… Usa modelo REAL existente
- âœ… Sin RAM local (servidor remoto)
- âœ… ConfiguraciÃ³n dinÃ¡mica con variables de entorno
- âœ… n_ctx flexible (512/2048)

---

### 2. LFM2 â†’ Prioridad MÃ¡xima

**AHORA**:
```yaml
lfm2:
  name: "LiquidAI-LFM2-1.2B"
  backend: "gguf"
  model_path: "models/cache/lfm2/lfm2-1.2b.Q4_K_M.gguf"
  
  load_on_demand: false  # âœ… SIEMPRE cargado por defecto
  priority: 10  # âœ… Prioridad MÃXIMA
  max_memory_mb: 700
  
  # PolÃ­tica de coexistencia con Qwen3-VL
  allow_unload_for_vision: true  # Puede descargarse temporalmente para visiÃ³n
```

**PolÃ­tica**:
- âœ… **LFM2 siempre en memoria** (modelo base para todo)
- âœ… **EXCEPCIÃ“N**: Se descarga temporalmente si Qwen3-VL necesita RAM
- âœ… Se **recarga automÃ¡ticamente** despuÃ©s de tarea de visiÃ³n

---

### 3. Qwen3-VL â†’ Coexistencia Inteligente

**AHORA**:
```yaml
qwen3_vl:
  name: "Qwen3-VL-4B-Instruct"
  backend: "multimodal"
  
  load_on_demand: true  # Solo cuando se necesita visiÃ³n
  priority: 7  # Prioridad ALTA para tareas de visiÃ³n
  max_memory_mb: 4096
  
  # PolÃ­tica de coexistencia con LFM2
  can_evict_lfm2: true  # Puede solicitar descarga temporal de LFM2
```

**PolÃ­tica**:
- âœ… Se carga **solo para tareas de visiÃ³n**
- âœ… Puede solicitar descarga temporal de LFM2
- âœ… Se descarga automÃ¡ticamente despuÃ©s de uso
- âœ… LFM2 se recarga inmediatamente

---

## ğŸ“Š Prioridades Finales

| Modelo | Priority | RAM Local | PolÃ­tica |
|--------|----------|-----------|----------|
| **LFM2** | 10 | 700 MB | Siempre en memoria (excepto visiÃ³n temporal) |
| **SOLAR short** | 9 | 0 MB | Servidor Ollama remoto |
| **SOLAR long** | 8 | 0 MB | Servidor Ollama remoto |
| **Qwen3-VL** | 7 | 4096 MB | Solo visiÃ³n (carga bajo demanda) |
| **Qwen-Omni** | 6 | 4096 MB | Solo audio (carga bajo demanda) |

---

## ğŸ”„ Estados de Memoria

### Estado Normal (Texto)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LFM2 (700 MB)           â”‚ â† SIEMPRE activo
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SOLAR (Ollama remoto)   â”‚ â† 0 MB local
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RAM Total Usada: ~700 MB
```

### Estado VisiÃ³n (Temporal)

```
PASO 1: Necesidad de visiÃ³n detectada
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LFM2 descargado         â”‚ â† Liberado temporalmente
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Qwen3-VL (4096 MB)      â”‚ â† Cargado para tarea
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 2: Tarea de visiÃ³n completada
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LFM2 (700 MB)           â”‚ â† RECARGADO automÃ¡ticamente
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Qwen3-VL descargado     â”‚ â† Liberado
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RAM Total Usada: ~4GB (temporal) â†’ ~700MB (recarga)
```

---

## ğŸ¯ LÃ³gica de DecisiÃ³n del ModelPool

```python
# core/model_pool.py (pseudo-cÃ³digo)

class ModelPool:
    def get_model(self, name: str):
        config = load_config(name)
        
        # Caso especial: VisiÃ³n con presiÃ³n de RAM
        if name == "qwen3_vl" and self.ram_available() < 4096:
            # Descargar LFM2 temporalmente
            if "lfm2" in self.cache and config.get("can_evict_lfm2"):
                self.unload("lfm2", temporary=True)
            
            # Cargar Qwen3-VL
            model = self._load_model(name, config)
            
            # Registrar callback para recargar LFM2
            self.register_unload_callback("qwen3_vl", lambda: self.reload("lfm2"))
            
            return model
        
        # Caso normal
        return self._load_model(name, config)
```

---

## ğŸ“ Notas CrÃ­ticas Actualizadas

### 1. VerificaciÃ³n de Modelos Locales

**ANTES de usar GGUF locales, verificar que existen**:

```bash
# LFM2 (CRÃTICO - debe existir)
ls -lh models/cache/lfm2/lfm2-1.2b.Q4_K_M.gguf

# Si NO existe:
# 1. Descargar de HuggingFace
# 2. O ajustar model_path en models.yaml
```

**SOLAR NO requiere archivo local** (servidor Ollama).

---

### 2. Variables de Entorno Requeridas

```bash
# .env (VERIFICAR)
OLLAMA_BASE_URL=http://<OLLAMA_HOST>:11434
SOLAR_MODEL_NAME=hf.co/fblgit/UNA-SOLAR-10.7B-Instruct-v1.0:Q5_K_M
```

Si estas variables no estÃ¡n definidas, SOLAR fallarÃ¡ al cargar.

---

### 3. MigraciÃ³n Futura SOLAR Local

**Cuando descargues SOLAR en local**:

```yaml
# Cambiar en models.yaml:
solar_short:
  backend: "gguf"  # Era: "ollama"
  model_path: "models/cache/solar/UNA-SOLAR-10.7B.Q5_K_M.gguf"  # Era: api_url + model_name
  n_threads: 6
  use_mmap: true
  use_mlock: false
  priority: 9
  max_memory_mb: 5500  # NecesitarÃ¡ RAM local
```

**NO modificar cÃ³digo Python**, solo YAML.

---

## âœ… ValidaciÃ³n de Cambios

### Checklist

- [x] SOLAR usa backend Ollama (no GGUF local)
- [x] Variables de entorno correctas (${OLLAMA_BASE_URL}, ${SOLAR_MODEL_NAME})
- [x] LFM2 priority=10 (siempre en memoria)
- [x] Qwen3-VL priority=7 (solo visiÃ³n)
- [x] PolÃ­ticas de coexistencia documentadas
- [x] Notas de uso actualizadas
- [x] Estados de memoria documentados

---

## ğŸ”„ PrÃ³ximos Pasos

1. **Verificar LFM2 local**:
   ```bash
   ls -lh models/cache/lfm2/lfm2-1.2b.Q4_K_M.gguf
   ```

2. **Verificar Ollama accesible**:
   ```bash
  curl ${OLLAMA_BASE_URL}/api/tags
   ```

3. **Implementar lÃ³gica de coexistencia** en `core/model_pool.py`:
   - MÃ©todo `unload(model, temporary=True)`
   - Callbacks de recarga
   - PolÃ­tica `can_evict_lfm2`

4. **Tests**:
   - Test carga LFM2 al inicio
   - Test descarga temporal LFM2 cuando Qwen3-VL carga
   - Test recarga automÃ¡tica LFM2 despuÃ©s de visiÃ³n

---

**FIN CORRECCIÃ“N - models.yaml ajustado a configuraciÃ³n real**
