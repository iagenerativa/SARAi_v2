#!/usr/bin/env python3
"""
Cuantizaci√≥n INT8 para Windows con GPU Support
Optimizado para: 32GB RAM + 8GB VRAM

VELOCIDAD: ~2-3 minutos con GPU vs 5-10 min CPU
PRECISI√ìN: Id√©ntica a versi√≥n CPU
"""

import os
import sys
from pathlib import Path

# Verificar dependencias
try:
    import onnx
    from onnxruntime.quantization import quantize_dynamic, QuantType
except ImportError as e:
    print(f"‚ùå Falta dependencia: {e}")
    print("Instalar con: pip install onnx onnxruntime-gpu")
    print("(usa onnxruntime-gpu para aprovechar CUDA/DirectML)")
    sys.exit(1)


def check_gpu_support():
    """Detecta si hay GPU disponible"""
    try:
        import onnxruntime as ort
        
        # Verificar providers disponibles
        available_providers = ort.get_available_providers()
        
        print("üîç Detectando hardware...")
        print(f"   Providers disponibles: {', '.join(available_providers)}")
        
        if 'CUDAExecutionProvider' in available_providers:
            print("   ‚úÖ CUDA detectado (NVIDIA GPU)")
            return 'cuda'
        elif 'DmlExecutionProvider' in available_providers:
            print("   ‚úÖ DirectML detectado (AMD/Intel/NVIDIA GPU)")
            return 'directml'
        else:
            print("   ‚ö†Ô∏è  Solo CPU disponible")
            return 'cpu'
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error detectando GPU: {e}")
        return 'cpu'


def quantize_audio_model_windows(
    input_model_path: str,
    output_model_path: str,
    quant_type: QuantType = QuantType.QInt8
):
    """
    Cuantiza modelo ONNX a INT8 (versi√≥n Windows optimizada)
    
    Args:
        input_model_path: Ruta al modelo FP32 original (4.3GB)
        output_model_path: Ruta de salida para modelo INT8 (~1.1GB)
        quant_type: Tipo de cuantizaci√≥n (QInt8)
    """
    print("üîÑ Cuantizaci√≥n INT8 Dynamic (Windows)")
    print("=" * 70)
    print(f"Input:  {input_model_path}")
    print(f"Output: {output_model_path}")
    print(f"Tipo:   {quant_type}")
    print()
    
    # Verificar que el modelo existe
    if not os.path.exists(input_model_path):
        raise FileNotFoundError(f"Modelo no encontrado: {input_model_path}")
    
    # Verificar archivo .data asociado
    data_path = input_model_path + ".data"
    if not os.path.exists(data_path):
        print(f"‚ö†Ô∏è  Advertencia: {data_path} no encontrado")
        print(f"   El modelo podr√≠a estar self-contained")
    else:
        data_size_gb = os.path.getsize(data_path) / (1024**3)
        print(f"üìä Modelo original: {data_size_gb:.2f} GB")
    
    # Detectar GPU
    gpu_type = check_gpu_support()
    
    print(f"\nüöÄ Hardware detectado: {gpu_type.upper()}")
    print(f"   RAM disponible: 32 GB")
    if gpu_type != 'cpu':
        print(f"   VRAM disponible: 8 GB")
    print()
    
    print("üîÑ Iniciando cuantizaci√≥n...")
    print("   Tiempo estimado: 2-3 minutos con GPU, 5-10 min con CPU")
    print("   Procesando: FP32 ‚Üí INT8...")
    print()
    
    try:
        import time
        start_time = time.time()
        
        # üöÄ CUANTIZACI√ìN OPTIMIZADA PARA WINDOWS
        quantize_dynamic(
            model_input=input_model_path,
            model_output=output_model_path,
            weight_type=quant_type,
            
            # Mantener formato de datos externos (4.3GB ‚Üí 1.1GB)
            use_external_data_format=True,
            
            # Cuantizar todos los nodos compatibles
            op_types_to_quantize=None,  # None = todos
            
            # Configuraci√≥n extra para mejor performance
            extra_options={
                'EnableSubgraph': True,
                'ForceQuantizeNoInputCheck': False,
                'MatMulConstBOnly': False,
            }
        )
        
        elapsed_time = time.time() - start_time
        
        print(f"\n‚úÖ Cuantizaci√≥n completada en {elapsed_time:.1f}s ({elapsed_time/60:.1f} min)")
        
        # Verificar modelo de salida
        if os.path.exists(output_model_path):
            output_size = os.path.getsize(output_model_path)
            print(f"üìä Modelo cuantizado: {output_size:,} bytes (~{output_size/1024:.1f} KB)")
            
            output_data_path = output_model_path + ".data"
            if os.path.exists(output_data_path):
                output_data_size_gb = os.path.getsize(output_data_path) / (1024**3)
                print(f"üìä Datos cuantizados: {output_data_size_gb:.2f} GB")
                
                # Calcular reducci√≥n
                if os.path.exists(data_path):
                    reduction = (1 - output_data_size_gb / data_size_gb) * 100
                    speedup = data_size_gb / output_data_size_gb
                    print(f"üéØ Reducci√≥n de tama√±o: {reduction:.1f}%")
                    print(f"üéØ Speedup esperado: {speedup:.1f}x m√°s r√°pido")
            
            print(f"\n‚úÖ Modelo guardado en: {output_model_path}")
            return True
        else:
            print(f"‚ùå Error: modelo de salida no generado")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Error durante cuantizaci√≥n: {e}")
        import traceback
        traceback.print_exc()
        return False


def validate_quantized_model_windows(model_path: str):
    """Valida modelo cuantizado con GPU si est√° disponible"""
    print("\nüß™ Validando modelo cuantizado...")
    
    try:
        import onnxruntime as ort
        import numpy as np
        
        # Detectar mejor provider
        gpu_type = check_gpu_support()
        
        if gpu_type == 'cuda':
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            print("   Usando: CUDA (NVIDIA GPU)")
        elif gpu_type == 'directml':
            providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
            print("   Usando: DirectML (GPU gen√©rico)")
        else:
            providers = ['CPUExecutionProvider']
            print("   Usando: CPU")
        
        # Cargar modelo
        session = ort.InferenceSession(model_path, providers=providers)
        
        print("‚úÖ Modelo carga correctamente")
        
        # Verificar provider activo
        print(f"   Provider activo: {session.get_providers()[0]}")
        
        # Verificar inputs/outputs
        print("\nüìã Inputs del modelo:")
        for inp in session.get_inputs():
            print(f"   - {inp.name}: {inp.shape} ({inp.type})")
        
        print("\nüìã Outputs del modelo:")
        for out in session.get_outputs():
            print(f"   - {out.name}: {out.shape} ({out.type})")
        
        # Test de inferencia con timing
        print("\nüîÑ Test de inferencia con GPU...")
        dummy_input = np.random.randint(0, 1024, size=(1, 16, 128), dtype=np.int64)
        
        # Warmup (compilar kernels GPU)
        print("   Warmup (compilando kernels)...")
        for _ in range(3):
            _ = session.run(None, {"audio_codes": dummy_input})
        
        # Benchmark real
        print("   Benchmark (5 iteraciones)...")
        import time
        times = []
        for i in range(5):
            start = time.time()
            outputs = session.run(None, {"audio_codes": dummy_input})
            elapsed = time.time() - start
            times.append(elapsed)
            print(f"      Iteraci√≥n {i+1}: {elapsed:.3f}s")
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        
        print(f"\n‚úÖ Inferencia exitosa")
        print(f"   Output shape: {outputs[0].shape}")
        print(f"   Latencia promedio: {avg_time:.3f}s ¬± {std_time:.3f}s")
        print(f"   Throughput: {1/avg_time:.1f} infer/s")
        
        # Comparaci√≥n con objetivo
        target_latency = 2.0
        if avg_time <= target_latency:
            print(f"   üéØ Objetivo alcanzado (‚â§{target_latency}s)")
        else:
            print(f"   ‚ö†Ô∏è  Por encima del objetivo ({target_latency}s)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Validaci√≥n fall√≥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Script principal para Windows"""
    # Detectar si estamos en Windows
    if os.name != 'nt':
        print("‚ö†Ô∏è  Este script est√° optimizado para Windows")
        print("   En Linux, usa: quantize_onnx_int8.py")
        response = input("¬øContinuar de todas formas? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            return 1
    
    # Rutas (compatibles con Windows y Linux)
    base_dir = Path(__file__).parent.parent
    input_model = base_dir / "models" / "onnx" / "agi_audio_core.onnx"
    output_model = base_dir / "models" / "onnx" / "agi_audio_core_int8.onnx"
    
    print("üöÄ Cuantizaci√≥n INT8 - Qwen3-Omni-3B (Windows Optimizado)")
    print("=" * 70)
    print()
    print("‚öôÔ∏è  Configuraci√≥n:")
    print(f"   Modelo FP32:  {input_model}")
    print(f"   Modelo INT8:  {output_model}")
    print(f"   Sistema:      Windows 11")
    print(f"   RAM:          32 GB")
    print(f"   VRAM:         8 GB")
    print()
    print("üìä Beneficios esperados:")
    print(f"   ‚Ä¢ Tama√±o:     4.3 GB ‚Üí ~1.1 GB (-74%)")
    print(f"   ‚Ä¢ Velocidad:  ~7s ‚Üí ~2s latencia (-71%)")
    print(f"   ‚Ä¢ Precisi√≥n:  98-99% (p√©rdida <1-2%)")
    print(f"   ‚Ä¢ Tiempo:     2-3 min con GPU, 5-10 min con CPU")
    print()
    
    # Confirmar
    response = input("¬øContinuar con cuantizaci√≥n? (y/N): ")
    if response.lower() not in ['y', 'yes', 's', 'si', 's√≠']:
        print("‚ùå Cuantizaci√≥n cancelada")
        return 1
    
    print()
    
    # Cuantizar
    success = quantize_audio_model_windows(
        input_model_path=str(input_model),
        output_model_path=str(output_model),
        quant_type=QuantType.QInt8
    )
    
    if not success:
        print("\n‚ùå Cuantizaci√≥n fall√≥")
        return 1
    
    # Validar
    if not validate_quantized_model_windows(str(output_model)):
        print("\n‚ùå Validaci√≥n fall√≥")
        return 1
    
    print("\n" + "=" * 70)
    print("üèÜ CUANTIZACI√ìN EXITOSA")
    print("=" * 70)
    print()
    print("üìù Pr√≥ximos pasos:")
    print()
    print("   1. Transferir modelo a Linux:")
    print(f"      scp {output_model} noel@agi1:~/SARAi_v2/models/onnx/")
    print(f"      scp {output_model}.data noel@agi1:~/SARAi_v2/models/onnx/")
    print()
    print("   2. En Linux, actualizar config/sarai.yaml:")
    print(f"      model_path: 'models/onnx/agi_audio_core_int8.onnx'")
    print(f"      max_memory_mb: 1200")
    print()
    print("   3. Ejecutar tests en Linux:")
    print("      python3 scripts/test_onnx_pipeline.py")
    print()
    print("   4. Verificar latencia:")
    print("      Esperado: ~2s (vs 7s actual, -71%)")
    print()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
