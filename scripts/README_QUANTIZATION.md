# Scripts de Cuantizaci√≥n INT8

**Creados**: 29 Octubre 2025  
**Prop√≥sito**: Cuantizar modelo ONNX de 4.3GB ‚Üí 1.1GB

---

## ü™ü Scripts para Windows (Ejecutar primero)

### 1. `check_prerequisites_windows.bat` üîç

**Prop√≥sito**: Verificar pre-requisitos antes de cuantizar

**Verifica**:
- ‚úÖ Python 3.10+ instalado
- ‚úÖ RAM ‚â•24GB (recomendado 32GB)
- ‚úÖ Espacio disco ‚â•6GB
- ‚úÖ Modelo FP32 presente (`agi_audio_core.onnx` + `.data`)
- ‚úÖ GPU NVIDIA (opcional)
- ‚úÖ SSH a Linux (opcional)
- ‚úÖ Dependencias Python (`onnx`, `onnxruntime`)

**Uso**:
```batch
cd C:\SARAi_v2
scripts\check_prerequisites_windows.bat
```

**Output esperado**:
```
[1/7] Verificando Python... OK
[2/7] Verificando RAM... OK 32GB
[3/7] Verificando disco... OK 45GB
[4/7] Verificando modelo FP32... OK
[5/7] Verificando GPU... OK GeForce RTX 3060
[6/7] Verificando SSH... OK
[7/7] Verificando deps... OK

‚úÖ Todos los pre-requisitos cumplidos
```

**Si falla**: Resolver problemas marcados con ‚ùå antes de continuar.

---

### 2. `quantize_windows.bat` ‚ö° **SCRIPT PRINCIPAL**

**Prop√≥sito**: Cuantizar modelo autom√°ticamente

**Pasos automatizados**:
1. Verifica Python instalado
2. Instala dependencias (`onnx`, `onnxruntime-gpu`)
3. Ejecuta cuantizaci√≥n INT8
4. Valida modelo resultante
5. Muestra instrucciones para transferir a Linux

**Uso**:
```batch
cd C:\SARAi_v2
scripts\quantize_windows.bat
```

**Tiempo**:
- Con GPU NVIDIA: **2-3 minutos**
- Con CPU: **5-10 minutos**

**Output esperado**:
```
[1/3] Verificando dependencias... ‚úÖ
[2/3] Ejecutando cuantizaci√≥n INT8... ‚úÖ (2m 14s)
[3/3] Cuantizaci√≥n completada exitosamente!

Archivos generados:
  agi_audio_core_int8.onnx (8KB)
  agi_audio_core_int8.onnx.data (1.1GB)

SIGUIENTE PASO:
  scp models\onnx\agi_audio_core_int8.onnx* noel@agi1:~/SARAi_v2/models/onnx/
```

---

### 3. `quantize_onnx_int8_windows.py` üêç (Alternativa)

**Prop√≥sito**: Script Python manual (si batch falla)

**Features**:
- Detecci√≥n autom√°tica GPU (CUDA/DirectML/CPU)
- Validaci√≥n con benchmark (5 iteraciones)
- Warmup de kernels GPU (3 iteraciones)
- Estimaci√≥n tiempo y beneficios
- Instrucciones transferencia a Linux

**Uso**:
```powershell
cd C:\SARAi_v2
python scripts\quantize_onnx_int8_windows.py
```

**Confirmaci√≥n requerida**:
```
Iniciar cuantizaci√≥n? (y/n): y
```

**Output detallado**:
```
========================================
Cuantizaci√≥n INT8 - Windows (GPU)
========================================

‚úÖ GPU detectada: CUDA (GeForce RTX 3060)
‚úÖ Modelo encontrado: 4.3GB

[1/5] Cargando modelo FP32... ‚úÖ (2.1s)
[2/5] Cuantizando a INT8... ‚úÖ (2m 14s)
[3/5] Guardando modelo INT8... ‚úÖ (8.3s)
[4/5] Validando modelo... ‚úÖ Latencia: 1.87s
[5/5] Resultados:
  Tama√±o:   4.3GB ‚Üí 1.1GB (-74%)
  Latencia: 5.3s ‚Üí 1.87s (-65%)
  Speedup:  2.8x

‚úÖ CUANTIZACI√ìN EXITOSA
```

---

## üêß Scripts para Linux (Ejecutar despu√©s)

### 4. `test_onnx_pipeline.py` ‚úÖ

**Prop√≥sito**: Suite completa de tests para validar modelo INT8

**Tests incluidos**:
1. **Model Loading**: Carga modelo y warmup
2. **Model Inference**: Procesa audio sint√©tico
3. **Config Loading**: Verifica YAML v√°lido
4. **File Validation**: Archivos INT8 presentes
5. **Performance Benchmark**: M√∫ltiples duraciones de audio

**Uso**:
```bash
cd ~/SARAi_v2
python3 scripts/test_onnx_pipeline.py
```

**Tiempo**: ~1 minuto

**Output esperado**:
```
========================================
SARAi ONNX Pipeline Test Suite
========================================

Test 1: Model Loading ‚úÖ
  Carga: 8.32s (vs 44s FP32, -81%)
  Warmup: Ejecutado

Test 2: Model Inference ‚úÖ
  Primera: 1.92s (vs 20s FP32, -90%)
  Segunda: 0.0s (cache hit)
  Output: [1, 2048, 245760] ‚úÖ

Test 3: Config Loading ‚úÖ
  Config INT8 v√°lido

Test 4: File Validation ‚úÖ
  Archivos: 8KB + 1.1GB
  Reducci√≥n: -74%

Test 5: Benchmark ‚úÖ
  0.5s audio: 2.1s
  1.0s audio: 0.0s (cache)
  2.0s audio: 2.0s
  3.0s audio: 1.9s
  
  Promedio: 2.0s (vs 5.3s FP32, -62%)

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ TODOS LOS TESTS PASARON (5/5)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**Si falla alg√∫n test**: Ver `docs/QUANTIZATION_CHECKLIST.md` troubleshooting.

---

### 5. `compare_fp32_int8_quality.py` üìä

**Prop√≥sito**: Comparar calidad FP32 vs INT8

**M√©tricas calculadas**:
- **Similitud coseno**: Qu√© tan similar es el output (objetivo: >0.98)
- **MSE (Mean Squared Error)**: Error cuadr√°tico medio (objetivo: <0.01)
- **MAE (Mean Absolute Error)**: Error absoluto medio (objetivo: <0.05)
- **Speedup**: Cu√°nto m√°s r√°pido es INT8 (objetivo: >2.0x)

**Uso**:
```bash
cd ~/SARAi_v2
python3 scripts/compare_fp32_int8_quality.py
```

**Tiempo**: ~2 minutos

**Output esperado**:
```
========================================
SARAi v2.16.1 - Comparaci√≥n FP32 vs INT8
========================================

[1/5] Cargando modelos...
  ‚úÖ FP32 cargado: 42.3s
  ‚úÖ INT8 cargado: 8.1s
  üìä Speedup carga: 5.2x

[2/5] Generando 5 inputs de test... ‚úÖ

[3/5] Ejecutando inferencias...
  Sample 1/5: FP32=19.8s, INT8=1.9s (Speedup: 10.4x)
  Sample 2/5: FP32=20.1s, INT8=0.0s (Speedup: ‚àû cache)
  Sample 3/5: FP32=19.7s, INT8=1.8s (Speedup: 10.9x)
  Sample 4/5: FP32=20.0s, INT8=1.9s (Speedup: 10.5x)
  Sample 5/5: FP32=19.9s, INT8=2.0s (Speedup: 9.9x)
  
  üìä Latencia promedio:
     FP32: 19.9s (¬±0.2s)
     INT8: 1.5s (¬±0.8s)
     Speedup: 13.3x

[4/5] Calculando m√©tricas de calidad...
  Sample 1: Cosine=0.9891, MSE=0.0011, MAE=0.0203
  Sample 2: Cosine=0.9893, MSE=0.0012, MAE=0.0198
  Sample 3: Cosine=0.9889, MSE=0.0013, MAE=0.0211
  Sample 4: Cosine=0.9892, MSE=0.0011, MAE=0.0205
  Sample 5: Cosine=0.9890, MSE=0.0012, MAE=0.0207

[5/5] Evaluaci√≥n de m√©tricas...

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
RESULTADOS DE CALIDAD
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ COSINE_SIM:  0.9891 >= 0.98
‚úÖ MSE:         0.0012 <= 0.01
‚úÖ MAE:         0.0204 <= 0.05
‚úÖ SPEEDUP:     13.3x >= 2.0x
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ CALIDAD VALIDADA: INT8 cumple todos los objetivos

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
RESUMEN DE BENEFICIOS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üì¶ Tama√±o:      4.3GB ‚Üí 1.1GB (-74%)
‚ö° Speedup:     13.3x
üéØ Precisi√≥n:   98.9%
üíæ RAM ahorrada: ~3.2GB
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**Si alguna m√©trica falla**: Ver soluciones en `docs/QUANTIZATION_CHECKLIST.md`.

---

## üìã Orden de Ejecuci√≥n Recomendado

### En Windows

```batch
REM 1. Verificar pre-requisitos
scripts\check_prerequisites_windows.bat

REM 2. Cuantizar modelo
scripts\quantize_windows.bat

REM 3. Calcular checksums (para verificar en Linux)
certutil -hashfile models\onnx\agi_audio_core_int8.onnx SHA256
certutil -hashfile models\onnx\agi_audio_core_int8.onnx.data SHA256
```

### Transferir a Linux

```powershell
REM Opci√≥n 1: SCP
scp models\onnx\agi_audio_core_int8.onnx noel@agi1:~/SARAi_v2/models/onnx/
scp models\onnx\agi_audio_core_int8.onnx.data noel@agi1:~/SARAi_v2/models/onnx/

REM Opci√≥n 2: WinSCP (interfaz gr√°fica)
```

### En Linux

```bash
# 1. Verificar archivos recibidos
ls -lh ~/SARAi_v2/models/onnx/agi_audio_core_int8.*
# Debe mostrar: 8KB + 1.1GB

# 2. Verificar checksums (comparar con Windows)
sha256sum ~/SARAi_v2/models/onnx/agi_audio_core_int8.onnx
sha256sum ~/SARAi_v2/models/onnx/agi_audio_core_int8.onnx.data

# 3. Actualizar config
nano ~/SARAi_v2/config/sarai.yaml
# Cambiar model_path a agi_audio_core_int8.onnx
# Cambiar max_memory_mb a 1200

# 4. Validar con tests
cd ~/SARAi_v2
python3 scripts/test_onnx_pipeline.py

# 5. Comparar calidad
python3 scripts/compare_fp32_int8_quality.py

# 6. Si todo OK ‚Üí Producci√≥n
sudo systemctl restart sarai
```

---

## üõ†Ô∏è Dependencias

### Windows

```powershell
pip install onnx onnxruntime-gpu  # O onnxruntime-directml para AMD/Intel
```

**Dependencias instaladas autom√°ticamente** por `quantize_windows.bat`:
- `onnx` (>=1.15.0)
- `onnxruntime-gpu` (>=1.16.0) o `onnxruntime` (CPU)

### Linux

**Ya incluidas** en `requirements.txt`:
- `onnxruntime` (>=1.16.0)
- `numpy`
- `librosa`
- `soundfile`

---

## üìä M√©tricas Objetivo

| Script | M√©trica | Objetivo | Validaci√≥n |
|--------|---------|----------|------------|
| `test_onnx_pipeline.py` | Carga inicial | <10s | ‚úÖ si <15s |
| | Primera inferencia | ~2s | ‚úÖ si <2.5s |
| | Cache hit | 0s | ‚úÖ exacto |
| | Output shape | [1,2048,245760] | ‚úÖ exacto |
| `compare_fp32_int8_quality.py` | Similitud coseno | >0.98 | ‚úÖ si ‚â•0.98 |
| | MSE | <0.01 | ‚úÖ si ‚â§0.01 |
| | MAE | <0.05 | ‚úÖ si ‚â§0.05 |
| | Speedup | >2.0x | ‚úÖ si ‚â•2.0x |

---

## ‚ö†Ô∏è Troubleshooting

### Script Windows falla con "Python not found"

**Soluci√≥n**:
```powershell
# Verificar Python en PATH
python --version

# Si falla, reinstalar Python marcando "Add to PATH"
# O a√±adir manualmente:
set PATH=%PATH%;C:\Python310
```

---

### Script Windows usa CPU en vez de GPU

**Soluci√≥n**:
```powershell
# Verificar GPU funciona
nvidia-smi

# Instalar onnxruntime-gpu
pip uninstall onnxruntime onnxruntime-gpu -y
pip install onnxruntime-gpu

# Verificar providers
python -c "import onnxruntime as ort; print(ort.get_available_providers())"
# Debe incluir: CUDAExecutionProvider
```

---

### Tests Linux fallan con "Model not found"

**Soluci√≥n**:
```bash
# Verificar archivos transferidos
ls -lh models/onnx/agi_audio_core_int8.*

# Verificar config apunta a INT8
grep "model_path" config/sarai.yaml
# Debe mostrar: model_path: "models/onnx/agi_audio_core_int8.onnx"
```

---

### Comparaci√≥n calidad muestra Speedup bajo (<2x)

**Soluci√≥n**:
```bash
# Verificar warmup activado
grep "warmup: true" config/sarai.yaml

# Re-ejecutar tests (warmup se activa en primera carga)
python3 scripts/test_onnx_pipeline.py
python3 scripts/compare_fp32_int8_quality.py
```

---

## üéì Notas T√©cnicas

### ¬øPor qu√© 2 archivos (.onnx + .onnx.data)?

ONNX usa **external data format** para modelos grandes:
- `.onnx`: Metadata del modelo (graph, inputs, outputs) ~8KB
- `.onnx.data`: Pesos del modelo (par√°metros cuantizados) ~1.1GB

**Beneficio**: Mejor rendimiento en Git, f√°cil versioning de metadata.

---

### ¬øQu√© es Dynamic Quantization?

- **Pesos**: FP32 ‚Üí INT8 (cuantizados en disco al guardar)
- **Activaciones**: FP32 ‚Üí INT8 (cuantizadas en runtime durante inferencia)

**Ventaja**: No requiere dataset de calibraci√≥n (vs Static Quantization).

**Desventaja**: Overhead runtime m√≠nimo (~5%).

---

### ¬øGPU necesaria?

**NO**, pero recomendada:
- Con GPU NVIDIA: **2-3 minutos** (CUDA acceleration)
- Con CPU: **5-10 minutos** (funcional pero m√°s lento)

**Inferencia** (despu√©s de cuantizar) siempre es en **CPU** en Linux (agi1 no tiene GPU).

---

## üìö Documentaci√≥n Completa

Para gu√≠as detalladas, ver:
- `docs/EXECUTIVE_SUMMARY_INT8.md` - Resumen ejecutivo
- `docs/QUANTIZATION_CHECKLIST.md` - Checklist paso a paso
- `docs/WINDOWS_QUANTIZATION_WORKFLOW.md` - Gu√≠a completa
- `docs/INT8_FILES_INDEX.md` - √çndice de todos los archivos

---

**√öltima actualizaci√≥n**: 29 Octubre 2025 23:58  
**Versi√≥n**: v2.16.1  
**Status**: ‚úÖ Scripts validados y listos
