"""Generate training dataset for the LoRA Router (TRM vs LLM vs TraducciÃ³n).

This script creates a labelled dataset of Spanish utterances for the router
classifier. Each utterance is embedded using the BERT Embedder so the training
process can run without requiring the original text inputs afterwards.

Output file: data/router_training.npz with keys:
    â€¢ embeddings: np.ndarray[float32] shape (N, 768)
    â€¢ labels: np.ndarray[int64] shape (N,)
    â€¢ phrases: np.ndarray[str] shape (N,) (metadata / debugging)
    â€¢ label_names: list[str] length 3 (order: TRM, LLM, Traducir)
"""

from __future__ import annotations

import argparse
import random
import sys
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import numpy as np

BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data"

# Ensure repository root is importable when running as a script
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

# Import BERT embedder from the project
try:
    from core.layer1_io.bert_embedder import BERTEmbedder
except ModuleNotFoundError as exc:  # pragma: no cover - helpful message for CLI
    raise SystemExit(
        "BERT embedder not found. Verify PYTHONPATH or run from repository root"
    ) from exc

LABEL_TRM = 0
LABEL_LLM = 1
LABEL_TRANSLATE = 2
LABEL_NAMES = ["TRM", "LLM", "Traducir"]


def _expand_variants(phrases: Iterable[str]) -> List[str]:
    """Return a unique list containing the original phrases plus simple variants."""
    variants: List[str] = []
    for phrase in phrases:
        base = phrase.strip()
        if not base:
            continue
        candidates = {
            base,
            base.capitalize(),
            base.lower(),
            base.upper(),
        }
        variants.extend(candidates)
    # Preserve order while removing duplicates
    seen: set[str] = set()
    deduped: List[str] = []
    for item in variants:
        if item not in seen:
            seen.add(item)
            deduped.append(item)
    return deduped


def _build_phrase_bank() -> Dict[int, List[str]]:
    """Create labelled phrase lists for each router class."""
    trm_base = [
        "hola",
        "hola sara",
        "hola asistente",
        "buenos dÃ­as",
        "buenas tardes",
        "buenas noches",
        "cÃ³mo estÃ¡s",
        "quÃ© tal",
        "gracias",
        "muchas gracias",
        "de nada",
        "perdona",
        "lo siento",
        "dame un momento",
        "sÃ­",
        "no",
        "tal vez",
        "claro",
        "por supuesto",
        "quiÃ©n eres",
        "quÃ© puedes hacer",
        "cuÃ©ntame algo",
        "despÃ­dete",
        "adiÃ³s",
        "hasta luego",
        "nos vemos",
        "me ayudas",
        "ayuda",
        "quÃ© hora es",
        "quÃ© dÃ­a es hoy",
        "enciende la luz",
        "apaga la luz",
        "sube el volumen",
        "baja el volumen",
        "abre la puerta",
        "cierra la puerta",
        "pon una alarma",
        "quÃ© tiempo hace",
        "quÃ© temperatura hace",
        "muÃ©strame el calendario",
        "recuÃ©rdame una cita",
        "elige una canciÃ³n",
        "pon mÃºsica relajante",
        "pausa la mÃºsica",
        "reanuda la canciÃ³n",
        "siguiente canciÃ³n",
        "anterior canciÃ³n",
    ]

    llm_questions = [
        "explÃ­came la teorÃ­a de la relatividad",
        "dame un resumen de la segunda guerra mundial",
        "cÃ³mo funciona el aprendizaje profundo",
        "quÃ© diferencias hay entre python y javascript",
        "escribe un script en bash que haga una copia de seguridad",
        "genera un ejemplo de cÃ³digo en python para ordenar una lista",
        "cÃ³mo puedo entrenar una red neuronal",
        "quÃ© opinas de la energÃ­a nuclear",
        "cuÃ¡les son las causas del cambio climÃ¡tico",
        "quÃ© impactos tiene la inteligencia artificial en la sociedad",
        "redacta una carta formal de renuncia",
        "ayÃºdame a planificar un viaje de 5 dÃ­as por madrid",
        "explica la teorÃ­a cuÃ¡ntica en tÃ©rminos sencillos",
        "por quÃ© el cielo es azul",
        "cuÃ¡l es la diferencia entre tcp y udp",
        "quÃ© es kubernetes",
        "cÃ³mo implemento una cola de prioridad en c++",
        "dame una lista de ideas para contenido de redes sociales",
        "quÃ© estrategias existen para reducir la deuda tÃ©cnica",
        "cÃ³mo puedo mejorar la retenciÃ³n de clientes",
        "cuÃ¡les son los pasos para abrir una empresa en espaÃ±a",
        "quÃ© recomienda la guÃ­a nist para ciberseguridad",
        "analiza esta frase y dime su sentimiento",
        "quÃ© implicaciones legales tiene el uso de drones",
        "quÃ© tÃ©cnica de machine learning es mejor para clasificaciÃ³n",
        "explica el mÃ©todo cientÃ­fico",
    ]

    translate_samples = [
        "tradÃºceme hola mundo al inglÃ©s",
        "traduce 'good morning' al espaÃ±ol",
        "puedes traducir esto: je suis trÃ¨s content",
        "quÃ© significa 'guten morgen'",
        "translate this sentence into spanish",
        "could you translate the following text to english",
        "bonjour, comment Ã§a va",
        "ich mÃ¶chte wissen, was das bedeutet",
        "traductor, dime quÃ© significa 'arrivederci'",
        "help me translate 'feliz cumpleaÃ±os' to portuguese",
        "quÃ© quiere decir 'buen appetit'",
        "cÃ³mo se dice 'gracias' en japonÃ©s",
        "dime cÃ³mo traducir 'see you later'",
        "me puedes traducir esta frase al francÃ©s",
        "quÃ© significa 'buenos dÃ­as' en alemÃ¡n",
        "quiero que traduzcas este texto",
        "hello, can you translate this paragraph",
        "traduce al inglÃ©s: espero que tengas un buen dÃ­a",
        "traduce al espaÃ±ol: have a nice weekend",
        "please translate: guten tag, wie geht es dir",
        "quÃ© significa la palabra 'tschÃ¼ss'",
    ]

    return {
        LABEL_TRM: _expand_variants(trm_base),
        LABEL_LLM: _expand_variants(llm_questions),
        LABEL_TRANSLATE: _expand_variants(translate_samples),
    }


def _augment_with_templates(phrases: List[str], templates: Iterable[str]) -> List[str]:
    """Apply phrase templates by formatting each template with a phrase."""
    augmented = phrases[:]
    for template in templates:
        for phrase in phrases:
            augmented.append(template.format(phrase=phrase))
    return _expand_variants(augmented)


def build_dataset(embedder: BERTEmbedder, seed: int = 42) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    random.seed(seed)
    np.random.seed(seed)

    phrase_bank = _build_phrase_bank()

    # Simple templates to increase diversity
    trm_templates = [
        "{phrase}?",
        "oye, {phrase}",
        "disculpa, {phrase}",
        "por favor, {phrase}",
    ]
    llm_templates = [
        "necesito que me expliques {phrase}",
        "podrÃ­as analizar {phrase}?",
        "me ayudas a entender {phrase}?",
        "haz un resumen sobre {phrase}",
    ]
    translate_templates = [
        "sÃ© traductor: {phrase}",
        "quiero saber cÃ³mo se dice: {phrase}",
        "podrÃ­as decirme quÃ© significa: {phrase}",
    ]

    phrase_bank[LABEL_TRM] = _augment_with_templates(phrase_bank[LABEL_TRM], trm_templates)
    phrase_bank[LABEL_LLM] = _augment_with_templates(phrase_bank[LABEL_LLM], llm_templates)
    phrase_bank[LABEL_TRANSLATE] = _augment_with_templates(phrase_bank[LABEL_TRANSLATE], translate_templates)

    embeddings: List[np.ndarray] = []
    labels: List[int] = []
    phrases: List[str] = []

    for label, phrase_list in phrase_bank.items():
        print(f"â†’ Generando embeddings para {LABEL_NAMES[label]} ({len(phrase_list)} frases)...")
        # Shuffle for diversity before encoding
        random.shuffle(phrase_list)
        for chunk_start in range(0, len(phrase_list), 32):
            chunk = phrase_list[chunk_start:chunk_start + 32]
            chunk_embeddings = embedder.encode_batch(chunk)
            embeddings.append(chunk_embeddings.astype(np.float32))
            labels.extend([label] * len(chunk))
            phrases.extend(chunk)

    # Concatenate all embeddings
    embeddings_array = np.vstack(embeddings)
    labels_array = np.asarray(labels, dtype=np.int64)
    phrases_array = np.asarray(phrases, dtype=object)

    print("âœ… Dataset generado")
    print(f"   Total muestras: {embeddings_array.shape[0]}")
    print(f"   DimensiÃ³n embeddings: {embeddings_array.shape[1]}")

    return embeddings_array, labels_array, phrases_array.tolist()


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate training data for the LoRA Router")
    parser.add_argument(
        "--output",
        type=Path,
        default=DATA_DIR / "router_training.npz",
        help="Ruta del archivo .npz de salida",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Semilla para reproducibilidad",
    )
    args = parser.parse_args()

    output_path: Path = args.output
    output_path.parent.mkdir(parents=True, exist_ok=True)

    print("ðŸ§  Cargando BERT Embedder...")
    embedder = BERTEmbedder()

    embeddings, labels, phrases = build_dataset(embedder, seed=args.seed)

    np.savez_compressed(
        output_path,
        embeddings=embeddings,
        labels=labels,
        phrases=np.array(phrases, dtype=object),
        label_names=np.array(LABEL_NAMES, dtype=object),
    )

    print(f"\nâœ… Dataset guardado en: {output_path}")
    print("   DistribuciÃ³n de clases:")
    total = len(labels)
    for label_id, name in enumerate(LABEL_NAMES):
        count = int(np.sum(labels == label_id))
        pct = 100.0 * count / max(total, 1)
        print(f"     â€¢ {name:<9}: {count:>4} muestras ({pct:5.1f}%)")


if __name__ == "__main__":
    main()
