# docker-compose.override.yml - SARAi v2.16 "Omni-Loop Sentience"
#
# ARQUITECTURA REAL v2.16:
# - audio_onnx: Qwen2.5-7B Audio optimizado (models/onnx/qwen25_7b_audio.onnx)
# - qwen3_vl: Qwen3-VL-4B para visi√≥n multimodal
# - searxng: Motor de b√∫squeda web (v2.10 RAG)
# - omni_loop: Sistema reflexivo integrado en LangGraph
#
# Activaci√≥n:
#   1. Copiar .env.example a .env
#   2. Modelo ONNX optimizado: models/onnx/qwen25_7b_audio.onnx
#   3. Modelo Qwen3-VL: se descarga autom√°ticamente v√≠a HuggingFace
#   4. docker-compose up -d

services:
  # ==========================================================================
  # Audio Engine ONNX (Qwen2.5-7B Audio Optimizado) - v2.16.2 MODULAR
  # ==========================================================================
  # ARQUITECTURA v2.16.2:
  #   - Pipeline modular: Encoder (PyTorch) + Talker (ONNX 41MB) + Vocoder (PyTorch)
  #   - Latencia: ~100ms E2E (-30% vs monol√≠tico)
  #   - RAM: ~4.7GB (Encoder 3.5GB + Talker 41MB + Vocoder 1.2GB)
  #   - Fallback autom√°tico a agi_audio_core_int8.onnx si falla modular
  # ==========================================================================
  audio_onnx:
    image: onnxruntime/onnxruntime:latest-cpu
    container_name: sarai-audio-onnx
    
    # üîí SEGURIDAD KERNEL-LEVEL (No Negociable)
    # ========================================================================
    
    # Filesystem read-only (inmutable)
    read_only: true
    
    # Prevenci√≥n de escalada de privilegios
    security_opt:
      - no-new-privileges:true
    
    # Drop ALL Linux capabilities
    cap_drop:
      - ALL
    
    # Usuario non-root
    user: "1000:1000"
    
    # tmpfs para √°reas escribibles (RAM-only)
    tmpfs:
      - /tmp:size=256M,mode=1777
      - /app/logs:size=128M,mode=0755
      - /app/state:size=64M,mode=0755
    
    # Vol√∫menes expl√≠citos (m√≠nimos necesarios)
    volumes:
      # Modelo ONNX optimizado (read-only)
      - ./models/onnx:/app/models:ro
      
      # Config (read-only)
      - ./config:/app/config:ro
    
    # Resource limits (modular: ~4.7GB)
    deploy:
      resources:
        limits:
          memory: 5G        # Modular: Encoder 3.5GB + Talker 41MB + Vocoder 1.2GB + overhead
          cpus: '4'         # Max 4 cores (para PyTorch Encoder/Vocoder)
        reservations:
          memory: 2G        # M√≠nimo garantizado
          cpus: '1'
    
    # Acceso a hardware de audio
    devices:
      - /dev/snd:/dev/snd  # Tarjeta de sonido
    
    # Variables de entorno (v2.16.2)
    environment:
      # Pipeline modular (v2.16.2)
      - AUDIO_PIPELINE_MODE=modular
      - TALKER_MODEL_PATH=/app/models/qwen25_7b_audio.onnx  # 41MB optimizado ‚ö°
      - ENCODER_BACKEND=pytorch
      - VOCODER_BACKEND=pytorch
      
      # Fallback monol√≠tico
      - AUDIO_MODEL_PATH_FALLBACK=/app/models/agi_audio_core_int8.onnx  # 1.1GB
      
      # Configuraci√≥n com√∫n
      - AUDIO_PORT=8001
      - SAMPLE_RATE=16000  # Qwen2.5-Omni usa 16kHz
      - SARAI_HMAC_SECRET=${SARAI_HMAC_SECRET:-sarai-v2.16-audio}
      - PYTHONUNBUFFERED=1
      - ORT_NUM_THREADS=${ORT_NUM_THREADS:-4}
    
    # Puertos (solo accesible desde red interna)
    expose:
      - "8001"
    
    # Red interna (aislada)
    networks:
      - sarai_internal
    
    # Healthcheck (liveness probe)
    healthcheck:
      test: ["CMD", "python", "-c", "import onnxruntime; print('OK')"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s  # M√°s tiempo para cargar componentes PyTorch
    
    # Restart policy
    restart: unless-stopped
  
  # ==========================================================================
  # skill_draft (Qwen3-VL-4B-Instruct Draft Engine) - v2.16 Phoenix
  # ==========================================================================
  # Qwen3-VL Vision Model (Multimodal) - v2.16
  # ==========================================================================
  qwen3_vl:
    build:
      context: .
      dockerfile: skills/qwen3_vl/Dockerfile
    container_name: sarai-qwen3-vl
    image: sarai-qwen3-vl:v2.16
    
    # üîí SEGURIDAD KERNEL-LEVEL
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "1000:1000"
    
    # tmpfs para cache y procesamiento
    tmpfs:
      - /tmp:size=512M,mode=1777
      - /app/cache:size=256M,mode=0755
    
    # Vol√∫menes (modelos read-only)
    volumes:
      - ./models:/models:ro
      - ./config:/app/config:ro
      - ./state/images:/app/state:rw  # Cache de im√°genes procesadas
    
    # Resource limits (Qwen3-VL-4B)
    deploy:
      resources:
        limits:
          memory: 6G        # 4B modelo + overhead
          cpus: '4'         # Max 4 cores
        reservations:
          memory: 4G        # M√≠nimo garantizado
          cpus: '2'
    
    # Puerto gRPC (interno)
    expose:
      - "50051"
    
    # Red interna
    networks:
      - sarai_internal
    
    # Variables de entorno
    environment:
      - GRPC_PORT=50051
      - MODEL_NAME=Qwen/Qwen2-VL-7B-Instruct
      - MAX_NEW_TOKENS=512
      - TEMPERATURE=0.7
      - MAX_IMAGE_SIZE=1024
      - DEVICE=cpu
      - N_THREADS=${QWEN3_VL_THREADS:-4}
      - SARAI_HMAC_SECRET=${SARAI_HMAC_SECRET:-sarai-v2.16-vision}
      - PYTHONUNBUFFERED=1
    
    # Healthcheck gRPC
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50051'); channel.close()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s  # Warm-up del modelo vision
    
    restart: unless-stopped
  
  # ==========================================================================
  # Image Preprocessor (OpenCV + Cache) - v2.16
  # ==========================================================================
  image_preprocessor:
    build:
      context: .
      dockerfile: skills/skill_draft/Dockerfile
    container_name: sarai-skill-draft
    image: sarai-skill-draft:v2.16
    
    # üîí SEGURIDAD KERNEL-LEVEL
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "1000:1000"
    
    # tmpfs para cache y logs
    tmpfs:
      - /tmp:size=512M,mode=1777
      - /app/cache:size=256M,mode=0755
    
    # Vol√∫menes (modelos read-only)
    volumes:
      - ./models/gguf:/models/gguf:ro
      - ./config:/app/config:ro
    
    # Resource limits (Qwen3-VL-4B-Instruct Q4)
    deploy:
      resources:
        limits:
          memory: 3G        # 2.1GB modelo Q4 + overhead
          cpus: '2'         # Max 2 cores
        reservations:
          memory: 2G        # M√≠nimo garantizado
          cpus: '1'
    
    # Puerto gRPC (interno)
    expose:
      - "50051"
    
    # Red interna
    networks:
      - sarai_internal
    
    # Variables de entorno
    environment:
      - GRPC_PORT=50051
      - MODEL_PATH=/models/gguf/qwen25-omni-3b-q4.gguf
      - MAX_NEW_TOKENS=512
      - TEMPERATURE=0.7
      - N_CTX=2048
      - N_THREADS=${SKILL_DRAFT_THREADS:-4}
      - SARAI_HMAC_SECRET=${SARAI_HMAC_SECRET:-sarai-v2.16-draft}
      - PYTHONUNBUFFERED=1
    
    # Healthcheck gRPC
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50051'); channel.close()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Warm-up del modelo
    
    restart: unless-stopped
  
  # ==========================================================================
  # Image Preprocessor (OpenCV + Cache) - v2.16
  # ==========================================================================
  image_preprocessor:
    build:
      context: .
      dockerfile: skills/skill_image/Dockerfile
    container_name: sarai-image-preprocessor
    image: sarai-image-preprocessor:v2.16
    
    # üîí SEGURIDAD KERNEL-LEVEL
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "1000:1000"
    
    # tmpfs para procesamiento temporal
    tmpfs:
      - /tmp:size=256M,mode=1777
      - /app/cache:size=128M,mode=0755
    
    # Vol√∫menes (cache persistente separado)
    volumes:
      - ./state/images:/app/state:rw  # Cache persistente de im√°genes procesadas
      - ./config:/app/config:ro
    
    # Resource limits (OpenCV ligero)
    deploy:
      resources:
        limits:
          memory: 500M      # OpenCV + procesamiento
          cpus: '2'         # Max 2 cores
        reservations:
          memory: 256M      # M√≠nimo garantizado
          cpus: '0.5'
    
    # Puerto gRPC (interno)
    expose:
      - "50052"
    
    # Red interna
    networks:
      - sarai_internal
    
    # Variables de entorno
    environment:
      - GRPC_PORT=50052
      - MAX_IMAGE_SIZE=1024
      - WEBP_QUALITY=85
      - CACHE_DIR=/app/state
      - PERCEPTUAL_HASH=true
      - SARAI_HMAC_SECRET=${SARAI_HMAC_SECRET:-sarai-v2.16-image}
      - PYTHONUNBUFFERED=1
    
    # Healthcheck gRPC
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50052'); channel.close()"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s  # Ligero, no necesita warm-up largo
    
    restart: unless-stopped
  
  # ==========================================================================
  # SearXNG (Motor de B√∫squeda Web) - v2.10
  # ==========================================================================
  searxng:
    image: searxng/searxng:latest
    container_name: sarai-searxng
    
    read_only: true
    
    # üõ°Ô∏è HARDENING v2.11 (Ajuste C)
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    
    volumes:
      - ./config/searxng:/etc/searxng:ro
      - searxng_data:/var/lib/searxng:rw
    
    tmpfs:
      - /tmp:size=256M,mode=1777
    
    # Puertos para desarrollo (comentar en producci√≥n)
    ports:
      - "8888:8080"  # Mapeo localhost:8888 ‚Üí container:8080
    
    expose:
      - "8080"
    
    networks:
      - sarai_internal
    
    environment:
      - SEARXNG_BASE_URL=http://searxng:8080/
    
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 5s
      retries: 3
    
    restart: unless-stopped
  
  # ==========================================================================
  # Home Assistant Proxy (Opcional - v2.11)
  # ==========================================================================
  # Descomenta si tienes Home Assistant corriendo localmente
  # home_assistant_proxy:
  #   image: nginx:alpine
  #   container_name: sarai-ha-proxy
  #   
  #   read_only: true
  #   
  #   volumes:
  #     - ./config/nginx/ha-proxy.conf:/etc/nginx/nginx.conf:ro
  #     - /tmp
  #   
  #   expose:
  #     - "8123"
  #   
  #   networks:
  #     - sarai_internal
  #   
  #   environment:
  #     - HOME_ASSISTANT_URL=${HOME_ASSISTANT_URL:-http://localhost:8123}
  #   
  #   restart: unless-stopped

# ============================================================================
# Redes
# ============================================================================
networks:
  sarai_internal:
    driver: bridge
    internal: false  # ÔøΩ Desarrollo: permitir acceso externo (cambiar a true en producci√≥n)
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ============================================================================
# Vol√∫menes
# ============================================================================
volumes:
  searxng_data:
    driver: local
