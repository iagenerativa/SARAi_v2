#!/usr/bin/env python3
"""
Qwen2.5-Omni Native Wrapper con LangChain
Soporte para texto, visiÃ³n y audio (multimodal)
"""

from pathlib import Path
from typing import Optional, Dict, Any
import os
import base64

try:
    from llama_cpp import Llama, llama_chat_format
    from llama_cpp.llama_chat_format import Llava15ChatHandler
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False

try:
    from langchain_community.llms import LlamaCpp
    from langchain_core.callbacks.manager import CallbackManager
    from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

# Verificar dependencias
if not LLAMA_CPP_AVAILABLE:
    print("âš ï¸  Dependencia faltante: llama-cpp-python")
    print("   pip install llama-cpp-python")
    exit(1)

if not LANGCHAIN_AVAILABLE:
    print("â„¹ï¸  LangChain no disponible (opcional)")
    print("   pip install langchain langchain-community")


class QwenOmniNative:
    """
    Wrapper nativo para Qwen3-VL-4B-Instruct con soporte completo multimodal
    
    Capacidades:
    - Texto (siempre)
    - VisiÃ³n (imÃ¡genes) vÃ­a CLIP projector
    - Audio (pendiente: requiere procesamiento separado)
    
    v2.16: Optimizado con LangChain + Low-VRAM Mode (dynamic projector loading)
    """
    
    def __init__(
        self,
        model_path: str,
        mmproj_path: Optional[str] = None,
        n_ctx: int = 4096,
        n_threads: int = 6,
        use_langchain: bool = True,
        verbose: bool = False,
        dynamic_vision: bool = True  # NEW: Dynamic projector loading
    ):
        """
        Args:
            model_path: Ruta al GGUF del modelo
            mmproj_path: Ruta al projector multimodal (CLIP para visiÃ³n)
            n_ctx: TamaÃ±o de contexto
            n_threads: Threads CPU
            use_langchain: Usar wrapper de LangChain (optimizado)
            verbose: Mostrar debug
            dynamic_vision: Cargar/descargar projector bajo demanda (ahorra ~2.6GB RAM)
        """
        self.model_path = Path(model_path)
        self.mmproj_path = Path(mmproj_path) if mmproj_path else None
        self.use_langchain = use_langchain
        self.dynamic_vision = dynamic_vision
        
        # Dynamic loading state (Low-VRAM Mode)
        self.vision_loaded = False
        self.chat_handler = None
        
        # Cargar modelo base
        print(f"ðŸ”„ Cargando {self.model_path.name}...")
        
        if use_langchain and not mmproj_path:
            # Wrapper LangChain optimizado (solo texto)
            print(f"ðŸš€ Usando LangChain optimizado")
            
            # Callback para streaming (opcional)
            callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) if verbose else None
            
            self.llm = LlamaCpp(
                model_path=str(self.model_path),
                n_ctx=n_ctx,
                n_threads=n_threads,
                n_gpu_layers=0,  # CPU-only
                n_batch=512,  # TamaÃ±o de batch optimizado
                f16_kv=True,  # Usar FP16 para cache KV (ahorra RAM)
                temperature=0.7,
                max_tokens=512,
                top_p=0.9,
                callback_manager=callback_manager,
                verbose=verbose,
                # Optimizaciones especÃ­ficas para CPU
                use_mlock=False,  # No lockear memoria (evita OOM)
                use_mmap=True,  # Memory mapping para acceso rÃ¡pido
            )
            self.chat_handler = None
            
        elif self.mmproj_path and self.mmproj_path.exists():
            # Multimodal con projector CLIP
            print(f"ðŸ”„ Cargando projector multimodal: {self.mmproj_path.name}")
            self.chat_handler = Llava15ChatHandler(
                clip_model_path=str(self.mmproj_path)
            )
            self.llm = Llama(
                model_path=str(self.model_path),
                chat_handler=self.chat_handler,
                n_ctx=n_ctx,
                n_threads=n_threads,
                logits_all=True,
                verbose=verbose
            )
        else:
            # Fallback: llama.cpp directo
            print("âš ï¸  Sin projector multimodal. Modo texto con llama.cpp directo.")
            self.llm = Llama(
                model_path=str(self.model_path),
                n_ctx=n_ctx,
                n_threads=n_threads,
                verbose=verbose
            )
            self.chat_handler = None
        
        print(f"âœ… Modelo cargado: {self.model_path.name}")
    
    def generate_text(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """
        Genera texto usando LangChain (optimizado) o llama.cpp directo
        
        Args:
            prompt: Prompt de entrada
            max_tokens: MÃ¡ximo de tokens a generar
            temperature: Temperatura de sampling
            
        Returns:
            Texto generado
        """
        # Si usamos LangChain, invocar con su API
        if isinstance(self.llm, LlamaCpp):
            return self.llm.invoke(prompt)
        
        # Fallback: llama.cpp directo (modo multimodal)
        response = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["</s>", "<|endoftext|>"]
        )
        return response['choices'][0]['text']
    
    def _load_vision_projector(self):
        """
        Carga CLIP projector solo cuando se necesita (Low-VRAM Mode)
        Inspirado en: https://github.com/QwenLM/Qwen2.5-Omni/tree/main/low-VRAM-mode
        """
        if self.vision_loaded or not self.mmproj_path:
            return
        
        if not self.mmproj_path.exists():
            print(f"âš ï¸  Projector no encontrado: {self.mmproj_path}")
            return
        
        try:
            print(f"ðŸ”„ Cargando CLIP projector (~2.6GB)...")
            self.chat_handler = Llava15ChatHandler(
                clip_model_path=str(self.mmproj_path)
            )
            self.vision_loaded = True
            print("âœ… CLIP projector cargado (visiÃ³n habilitada)")
        except Exception as e:
            print(f"âŒ Error cargando projector: {e}")
            self.vision_loaded = False
    
    def _unload_vision_projector(self):
        """
        Descarga CLIP projector tras uso (libera ~2.6GB RAM)
        PatrÃ³n Qwen oficial: self.visual.to('cpu') + torch.cuda.empty_cache()
        En CPU: del objeto + gc.collect()
        """
        if not self.vision_loaded:
            return
        
        try:
            del self.chat_handler
            self.chat_handler = None
            self.vision_loaded = False
            
            import gc
            gc.collect()
            
            print("ðŸ—‘ï¸  CLIP projector descargado (~2.6GB liberados)")
        except Exception as e:
            print(f"âš ï¸  Error descargando projector: {e}")
    
    def generate_with_image(
        self,
        prompt: str,
        image_path: str,
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """
        GeneraciÃ³n con imagen (requiere projector CLIP)
        
        Args:
            prompt: Pregunta sobre la imagen
            image_path: Ruta a la imagen
            max_tokens: Tokens mÃ¡ximos
            temperature: Temperatura de muestreo
        
        Returns:
            Respuesta del modelo
        
        Low-VRAM Mode: Carga projector â†’ procesa â†’ descarga automÃ¡ticamente
        """
        # Dynamic loading (Low-VRAM Mode)
        if self.dynamic_vision:
            self._load_vision_projector()
        
        if not self.chat_handler:
            raise RuntimeError(
                "Projector multimodal no cargado. "
                "Especifica mmproj_path al crear QwenOmniNative."
            )
        
        try:
            # Codificar imagen en base64
            with open(image_path, 'rb') as f:
                image_b64 = base64.b64encode(f.read()).decode('utf-8')
            
            # Formato de mensajes para LLaVA
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_b64}"
                            }
                        }
                    ]
                }
            ]
            
            response = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            result = response['choices'][0]['message']['content'].strip()
            
        finally:
            # Dynamic unloading (Low-VRAM Mode)
            if self.dynamic_vision:
                self._unload_vision_projector()
        
        return result
    
    def generate_with_audio(
        self,
        prompt: str,
        audio_path: str,
        max_tokens: int = 512
    ) -> str:
        """
        GeneraciÃ³n con audio (pendiente implementaciÃ³n)
        
        Qwen2.5-Omni requiere procesamiento de audio separado con
        librosa o torchaudio antes de pasar al LLM
        """
        raise NotImplementedError(
            "Procesamiento de audio requiere implementaciÃ³n con "
            "torchaudio/librosa. Roadmap v2.16 pendiente."
        )
    
    def __del__(self):
        """Limpieza de recursos"""
        if hasattr(self, 'llm'):
            del self.llm
        if hasattr(self, 'chat_handler'):
            del self.chat_handler


def find_omni_model() -> Optional[Path]:
    """
    Busca el GGUF de Qwen2.5-Omni en cache de Ollama
    
    Returns:
        Path al GGUF o None
    """
    # Ubicaciones posibles de Ollama
    ollama_paths = [
        Path("/usr/share/ollama/.ollama/models/blobs"),
        Path.home() / ".ollama" / "models" / "blobs"
    ]
    
    for ollama_models in ollama_paths:
        if not ollama_models.exists():
            continue
        
        # Buscar blobs que correspondan a Qwen-Omni (~2.4GB)
        for blob in ollama_models.glob("sha256-*"):
            size_gb = blob.stat().st_size / (1024**3)
            # El modelo principal de Qwen-Omni-3B Q5_K_M es ~2.4GB
            if 2.3 <= size_gb <= 2.5:
                return blob
    
    return None


def find_omni_projector() -> Optional[Path]:
    """
    Busca el projector CLIP de Qwen2.5-Omni en cache de Ollama
    
    Returns:
        Path al projector o None
    """
    # Ubicaciones posibles de Ollama
    ollama_paths = [
        Path("/usr/share/ollama/.ollama/models/blobs"),
        Path.home() / ".ollama" / "models" / "blobs"
    ]
    
    for ollama_models in ollama_paths:
        if not ollama_models.exists():
            continue
        
        # El projector CLIP es el blob de ~2.6GB
        for blob in ollama_models.glob("sha256-*"):
            size_gb = blob.stat().st_size / (1024**3)
            if 2.5 <= size_gb <= 2.7:
                return blob
    
    return None


if __name__ == "__main__":
    import sys
    
    # Test del wrapper
    print("=" * 60)
    print("QWEN2.5-OMNI NATIVE WRAPPER TEST")
    print("=" * 60)
    print()
    
    # Buscar modelo en modelos locales primero
    local_model = Path("models/qwen_omni/Qwen3-VL-4B-Instruct-Q5_K_M.gguf")
    
    if local_model.exists():
        model_path = local_model
        print(f"âœ… Usando modelo local: {model_path}")
    else:
        # Fallback: buscar en cache de Ollama
        model_path = find_omni_model()
        if not model_path:
            print("âŒ No se encontrÃ³ el modelo Qwen2.5-Omni")
            print("   OpciÃ³n 1: Descargar a models/qwen_omni/")
            print("   wget https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen3-VL-4B-Instruct-Q5_K_M.gguf")
            print("   OpciÃ³n 2: Usar Ollama")
            print("   ollama pull hf.co/unsloth/Qwen3-VL-4B-Instruct-GGUF:Q5_K_M")
            sys.exit(1)
    
    # Projector multimodal (opcional)
    projector_path = find_omni_projector()
    
    print(f"âœ… Modelo encontrado: {model_path}")
    if projector_path:
        print(f"âœ… Projector encontrado: {projector_path}")
    else:
        print("âš ï¸  Projector no encontrado. Solo modo texto.")
    print()
    
    # Crear wrapper con LangChain optimizado + Low-VRAM Mode
    omni = QwenOmniNative(
        model_path=str(model_path),
        mmproj_path=str(projector_path) if projector_path else None,
        n_ctx=2048,
        n_threads=6,
        use_langchain=True,  # âœ… OptimizaciÃ³n LangChain activada
        verbose=False,
        dynamic_vision=True  # âœ… Low-VRAM Mode: carga/descarga projector bajo demanda
    )
    
    # Test 1: Texto puro
    print("TEST 1: GeneraciÃ³n de texto puro")
    print("-" * 60)
    response = omni.generate_text(
        "Explica quÃ© es un modelo multimodal en 2 frases.",
        max_tokens=100
    )
    print(f"Respuesta: {response}")
    print()
    
    # Test 2: Imagen (si hay projector)
    if projector_path:
        print("TEST 2: AnÃ¡lisis de imagen")
        print("-" * 60)
        
        # Usar la imagen de prueba que creamos
        test_image = "logs/test_images/test_multimodal.png"
        if Path(test_image).exists():
            try:
                response = omni.generate_with_image(
                    "Â¿QuÃ© texto ves en esta imagen?",
                    test_image,
                    max_tokens=150
                )
                print(f"Respuesta: {response}")
            except Exception as e:
                print(f"âš ï¸  Error con imagen: {e}")
        else:
            print(f"âš ï¸  Imagen de prueba no encontrada: {test_image}")
    
    print()
    print("=" * 60)
    print("âœ… Test completado")
