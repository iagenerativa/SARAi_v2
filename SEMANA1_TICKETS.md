# Semana 1: Consolidaci√≥n N√∫cleo (MoE + Prioridades)

**Per√≠odo**: 31 octubre - 6 noviembre 2025  
**Responsable**: Equipo SARAi  
**Estado**: üü¢ EN PROGRESO

---

## üìã Tickets Granulares

### T1.1: ~~Implementar carga din√°mica de skills MoE~~ **CANCELADO - Malentendido Conceptual**
**Prioridad**: ~~CR√çTICA~~ **OBSOLETO**  
**Estimaci√≥n**: ~~8 horas~~ **0 horas (no hacer)**  
**Dependencias**: Ninguna  
**Owner**: Backend Team  
**Estado**: ‚ùå CANCELADO (31 Oct 2025)

**Raz√≥n de cancelaci√≥n**:
**MALENTENDIDO**: Interpret√© skills como 6 LLMs separados (CodeLlama, Mistral, etc).  
**REALIDAD**: Skills son **CONFIGURACIONES** (prompts especializados + par√°metros) para SOLAR/LFM2.

Ejemplo:
```yaml
skills:
  programming:
    model: "solar"  # ‚Üê Usa SOLAR existente
    temperature: 0.2  # Precisi√≥n para c√≥digo
    system_prompt: "Eres un experto en programaci√≥n..."
    domains: ["c√≥digo", "python", "debugging"]
```

**Beneficio**: Especializaci√≥n profunda (prompts expertos) SIN cargar modelos adicionales (0 GB RAM extra).

**Archivos a REVERTIR**:
- ~~`core/model_pool.py` (+150 LOC)~~ ‚Üí Eliminar `get_skill()`, mantener solo `get()`
- ~~`config/sarai.yaml` (+90 LOC)~~ ‚Üí Skills quedan como referencia, NO se cargan como LLMs
- ~~`tests/test_model_pool_skills.py` (+320 LOC)~~ ‚Üí Borrar archivo

---

### T1.2: ~~Activar `route_to_skills` en MCP runtime~~ **CANCELADO - Reemplazado por Skill Selection**
**Prioridad**: ~~CR√çTICA~~ **OBSOLETO**  
**Estimaci√≥n**: ~~6 horas~~ **0 horas (no hacer)**  
**Dependencias**: T1.1 completado  
**Owner**: Backend Team  
**Estado**: ‚ùå CANCELADO (31 Oct 2025)

**Raz√≥n de cancelaci√≥n**:
El m√©todo correcto NO es `execute_skills_moe()` (que cargaba LLMs), sino `select_skill_and_generate()` que:
1. Recibe scores TRM (ej: {"programming": 0.85})
2. Selecciona skill de mayor score
3. Carga config especializada (prompt + temperature)
4. Genera con SOLAR/LFM2 usando ese contexto

**Archivos a REVERTIR**:
- ~~`core/mcp.py` (+110 LOC)~~ ‚Üí Eliminar `execute_skills_moe()`
- ~~`tests/test_mcp_skills.py` (+300 LOC)~~ ‚Üí Borrar archivo

---

### T1.1-FINAL: Skill Configs + TRM Heads Especializados (8h)
**Prioridad**: CR√çTICA  
**Estimaci√≥n**: 8 horas  
**Dependencias**: Ninguna  
**Owner**: ML Team  
**Estado**: ‚è≥ PENDIENTE

**Objetivo**: Implementar skills como **CONFIGURACIONES** (prompts + par√°metros) que mejoran SOLAR/LFM2, NO como LLMs separados.

**Arquitectura**:
```
Input ‚Üí TRM-Router (6 heads) ‚Üí Skill Config ‚Üí SOLAR/LFM2 con prompt especializado
```

**Tareas**:
- [ ] Crear `core/skill_configs.py` con 6 configs especializados:
  - `programming`: SOLAR + temp=0.2 + prompt experto en c√≥digo
  - `diagnosis`: SOLAR + temp=0.3 + prompt an√°lisis de logs/RCA
  - `finance`: SOLAR + temp=0.4 + prompt analista financiero (CFA)
  - `creative`: LFM2 + temp=0.9 + prompt escritor creativo
  - `reasoning`: SOLAR + temp=0.5 + prompt l√≥gica formal
  - `general`: SOLAR + temp=0.7 + prompt asistente gen√©rico
- [ ] Modificar TRM-Router con 6 heads (programming, diagnosis, finance, creative, reasoning, general)
- [ ] Implementar `MCP.select_skill_and_generate()`:
  - Recibe scores TRM ‚Üí selecciona best skill
  - Carga config (prompt + temperature + modelo)
  - Genera con SOLAR/LFM2 usando contexto especializado
- [ ] Dataset sint√©tico: 10K queries clasificadas por skill
- [ ] Entrenar TRM-Router con accuracy >85%
- [ ] 12 tests (6 skills √ó 2 tests cada uno)

**Archivos afectados**:
- `core/skill_configs.py` (NUEVO, +180 LOC: diccionario SKILL_CONFIGS)
- `core/trm_classifier.py` (+30 LOC: 6 heads especializados)
- `core/mcp.py` (+60 LOC: `select_skill_and_generate()`)
- `core/graph.py` (+25 LOC: integraci√≥n LangGraph)
- `scripts/generate_skill_dataset.py` (NUEVO, +200 LOC)
- `scripts/train_trm_skills.py` (NUEVO, +150 LOC)
- `tests/test_skill_configs.py` (NUEVO, +120 LOC)

**Criterios de aceptaci√≥n**:
- ‚úÖ Input "Escribe c√≥digo Python para decorador" ‚Üí TRM: `programming=0.85`
- ‚úÖ MCP carga `SKILL_CONFIGS["programming"]` ‚Üí temp=0.2, prompt experto
- ‚úÖ SOLAR genera con contexto especializado (NO CodeLlama separado)
- ‚úÖ Accuracy TRM >85% en test set (1K queries)
- ‚úÖ Latencia <50ms para clasificaci√≥n TRM
- ‚úÖ 12/12 tests pasando
- ‚úÖ **0 GB RAM adicional** (usa SOLAR/LFM2 existentes)

**Beneficio vs Skills MoE**:
- ‚ùå Skills MoE: 6 LLMs √ó 800 MB = +4.8 GB RAM
- ‚úÖ Skill Configs: 6 prompts en YAML = +0 GB RAM
- üéØ Especializaci√≥n: Prompts expertos > LLMs gen√©ricos de 7B

---

### T1.2-FINAL: MCP Skill Selection (6h)
**Prioridad**: CR√çTICA  
**Estimaci√≥n**: 6 horas  
**Dependencias**: T1.1-FINAL completado  
**Owner**: Backend Team  
**Estado**: ‚è≥ PENDIENTE

**Objetivo**: Integrar skill selection en LangGraph con feedback loop.

**Tareas**:
- [ ] Modificar `core/graph.py` para usar `select_skill_and_generate()` en vez de routing hard/soft simple
- [ ] Nodo `classify_skill` retorna scores de 6 skills
- [ ] Nodo `select_config` carga configuraci√≥n del skill ganador
- [ ] Nodo `generate_specialized` ejecuta SOLAR/LFM2 con prompt especializado
- [ ] Feedback logger registra skill usado + confianza
- [ ] 6 tests E2E (un flow por skill)

**Archivos afectados**:
- `core/graph.py` (+40 LOC: nodos especializados)
- `core/feedback.py` (+25 LOC: log skill metadata)
- `tests/test_graph_skills.py` (NUEVO, +150 LOC: E2E tests)

**Criterios de aceptaci√≥n**:
- ‚úÖ Query "¬øC√≥mo optimizo SQL?" ‚Üí programming skill ‚Üí SOLAR temp=0.2
- ‚úÖ Query "Cuenta un cuento de dragones" ‚Üí creative skill ‚Üí LFM2 temp=0.9
- ‚úÖ Logs incluyen: `{"skill": "programming", "confidence": 0.85, "model": "solar"}`
- ‚úÖ 6/6 tests E2E pasando
- ‚úÖ Latencia total <30s (incluye clasificaci√≥n + generaci√≥n)

---

### T1.3-FINAL: MCP Continuous Learning (6h)  
**Owner**: Performance Team

**Tareas**:
- [ ] Crear wrapper `PriorityQueue` con 3 niveles (critical/normal/batch)
- [ ] Modificar `main.py` para clasificar requests entrantes
- [ ] Activar batching `llama.cpp` solo en cola "batch"
- [ ] Configurar `n_parallel` din√°mico seg√∫n carga
- [ ] Tests: latencia por nivel, no starvation de cola normal

**Archivos afectados**:
- `core/batch_prioritizer.py` (+120 LOC modificadas)
- `main.py` (+60 LOC)
- `core/model_pool.py` (+40 LOC para n_parallel din√°mico)
- `tests/test_batch_prioritizer.py` (+150 LOC)

**Criterios de aceptaci√≥n**:
- ‚úÖ Requests marcados `critical` ‚Üí P99 ‚â§ 2s
- ‚úÖ Requests normales ‚Üí P50 ‚â§ 20s
- ‚úÖ Batching se activa solo con ‚â•3 requests en cola
- ‚úÖ No starvation: cola normal procesa al menos 1 req cada 5s
- ‚úÖ 8/8 tests pasando

---

### T1.4: Implementar m√©tricas de latencia por nivel
**Prioridad**: MEDIA  
**Estimaci√≥n**: 4 horas  
**Dependencias**: T1.3 completado  
**Owner**: Observability Team

**Tareas**:
- [ ] A√±adir histogramas Prometheus por nivel de prioridad
- [ ] Endpoint `/metrics` expone `sarai_latency_seconds{priority="critical|normal|batch"}`
- [ ] Dashboard Grafana con 3 paneles (P50, P95, P99 por nivel)
- [ ] Alertas si P99 critical > 2s

**Archivos afectados**:
- `sarai/health_dashboard.py` (+50 LOC)
- `extras/grafana_god.json` (+80 LOC)
- `tests/test_metrics_priority.py` (nuevo, +60 LOC)

**Criterios de aceptaci√≥n**:
- ‚úÖ M√©tricas visibles en `/metrics`
- ‚úÖ Grafana muestra 3 series de latencia
- ‚úÖ Alerta dispara webhook si P99 critical > 2s
- ‚úÖ 3/3 tests pasando

---

### T1.5: Benchmark end-to-end Semana 1
**Prioridad**: MEDIA  
**Estimaci√≥n**: 6 horas  
**Dependencias**: T1.1, T1.2, T1.3, T1.4 completados  
**Owner**: QA Team

**Tareas**:
- [ ] Ejecutar 100 queries mixtas (50% normal, 30% batch, 20% critical)
- [ ] Validar KPIs documentados (RAM ‚â§12GB, latencias por nivel)
- [ ] Generar reporte de rendimiento con gr√°ficos
- [ ] Identificar cuellos de botella y documentar en issues

**Archivos afectados**:
- `tests/benchmark_semana1.py` (nuevo, +200 LOC)
- `docs/BENCHMARK_SEMANA1_REPORT.md` (nuevo)

**Criterios de aceptaci√≥n**:
- ‚úÖ RAM P99 ‚â§ 12GB durante todo el test
- ‚úÖ Latencia P99 critical ‚â§ 2s
- ‚úÖ Latencia P50 normal ‚â§ 20s
- ‚úÖ Skills se cargan/descargan correctamente (sin leaks)
- ‚úÖ Informe generado con recomendaciones

---

## üìä M√©tricas de seguimiento Semana 1

| M√©trica | Target | Estado |
|---------|--------|--------|
| Tests implementados | 32/32 | 0/32 ‚è≥ |
| LOC a√±adidas | ~870 | 0 ‚è≥ |
| RAM P99 | ‚â§ 12GB | - ‚è≥ |
| Latencia P99 Critical | ‚â§ 2s | - ‚è≥ |
| Latencia P50 Normal | ‚â§ 20s | - ‚è≥ |
| Skills funcionales | 6 | 0 ‚è≥ |

---

## üéØ Definition of Done (Semana 1)

- ‚úÖ Todos los tickets T1.1‚ÄìT1.5 marcados como completados
- ‚úÖ 32/32 tests pasando en CI
- ‚úÖ Benchmark ejecutado con resultados dentro de targets
- ‚úÖ Documentaci√≥n actualizada (CHANGELOG, STATUS)
- ‚úÖ Code review aprobado por al menos 1 revisor
- ‚úÖ Merge a rama `master` sin conflictos

---

**Inicio**: 31 octubre 2025  
**Revisi√≥n mid-week**: 3 noviembre 2025  
**Cierre**: 6 noviembre 2025  
**Handoff a Semana 2**: 7 noviembre 2025 (AM)
