# SARAi v2.2 - Requisitos de Python
# Sistema optimizado para CPU-only, 16GB RAM
# Backend: GGUF (llama-cpp-python) para máxima eficiencia en CPU

# Core
torch>=2.0.0
transformers>=4.35.0
accelerate>=0.25.0
safetensors>=0.4.0

# CRÍTICO: Backend de inferencia CPU
llama-cpp-python>=0.2.20  # Para modelos GGUF (10x más rápido en CPU)

# Opcional: GPU (cuando migres)
# bitsandbytes>=0.41.0  # Descomenta para cuantización 4-bit en GPU

# LangChain ecosystem
langchain>=0.1.0
langchain-core>=0.1.0
langgraph>=0.0.40

# HuggingFace
huggingface-hub>=0.19.0
sentencepiece>=0.1.99  # Para tokenizadores

# Configuración
pyyaml>=6.0
python-dotenv>=1.0.0

# Utilidades
numpy>=1.24.0
tqdm>=4.66.0
diskcache>=5.6.0  # Para web_cache persistente (v2.10)
requests>=2.31.0  # Para SearXNG (v2.10)

# Testing (opcional)
pytest>=7.4.0
pytest-cov>=4.1.0
psutil>=5.9.0  # Para monitoreo de RAM

# Nota: Archivos GGUF requeridos (descargar con huggingface-cli):
# - SOLAR-10.7B-Instruct-v1.0-Q4_K_M.gguf (~6GB)
# - LFM2-1.2B-Q4_K_M.gguf (~700MB)
# - Qwen2.5-Omni-7B-Q4_K_M.gguf (~4GB)
# - google/embeddinggemma-300m-qat-q4_0-unquantized (~150MB)

scipy>=1.11.0

# TTS Engine (v2.17 - MeloTTS Optimizado)
# Instalación: pip3 install git+https://github.com/myshell-ai/MeloTTS.git
# 
# Optimizaciones aplicadas:
# - Speed: 1.3x (30% más rápido)
# - Preload: Modelo cargado al inicio
# - Cache: Audio cacheado para respuestas cortas
# - Latencia: 0.6-0.7s (antes: 1.3-1.7s)
#
# Nota: melotts-onnx evaluado pero no usado (modelos ONNX no disponibles en HF)
# Ver docs/MELOTTS_OPTIMIZATIONS.md para detalles
# Nota: MeloTTS descarga modelos automáticamente (~200MB)
melotts>=0.1.2
