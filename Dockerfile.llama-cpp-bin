# Dockerfile.llama-cpp-bin
# ========================
# Multi-stage build para binarios llama.cpp optimizados
# Target: ghcr.io/iagenerativa/llama-cpp-bin:2.16-rc
# 
# FEATURES:
# - Multi-arch: linux/amd64 (AVX2, AVX512), linux/arm64 (ARM_NEON)
# - Binarios comprimidos con UPX (~50% reducción)
# - SHA256 firmado con GPG
# - Size total: ~18 MB

# ============================================================================
# Stage 1: Builder
# ============================================================================
FROM ubuntu:22.04 AS builder

ARG TARGETARCH
ARG LLAMA_CPP_VERSION=b4226

# Install build dependencies
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    cmake \
    wget \
    upx-ucl \
    && rm -rf /var/lib/apt/lists/*

# Clone llama.cpp
WORKDIR /build
RUN git clone https://github.com/ggerganov/llama.cpp.git && \
    cd llama.cpp && \
    git checkout ${LLAMA_CPP_VERSION}

# Build with architecture-specific optimizations
WORKDIR /build/llama.cpp
RUN if [ "$TARGETARCH" = "amd64" ]; then \
        # x86_64: Enable AVX2 + AVX512
        cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_AVX=ON \
            -DLLAMA_AVX2=ON \
            -DLLAMA_AVX512=ON \
            -DLLAMA_FMA=ON \
            -DLLAMA_NATIVE=OFF; \
    elif [ "$TARGETARCH" = "arm64" ]; then \
        # ARM: Enable NEON
        cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_ARM_NEON=ON \
            -DLLAMA_NATIVE=OFF; \
    fi && \
    cmake --build build --config Release -j$(nproc)

# Compress binaries with UPX (50% size reduction)
# Solo comprimimos llama-cli (el único binario crítico para v2.16)
RUN cd build/bin && \
    upx --best --lzma llama-cli || true

# Generate SHA256 checksum
RUN cd build/bin && \
    sha256sum llama-cli > llama-binaries.sha256

# ============================================================================
# Stage 2: Runtime (minimal image)
# ============================================================================
FROM ubuntu:22.04

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Copy binaries AND shared libraries from builder
# NOTE: Shared libraries are in build/src/ and build/ggml/src/, not build/
COPY --from=builder /build/llama.cpp/build/bin/llama-cli /usr/local/bin/
COPY --from=builder /build/llama.cpp/build/bin/llama-binaries.sha256 /usr/local/bin/
COPY --from=builder /build/llama.cpp/build/src/libllama.so /usr/local/lib/
COPY --from=builder /build/llama.cpp/build/ggml/src/libggml.so /usr/local/lib/

# Update library cache
RUN ldconfig

# Make binaries executable
RUN chmod +x /usr/local/bin/llama-cli

# Metadata
LABEL org.opencontainers.image.title="llama.cpp binaries for SARAi v2.16"
LABEL org.opencontainers.image.description="Pre-compiled llama.cpp binaries (AVX2/AVX512/ARM_NEON)"
LABEL org.opencontainers.image.version="2.16-rc"
LABEL org.opencontainers.image.authors="SARAi Dev Team"
LABEL org.opencontainers.image.source="https://github.com/iagenerativa/SARAi_v2"

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=1 \
    CMD llama-cli --version || exit 1

# Default command: show version
CMD ["llama-cli", "--version"]
