# ğŸ—ï¸ SARAi v2.17 - Arquitectura Profesional de 4 Capas

**Fecha**: 30 de octubre de 2025  
**VersiÃ³n**: 2.17.0  
**Estado**: En ImplementaciÃ³n

---

## ğŸ“‹ VisiÃ³n General

Sistema de voz conversacional profesional con gestiÃ³n de memoria, fluidez natural y orquestaciÃ³n dinÃ¡mica de recursos.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA 1: I/O AsÃ­ncrono                        â”‚
â”‚  Hilo IN (Audio â†’ Texto) | Hilo OUT (Texto â†’ Audio)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAPA 2: Memoria Conversacional (RAG)               â”‚
â”‚  EmbeddingGemma 2B + Qdrant â†’ Contexto multi-turno             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CAPA 3: GestiÃ³n de Fluidez Natural                    â”‚
â”‚  Sherpa Streaming + Fillers â†’ Engagement durante latencias     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        CAPA 4: OrquestaciÃ³n DinÃ¡mica (LoRA Scheduler)          â”‚
â”‚  LoRA Adapter â†’ OptimizaciÃ³n de recursos en tiempo real        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ CAPA 1: I/O AsÃ­ncrono

### **PropÃ³sito**
Procesamiento paralelo de entrada (audio â†’ texto) y salida (texto â†’ audio) con colas thread-safe.

### **Componentes**

#### **Hilo IN (Input Thread)**
```
Audio Raw â†’ Sherpa VAD â†’ Vosk STT â†’ BERT-es â†’ DistilBART-es â†’ LoRA Router
                                                                      â†“
                                                    [Traducir] â†’ NLLB-200
                                                    [Normal] â†’ Proyector
                                                    [TRM] â†’ Cache rÃ¡pida
                                                    [LLM] â†’ LFM2-1.2B
```

**Componentes clave**:

| Componente | FunciÃ³n | Latencia | TamaÃ±o |
|------------|---------|----------|--------|
| **Sherpa-ONNX VAD** | Detecta segmentos de voz | <50ms | ~10MB |
| **Vosk STT** | TranscripciÃ³n espaÃ±ol | 100-200ms | 38MB |
| **BERT-es** | Embeddings semÃ¡nticos | 50ms | 110MB |
| **DistilBART-es** | ComprensiÃ³n contextual | 80ms | 290MB |
| **LoRA Router** | Clasifica tipo de query | 10ms | 5MB |
| **NLLB-200** | TraducciÃ³n multiidioma | 200ms | 600MB |

#### **Hilo OUT (Output Thread)**
```
Cola de Respuestas â†’ Talker (?) â†’ Kitter TTS (Piper) â†’ Audio Out
```

**Componentes clave**:

| Componente | FunciÃ³n | Latencia | TamaÃ±o |
|------------|---------|----------|--------|
| **Talker** | Generador de tokens audio | TBD | TBD |
| **Kitter TTS** | SÃ­ntesis de voz (Piper) | 395ms | 60MB |

### **Colas Thread-Safe**

```python
from queue import Queue

audio_raw_queue = Queue(maxsize=10)       # Audio bruto del micrÃ³fono
audio_chunks_queue = Queue(maxsize=50)    # Chunks tras VAD
text_queue = Queue(maxsize=20)            # Texto transcrito
routing_decision_queue = Queue(maxsize=20)  # DecisiÃ³n del LoRA Router
response_queue = Queue(maxsize=10)        # Respuestas del LLM
audio_output_queue = Queue(maxsize=10)    # Audio final para reproducir
```

### **LoRA Router - Clases de Salida**

```python
class RouterDecision:
    TRANSLATE = "translate"  # Texto no espaÃ±ol â†’ NLLB â†’ EspaÃ±ol
    NORMAL = "normal"        # EspaÃ±ol estÃ¡ndar â†’ Proyector directo
    TRM = "trm"              # Respuesta comÃºn â†’ Cache TRM
    LLM = "llm"              # Razonamiento profundo â†’ LFM2-1.2B
```

**Criterios de decisiÃ³n**:
- **TRANSLATE**: Idioma detectado != espaÃ±ol (usando langdetect)
- **TRM**: Query en cache de respuestas frecuentes (similitud >0.95)
- **LLM**: Query compleja, requiere razonamiento
- **NORMAL**: Por defecto

---

## ğŸ§  CAPA 2: Memoria Conversacional (RAG)

### **PropÃ³sito**
Mantener contexto multi-turno y coherencia conversacional mediante recuperaciÃ³n de turnos relevantes.

### **Componentes**

| Componente | FunciÃ³n | TamaÃ±o |
|------------|---------|--------|
| **EmbeddingGemma 2B** | Embeddings de alta calidad | 2GB |
| **Qdrant** (o ChromaDB) | Base de datos vectorial | Variable |

### **Flujo de Datos**

```python
# IndexaciÃ³n (cada turno)
user_input = "Â¿CÃ³mo preparo cafÃ©?"
embedding = embedding_gemma.encode(user_input)  # (2048,)
qdrant.upsert(
    collection="conversation_history",
    points=[{
        "id": turn_id,
        "vector": embedding.tolist(),
        "payload": {
            "text": user_input,
            "role": "user",
            "timestamp": datetime.now().isoformat()
        }
    }]
)

# Retrieval (antes de generar respuesta)
query_embedding = embedding_gemma.encode(current_query)
relevant_turns = qdrant.search(
    collection="conversation_history",
    query_vector=query_embedding,
    limit=5  # Top-5 turnos relevantes
)

# Contexto extendido para LLM
context = "\n".join([turn.payload["text"] for turn in relevant_turns])
prompt = f"{context}\n\nUsuario: {current_query}\nAsistente:"
```

### **GestiÃ³n de Memoria**

```python
# Limpieza automÃ¡tica (mantener Ãºltimas 100 conversaciones)
MAX_CONVERSATIONS = 100
MAX_TURNS_PER_CONVERSATION = 20

# PolÃ­tica de eviction: FIFO (First In, First Out)
if total_turns > MAX_CONVERSATIONS * MAX_TURNS_PER_CONVERSATION:
    oldest_conversation_id = get_oldest_conversation()
    qdrant.delete(filter={"conversation_id": oldest_conversation_id})
```

---

## ğŸ™ï¸ CAPA 3: GestiÃ³n de Fluidez Natural

### **PropÃ³sito**
Mantener engagement del usuario durante latencias de procesamiento mediante fillers (coletillas) y streaming TTS.

### **Componentes**

| Componente | FunciÃ³n |
|------------|---------|
| **Latency Detector** | Monitorea tiempo de respuesta |
| **Filler Manager** | Reproduce coletillas pregrabadas |
| **Sherpa-ONNX Streaming** | TTS streaming (low latency) |

### **Fillers Pregrabados**

```yaml
# assets/fillers/fillers.yaml
fillers:
  - id: hmm_01
    file: hmm_01.wav
    text: "Mmm..."
    duration_ms: 800
    use_case: "Inicio de pensamiento"
  
  - id: thinking_01
    file: thinking_01.wav
    text: "DÃ©jame pensar..."
    duration_ms: 1200
    use_case: "Query compleja detectada"
  
  - id: interesting_01
    file: interesting_01.wav
    text: "Interesante..."
    duration_ms: 1000
    use_case: "Reconocimiento de input complejo"
  
  - id: wait_moment_01
    file: wait_moment_01.wav
    text: "Un momento..."
    duration_ms: 900
    use_case: "BÃºsqueda en memoria"
```

### **PolÃ­tica de ActivaciÃ³n**

```python
class LatencyDetector:
    THRESHOLDS = {
        "immediate": 200,      # <200ms: Sin filler
        "acceptable": 800,     # 200-800ms: Filler corto ("Mmm")
        "noticeable": 2000,    # 800-2000ms: Filler medio ("DÃ©jame pensar")
        "critical": 5000       # >2000ms: Filler + streaming partial
    }
    
    def select_filler(self, estimated_latency_ms: int, query_type: str):
        if estimated_latency_ms < self.THRESHOLDS["acceptable"]:
            return None  # Sin filler
        
        elif estimated_latency_ms < self.THRESHOLDS["noticeable"]:
            return "hmm_01"
        
        elif query_type == "complex":
            return "thinking_01"
        
        else:
            return "wait_moment_01"
```

### **Sherpa-ONNX Streaming TTS**

**Ventaja sobre Piper**: Genera audio chunk por chunk (latencia perceived <100ms)

```python
# Streaming TTS (chunked output)
for text_chunk in response_stream:
    audio_chunk = sherpa_tts.synthesize_chunk(text_chunk)
    audio_output_queue.put(audio_chunk)  # ReproducciÃ³n inmediata
```

---

## âš™ï¸ CAPA 4: OrquestaciÃ³n DinÃ¡mica (LoRA Scheduler)

### **PropÃ³sito**
Optimizar uso de CPU/RAM en tiempo real ajustando recursos segÃºn tipo de query.

### **Componentes**

| Componente | FunciÃ³n |
|------------|---------|
| **LoRA Adapter** | Ajuste fino de LFM2 para routing |
| **Resource Monitor** | Monitoreo de CPU/RAM |
| **Priority Queue** | PriorizaciÃ³n de queries |

### **LoRA Adapter sobre LFM2**

```python
# LoRA config
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,  # Rank bajo para eficiencia
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Solo attention
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# Aplicar a LFM2
lfm2_lora = get_peft_model(lfm2_base, lora_config)
# TamaÃ±o: +5MB sobre LFM2 (vs 1.2GB base)
```

**Entrenamiento del LoRA**:
- Dataset: Logs histÃ³ricos de `routing_decision_queue`
- Objetivo: Predecir clase Ã³ptima (TRM/LLM/TRANSLATE)
- MÃ©tricas: Accuracy routing + Latencia reducida

### **Dynamic Resource Allocation**

```python
class ResourceScheduler:
    def adjust_threads(self, query_type: str, cpu_usage: float):
        """
        Ajusta n_threads de LFM2 segÃºn carga
        """
        if query_type == "TRM":
            return 2  # TRM es lightweight
        
        elif cpu_usage > 80:
            return 4  # CPU saturada â†’ reducir threads
        
        elif query_type == "LLM":
            return 8  # Query compleja â†’ max threads
        
        else:
            return 6  # Balance normal
    
    def adjust_batch_size(self, queue_length: int):
        """
        Batching dinÃ¡mico si hay mÃºltiples queries
        """
        if queue_length >= 3:
            return 4  # Procesar 4 queries en batch
        else:
            return 1  # Sin batching
```

### **Priority Queue**

```python
import heapq

class PriorityQueue:
    priorities = {
        "TRM": 1,       # MÃ¡xima prioridad (cache rÃ¡pida)
        "NORMAL": 2,
        "LLM": 3,
        "TRANSLATE": 4  # Menor prioridad (mÃ¡s lento)
    }
    
    def __init__(self):
        self.heap = []
    
    def put(self, query_type: str, data: dict):
        priority = self.priorities.get(query_type, 5)
        heapq.heappush(self.heap, (priority, data))
    
    def get(self):
        return heapq.heappop(self.heap)[1]
```

---

## ğŸ“Š MÃ©tricas de Rendimiento Objetivo

| MÃ©trica | Objetivo v2.17 | Actual v2.16 |
|---------|----------------|--------------|
| **Latencia P50 (TRM)** | â‰¤500ms | N/A |
| **Latencia P50 (LLM)** | â‰¤2s | 7.2s |
| **Latencia P50 (Traducir)** | â‰¤3s | N/A |
| **RAM P99** | â‰¤14GB | 10.8GB |
| **CPU Utilization** | 70-85% | 60% |
| **Cache Hit Rate (TRM)** | â‰¥40% | N/A |
| **Filler Activation** | 30-50% | 0% |
| **Context Recall (RAG)** | â‰¥0.85 | N/A |

---

## ğŸš€ Roadmap de ImplementaciÃ³n

### **Fase 1: Capa 1 - I/O AsÃ­ncrono** (ACTUAL)
- [x] Sherpa-ONNX VAD
- [x] Vosk STT wrapper
- [x] BERT-es embedder
- [ ] DistilBART-es processor
- [ ] LoRA Router (crear y entrenar)
- [ ] Kitter TTS wrapper
- [ ] Input/Output threads

### **Fase 2: Capa 2 - Memoria RAG**
- [ ] EmbeddingGemma 2B wrapper
- [ ] Qdrant setup local
- [ ] IndexaciÃ³n de turnos
- [ ] Retrieval K=5

### **Fase 3: Capa 3 - Fluidez Natural**
- [ ] Grabar fillers en espaÃ±ol
- [ ] Sherpa-ONNX streaming TTS
- [ ] Latency detector
- [ ] Filler injection automÃ¡tica

### **Fase 4: Capa 4 - LoRA Scheduler**
- [ ] LoRA adapter sobre LFM2
- [ ] Resource monitor
- [ ] Priority queue
- [ ] Dynamic adjustment

### **Fase 5: IntegraciÃ³n Final**
- [ ] Orquestador maestro
- [ ] Test E2E
- [ ] Benchmarking
- [ ] DocumentaciÃ³n

---

## ğŸ”§ ConfiguraciÃ³n del Sistema

```yaml
# config/sarai_v2.17.yaml
system:
  version: "2.17.0"
  max_ram_gb: 14
  max_cpu_percent: 85

layer1_io:
  vad:
    provider: "sherpa"  # "sherpa" | "silero"
    window_ms: 30
    min_speech_ms: 250
    min_silence_ms: 500
  
  stt:
    provider: "vosk"
    model: "vosk-model-small-es-0.42"
    sample_rate: 16000
  
  router:
    model: "lora_router_v1"
    threshold_trm: 0.95
    threshold_translate: 0.7

layer2_memory:
  embedding_model: "google/embedding-gemma-2b"
  vector_db: "qdrant"  # "qdrant" | "chromadb"
  collection: "conversation_history"
  top_k: 5
  max_conversations: 100

layer3_fluidity:
  filler_threshold_ms: 800
  streaming_tts: true
  tts_provider: "sherpa"  # "sherpa" | "piper"

layer4_orchestration:
  lora_enabled: true
  dynamic_threads: true
  priority_queue: true
  resource_monitor_interval_ms: 100
```

---

**Autor**: SARAi Development Team  
**Licencia**: Ver LICENSE_GUIDE.md
